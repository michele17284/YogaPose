{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numbers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torch import nn\n",
    "import os\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import genericpath\n",
    "import json\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "np.random.seed(17)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# force working on cpu due to memory limitation\n",
    "#device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "def add_margin(pil_img, top, right, bottom, left, color):\n",
    "    width, height = pil_img.size\n",
    "    new_width = width + right + left\n",
    "    new_height = height + top + bottom\n",
    "    result = Image.new(pil_img.mode, (new_width, new_height), color)\n",
    "    result.paste(pil_img, (left, top))\n",
    "    return result\n",
    "\n",
    "\n",
    "class YogaPoseDataset(Dataset):\n",
    "\n",
    "    def __init__(self, root, train, size=(256, 192), transform=None, example_images=None):\n",
    "        root = root\n",
    "        subdir = \"train\" if train else \"test\"\n",
    "        self.data_path = os.path.join(root,subdir)\n",
    "        self.size = size\n",
    "        self.transform = transform\n",
    "        # call to init the data\n",
    "        self._init_data()\n",
    "\n",
    "    def _init_data(self):\n",
    "        images = list()\n",
    "\n",
    "        for _, directory_class in enumerate(os.listdir(self.data_path)):\n",
    "            class_path = os.path.join(self.data_path, directory_class)\n",
    "            for file_name in os.listdir(class_path):\n",
    "                f = cv2.imread(os.path.join(class_path, file_name), cv2.IMREAD_COLOR)\n",
    "\n",
    "                f = cv2.cvtColor(f, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                if self.transform is not None:\n",
    "                    f = self.transform(f)\n",
    "\n",
    "                data = torch.reshape(torch.FloatTensor(f), (3, self.size[0], self.size[1]))\n",
    "\n",
    "                # format example  images[x][0] -> (label, input)\n",
    "                # format example  images[x][1] -> [other information]\n",
    "                # images[x] -> ((class_id, image_tensor), [filename])\n",
    "                images.append((int(directory_class), data))\n",
    "\n",
    "\n",
    "        np.random.shuffle(images,)\n",
    "        self.images = images\n",
    "\n",
    "    def __len__(self):\n",
    "        # returns the number of samples in our dataset\n",
    "        return len(self.images)\n",
    "\n",
    "    def getData(self):\n",
    "        return self.images\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            idx: the index of the sample\n",
    "\n",
    "        Returns: a tuple (class, input) for the given sample\n",
    "\n",
    "        \"\"\"\n",
    "        return self.images[idx]\n",
    "\n",
    "    def collate_fn(self, data):\n",
    "        #print(data)\n",
    "        Xs = torch.stack([x[1] for x in data])\n",
    "        y = torch.stack([torch.tensor(x[0]) for x in data])\n",
    "        return Xs, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 1200x800 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#DATASET ANALYSIS\n",
    "with open(\"/home/orlando/PycharmProjects/YogaPose/data/annotations/annotation.json\") as f:\n",
    "    datalsit = []\n",
    "    data = json.load(f)\n",
    "    for i,v in data.items():\n",
    "        datalsit.append(len(v[\"images_list\"]))\n",
    "# Iterating through the json\n",
    "# list\n",
    "\n",
    "\n",
    "bins = list(range(107))\n",
    "plt.figure(figsize=(15, 10), dpi=80)\n",
    "plt.bar(bins, datalsit, width = 0.85, color=\"cyan\" )\n",
    "plt.savefig(\"out/data_histogram.png\")\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "data_aug_parameters = {\n",
    "    \"RC_size\": 32, \"RC_padding\": 2,  # default = none\n",
    "    \"CJ_brightness\": 50,  # default = 0\n",
    "    \"CJ_contrast\": 0,  # default = 0\n",
    "    \"CJ_saturation\": 0,  # default = 0\n",
    "    \"CJ_hue\": 0,  # default = 0\n",
    "    \"P_padding\": 3, \"P_type\": \"constant\",  # default = constant\n",
    "    \"HF_p\": 0.5, \"VF_p\": 0.5, \"RR_degrees\": 60, \"RG_p\": 0.2}\n",
    "\n",
    "data_aug_transform = [transforms.ToPILImage()]\n",
    "norm_transform = transforms.Compose(data_aug_transform + [transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "test_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "DATASET_PATH = './data/split/'\n",
    "ANNOTATION_PATH = './data/annotations/'\n",
    "MODEL_NAME = \"tpr_a4_256x192\"\n",
    "testset = YogaPoseDataset(DATASET_PATH,train=False, transform=test_transform)\n",
    "trainset = YogaPoseDataset(DATASET_PATH,train=True, transform=norm_transform)\n",
    "valset = trainset[(len(trainset)//10)*9:]\n",
    "trainset = trainset[:(len(trainset)//10)*9]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>Load pretrained weights from url: https://github.com/yangsenius/TransPose/releases/download/Hub/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      "Successfully loaded model  (on cpu) with pretrained weights!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/orlando/.cache/torch/hub/yangsenius_TransPose_main\n",
      "/home/orlando/.cache/torch/hub/yangsenius_TransPose_main/lib/models/transpose_r.py:333: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  dim_t = temperature ** (2 * (dim_t // 2) / one_direction_feats)\n"
     ]
    },
    {
     "data": {
      "text/plain": "TransPoseR(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (3): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (reduce): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n  (global_encoder): TransformerEncoder(\n    (layers): ModuleList(\n      (0): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n        )\n        (linear1): Linear(in_features=256, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=1024, out_features=256, bias=True)\n        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n      (1): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n        )\n        (linear1): Linear(in_features=256, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=1024, out_features=256, bias=True)\n        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n      (2): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n        )\n        (linear1): Linear(in_features=256, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=1024, out_features=256, bias=True)\n        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n      (3): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n        )\n        (linear1): Linear(in_features=256, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=1024, out_features=256, bias=True)\n        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n    )\n  )\n  (deconv_layers): Sequential(\n    (0): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n  )\n  (final_layer): Conv2d(256, 17, kernel_size=(1, 1), stride=(1, 1))\n)"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get model from torch hub\n",
    "assert MODEL_NAME in [\"tpr_a4_256x192\", \"tph_a4_256x192\"]\n",
    "\n",
    "transpose_model = torch.hub.load('yangsenius/TransPose:main', MODEL_NAME, pretrained=True)\n",
    "transpose_model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "from TransPose.lib.config import cfg\n",
    "from TransPose.lib.utils import transforms\n",
    "from TransPose.lib.core.inference import get_final_preds\n",
    "from TransPose.visualize import inspect_atten_map_by_locations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "class PoseClassifier(nn.Module):\n",
    "    def __init__(self, n_class,\n",
    "                 transpose_model,device=device, fine_tune=False, pretrained=True):\n",
    "        super(PoseClassifier, self).__init__()\n",
    "        layers = []\n",
    "        dropout = 0.5\n",
    "        hidden_layers = [128, 512, 512, 512, 512, 256, 128, 128]\n",
    "        self.tpr = transpose_model\n",
    "        layers.append(nn.Conv2d(17, 128, 3, padding=1))\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.conv1 = nn.Conv2d(17, hidden_layers[0], 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(hidden_layers[0])\n",
    "        self.pool1 = nn.MaxPool2d((2, 2), 2)\n",
    "\n",
    "        self.pool2 = nn.MaxPool2d((3, 3), 3)\n",
    "        self.conv2 = nn.Conv2d(hidden_layers[0], hidden_layers[1], 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(hidden_layers[1])\n",
    "        self.conv3 = nn.Conv2d(hidden_layers[1], hidden_layers[2], 3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(hidden_layers[2])\n",
    "        self.conv4 = nn.Conv2d(hidden_layers[2], hidden_layers[3], 3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(hidden_layers[3])\n",
    "        self.conv5 = nn.Conv2d(hidden_layers[3], hidden_layers[4], 3, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(hidden_layers[4])\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.lin1 = nn.Linear(hidden_layers[4],hidden_layers[-1])\n",
    "        self.classifier = nn.Linear(hidden_layers[-1],n_class)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.tpr(x)\n",
    "        #print(out.size(),\"AFTER TPH\")\n",
    "        out = self.conv1(out)\n",
    "        out = self.bn1(out)\n",
    "        #print(out.size(),\"AFTER CONV1\")\n",
    "        out = self.pool1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        #print(out.size(),\"AFTER POOL\")\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        #print(out.size(),\"AFTER CONV2\")\n",
    "        out = self.pool1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        #print(out.size(),\"AFTER POOL\")\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        #print(out.size(),\"AFTER CONV3\")\n",
    "        out = self.pool2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        #print(out.size(),\"AFTER POOL\")\n",
    "        out = self.conv4(out)\n",
    "        out = self.bn4(out)\n",
    "        #print(out.size(),\"AFTER CONV4\")\n",
    "        out = self.pool2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        #print(out.size(),\"AFTER POOL\")\n",
    "        out = self.conv5(out)\n",
    "        out = self.bn5(out)\n",
    "        #print(out.size(),\"AFTER CONV5\")\n",
    "        #out = self.pool1(out)\n",
    "        out = self.flatten(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        #print(out.size(),\"AFTER POOL\")\n",
    "        out = self.lin1(out)\n",
    "        '''\n",
    "        out = self.pool1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        #print(out.size(),\"AFTER POOL\")\n",
    "        out = self.conv6(out)\n",
    "\n",
    "        #print(out.size(),\"AFTER CONV6\")\n",
    "        '''\n",
    "\n",
    "\n",
    "        out = self.flatten(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        #print(out.size(),\"AFTER FLATTEN\")\n",
    "\n",
    "        out = self.classifier(out)\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "num_classes = 107\n",
    "num_epochs = 100\n",
    "batch_size = 12\n",
    "learning_rate = 0.0001\n",
    "learning_rate_decay = 0.99\n",
    "\n",
    "model = PoseClassifier(n_class=num_classes, transpose_model=transpose_model)\n",
    "model.to(device)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=trainset, batch_size=batch_size, shuffle=False,collate_fn=testset.collate_fn)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=valset,batch_size=batch_size,shuffle=False,collate_fn=testset.collate_fn)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=testset,batch_size=batch_size,shuffle=False,collate_fn=testset.collate_fn)\n",
    "\n",
    "\n",
    "fine_tune = False\n",
    "if fine_tune:\n",
    "    params_to_update = []\n",
    "    for param in model.tpr.parameters():\n",
    "        param.requires_grad = False\n",
    "    for p in model.parameters():\n",
    "        if p.requires_grad == True:\n",
    "            params_to_update.append(p)\n",
    "else:\n",
    "    params_to_update = model.parameters()\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params_to_update, lr=learning_rate)\n",
    "\n",
    "def update_lr(optimizer, lr):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "372\n"
     ]
    },
    {
     "data": {
      "text/plain": "4455"
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(train_loader))\n",
    "len(train_loader.dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "#best_model = type(model)(num_classes, fine_tune, pretrained) # get a new instance\n",
    "def train(model,num_epochs=num_epochs,lr=learning_rate):\n",
    "    total_step = len(train_loader)\n",
    "    loss_train = []\n",
    "    loss_val = []\n",
    "    best_accuracy = None\n",
    "    accuracy_val = []\n",
    "    accuracy_test = []\n",
    "\n",
    "    train_f1 = []\n",
    "    val_f1 = []\n",
    "    for epoch in range(num_epochs):\n",
    "        train_predicts = torch.tensor([]).to(device)\n",
    "        train_labels = torch.tensor([]).to(device)\n",
    "        val_predicts = torch.tensor([]).to(device)\n",
    "        val_labels = torch.tensor([]).to(device)\n",
    "        model.train()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        loss_iter = 0\n",
    "\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            # Move tensors to the configured device\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            #print(outputs,labels)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_iter += loss.item()\n",
    "\n",
    "\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                      .format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))\n",
    "\n",
    "            train_predicts = torch.cat((train_predicts, predicted), dim=0)\n",
    "            train_labels = torch.cat((train_labels, labels), dim=0)\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        accuracy_test.append(accuracy)\n",
    "        loss_train.append(loss_iter / (len(train_loader) * batch_size))\n",
    "        print('Training accuracy is: {} %'.format(accuracy))\n",
    "        train_labels = train_labels.cpu().numpy()\n",
    "        train_predicts = train_predicts.cpu().numpy()\n",
    "        f1 =f1_score(train_labels, train_predicts, average='weighted')\n",
    "        f1 = f1 * 100\n",
    "        train_f1.append(f1)\n",
    "        print('Training F1 is: {} %'.format(f1))\n",
    "\n",
    "        # Code to update the lr\n",
    "        lr *= learning_rate_decay\n",
    "        update_lr(optimizer, lr)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            loss_iter = 0\n",
    "            for images, labels in val_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss_iter += loss.item()\n",
    "                val_predicts = torch.cat((val_predicts, predicted), dim=0)\n",
    "                val_labels = torch.cat((val_labels, labels), dim=0)\n",
    "            loss_val.append(loss_iter / (len(val_loader) * batch_size))\n",
    "            accuracy = 100 * correct / total\n",
    "            accuracy_val.append(accuracy)\n",
    "            print('Validation accuracy is: {} %'.format(accuracy))\n",
    "            val_labels = val_labels.cpu().numpy()\n",
    "            val_predicts = val_predicts.cpu().numpy()\n",
    "            f1 =f1_score(val_labels, val_predicts, average='weighted')\n",
    "            f1 = f1 * 100\n",
    "            val_f1.append(f1)\n",
    "            print('Validation F1 is: {} %'.format(f1))\n",
    "            print(\"--------------------- NEXT EPOCH ---------------------\")\n",
    "            early_stop = True\n",
    "            patience = 3\n",
    "            if epoch > patience - 1:\n",
    "                for j in range(patience - 1):\n",
    "                    if max(accuracy_val) > list(reversed(accuracy_val))[j]:\n",
    "                        if \"not_improving_epochs\" in locals():\n",
    "                            not_improving_epochs += 1\n",
    "                        else:\n",
    "                            not_improving_epochs = 1\n",
    "                        print('Not saving the model')\n",
    "                    else:\n",
    "                        not_improving_epochs = 0\n",
    "                        best_model = model\n",
    "                        print(\"Saving the model\")\n",
    "                        break\n",
    "                    if not_improving_epochs >= patience:\n",
    "                        early_stop = True\n",
    "                        print('Early stopping')\n",
    "                        break\n",
    "                    break\n",
    "\n",
    "    plt.figure(2)\n",
    "    plt.plot(loss_train, 'r', label='Train loss')\n",
    "    plt.plot(loss_val, 'g', label='Val loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(3)\n",
    "    plt.plot(accuracy_val, 'r', label='Val accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return {\"Loss/train\": loss_train, \"Loss/val\": loss_val, \"Accuracy/train\":accuracy_val, \"Accuracy/val\": accuracy_test, \"F1/train\": train_f1, \"F1/val\": val_f1}\n",
    "\n",
    "def test(model):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "            all_preds = torch.tensor([]).to(device)\n",
    "            all_labels = torch.tensor([]).to(device)\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            loss_iter = 0\n",
    "            for images, labels in test_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss_iter += loss.item()\n",
    "                all_preds = torch.cat((all_preds, predicted), dim=0)\n",
    "                all_labels = torch.cat((all_labels, labels), dim=0)\n",
    "\n",
    "            loss_test =(loss_iter / (len(test_loader) * batch_size))\n",
    "            accuracy = 100 * correct / total\n",
    "\n",
    "            all_preds = all_preds.cpu().numpy()\n",
    "            all_labels = all_labels.cpu().numpy()\n",
    "            f1 =  f1_score(all_labels, all_preds, average='weighted')\n",
    "            f1 = f1 * 100\n",
    "            print('Test accuracy is: {} %'.format(accuracy))\n",
    "            print('Test f1 is: {} %'.format(f1))\n",
    "            print('Test Loss: {:.4f}'.format(loss_test))\n",
    "    return all_preds, all_labels, {\"Accuracy/test\": accuracy, \"Loss/test\": loss_test, \"F1/test\" : f1}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is: 66.41074856046065 %\n",
      "Test f1 is: 66.29739947072856 %\n",
      "Test Loss: 0.1908\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"out/logs/15_Dec[21-38-49]/model.ckpt\"))\n",
    "test_predicted, test_labels , test_values = test(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_true = test_labels\n",
    "y_pred = test_predicted\n",
    "target_names = [str(x) for x in range(107)]\n",
    "\n",
    "print(target_names)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.0000    0.0000    0.0000        11\n",
      "           1     0.8462    0.9167    0.8800        12\n",
      "           2     0.5385    0.6364    0.5833        11\n",
      "           3     0.3636    0.4444    0.4000         9\n",
      "           4     0.8750    0.5833    0.7000        12\n",
      "           5     0.7000    0.8750    0.7778         8\n",
      "           6     1.0000    0.5833    0.7368        12\n",
      "           7     0.5714    0.6667    0.6154         6\n",
      "           8     1.0000    0.6364    0.7778        11\n",
      "           9     0.8462    0.7857    0.8148        14\n",
      "          10     0.6923    1.0000    0.8182         9\n",
      "          11     0.8333    0.5556    0.6667         9\n",
      "          12     0.7500    0.9000    0.8182        10\n",
      "          13     0.6667    0.5000    0.5714        12\n",
      "          14     0.5000    0.2500    0.3333        12\n",
      "          15     0.5455    0.7500    0.6316         8\n",
      "          16     1.0000    0.8000    0.8889        15\n",
      "          17     0.8667    1.0000    0.9286        13\n",
      "          18     0.2941    0.8333    0.4348         6\n",
      "          19     0.8000    0.7273    0.7619        11\n",
      "          20     0.2308    0.4286    0.3000         7\n",
      "          21     0.8000    0.8889    0.8421         9\n",
      "          22     1.0000    0.6364    0.7778        11\n",
      "          23     0.7778    0.8750    0.8235         8\n",
      "          24     0.7778    0.8750    0.8235         8\n",
      "          25     0.2000    0.5000    0.2857         4\n",
      "          26     0.7059    0.8571    0.7742        14\n",
      "          27     0.2400    0.5000    0.3243        12\n",
      "          28     0.5455    0.6667    0.6000         9\n",
      "          29     0.5556    0.8333    0.6667         6\n",
      "          30     0.6316    0.9231    0.7500        13\n",
      "          31     0.9231    0.8571    0.8889        14\n",
      "          32     0.9000    0.6429    0.7500        14\n",
      "          33     0.5455    0.6667    0.6000         9\n",
      "          34     0.8750    1.0000    0.9333         7\n",
      "          35     0.6923    0.9000    0.7826        10\n",
      "          36     0.2308    0.7500    0.3529         4\n",
      "          37     0.7143    1.0000    0.8333         5\n",
      "          38     0.3000    0.6000    0.4000         5\n",
      "          39     0.8750    0.5833    0.7000        12\n",
      "          40     0.8571    0.8571    0.8571         7\n",
      "          41     1.0000    1.0000    1.0000        10\n",
      "          42     0.3636    0.4000    0.3810        10\n",
      "          43     0.7500    0.6667    0.7059         9\n",
      "          44     0.3750    0.3750    0.3750         8\n",
      "          45     0.6667    1.0000    0.8000         8\n",
      "          46     0.4286    0.7500    0.5455         4\n",
      "          47     0.0000    0.0000    0.0000         4\n",
      "          48     0.4545    0.5556    0.5000         9\n",
      "          49     0.8750    0.7778    0.8235         9\n",
      "          50     0.7500    0.9000    0.8182        10\n",
      "          51     0.4167    0.5000    0.4545        10\n",
      "          52     0.7647    0.8667    0.8125        15\n",
      "          53     0.4286    0.2308    0.3000        13\n",
      "          54     0.6364    0.7778    0.7000         9\n",
      "          55     0.5833    0.5833    0.5833        12\n",
      "          56     0.5000    0.8889    0.6400         9\n",
      "          57     0.3750    0.3000    0.3333        10\n",
      "          58     1.0000    1.0000    1.0000         9\n",
      "          59     0.8333    0.6667    0.7407        15\n",
      "          60     0.6667    0.2857    0.4000         7\n",
      "          61     0.5000    0.6364    0.5600        11\n",
      "          62     0.2857    0.3333    0.3077         6\n",
      "          63     1.0000    0.8000    0.8889         5\n",
      "          64     0.5385    0.7778    0.6364         9\n",
      "          65     0.9091    0.9091    0.9091        11\n",
      "          66     0.6667    0.4000    0.5000        10\n",
      "          67     0.8000    0.5714    0.6667         7\n",
      "          68     0.7500    0.8571    0.8000        14\n",
      "          69     0.6000    0.6000    0.6000        10\n",
      "          70     0.8333    0.7143    0.7692        14\n",
      "          71     0.7273    0.8889    0.8000         9\n",
      "          72     1.0000    0.5556    0.7143         9\n",
      "          73     0.9231    0.8571    0.8889        14\n",
      "          74     0.9167    0.7333    0.8148        15\n",
      "          75     1.0000    0.8000    0.8889        15\n",
      "          76     0.8333    0.5000    0.6250        10\n",
      "          77     0.7500    0.5455    0.6316        11\n",
      "          78     0.6667    0.6667    0.6667         6\n",
      "          79     1.0000    0.2222    0.3636         9\n",
      "          80     0.2857    0.2857    0.2857         7\n",
      "          81     0.8333    0.8333    0.8333         6\n",
      "          82     0.5833    0.8750    0.7000         8\n",
      "          83     0.4286    0.3750    0.4000         8\n",
      "          84     0.6667    0.6667    0.6667         6\n",
      "          85     1.0000    0.5000    0.6667        12\n",
      "          86     0.6364    0.5833    0.6087        12\n",
      "          87     0.4286    0.2500    0.3158        12\n",
      "          88     0.5000    0.1667    0.2500         6\n",
      "          89     0.3333    0.5556    0.4167         9\n",
      "          90     0.8889    0.6667    0.7619        12\n",
      "          91     0.8333    0.8333    0.8333         6\n",
      "          92     0.9091    0.8333    0.8696        12\n",
      "          93     0.9000    0.6923    0.7826        13\n",
      "          94     0.7273    0.6154    0.6667        13\n",
      "          95     0.7143    0.7143    0.7143         7\n",
      "          96     0.5000    0.2667    0.3478        15\n",
      "          97     0.8750    0.8750    0.8750         8\n",
      "          98     0.7500    0.6923    0.7200        13\n",
      "          99     0.6667    0.8000    0.7273        10\n",
      "         100     0.7143    0.6250    0.6667         8\n",
      "         101     0.7500    0.3333    0.4615         9\n",
      "         102     0.7500    0.7500    0.7500        12\n",
      "         103     0.8333    0.7143    0.7692         7\n",
      "         104     0.7143    0.7143    0.7143        14\n",
      "         105     0.2143    0.5000    0.3000         6\n",
      "         106     0.8333    0.7143    0.7692         7\n",
      "\n",
      "    accuracy                         0.6641      1042\n",
      "   macro avg     0.6720    0.6599    0.6451      1042\n",
      "weighted avg     0.6995    0.6641    0.6630      1042\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/orlando/anaconda3/envs/AML_env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/orlando/anaconda3/envs/AML_env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/orlando/anaconda3/envs/AML_env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "txt = (classification_report(y_true, y_pred, target_names=target_names, digits=4))\n",
    "print(txt)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 1200x800 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def f1_ist(txt=txt):\n",
    "    f1list = []\n",
    "    for line in txt.splitlines():\n",
    "        if len(line.split()) > 3:\n",
    "            if line.split()[3][0] in [\"0\", \"1\"]:\n",
    "\n",
    "                f1list.append(float(line.split()[3]))\n",
    "    return f1list [:-2]\n",
    "\n",
    "f1list = f1_ist(txt)\n",
    "bins = list(range(107))\n",
    "plt.figure(figsize=(15, 10), dpi=80)\n",
    "plt.bar(bins, f1list, width = 0.85, color=\"red\" )\n",
    "plt.savefig(\"out/histogram.jpg\")\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}