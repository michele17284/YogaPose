{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os.path as osp\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from visualize import update_config, add_path\n",
    "\n",
    "lib_path = osp.join('lib')\n",
    "add_path(lib_path)\n",
    "\n",
    "import dataset as dataset\n",
    "from config import cfg\n",
    "import models\n",
    "import os\n",
    "import torchvision.transforms as T\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] ='0'\n",
    "file_name = 'experiments/coco/transpose_r/TP_R_256x192_d256_h1024_enc4_mh8.yaml' # choose a yaml file\n",
    "f = open(file_name, 'r')\n",
    "update_config(cfg, file_name)\n",
    "\n",
    "model_name = 'T-H-A4'\n",
    "assert model_name in ['T-R', 'T-H','T-H-L','T-R-A4', 'T-H-A6', 'T-H-A5', 'T-H-A4' ,'T-R-A4-DirectAttention']\n",
    "\n",
    "normalize = T.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "\n",
    "dataset = eval('dataset.'+cfg.DATASET.DATASET)(\n",
    "        cfg, cfg.DATASET.ROOT, cfg.DATASET.TEST_SET, False,\n",
    "        T.Compose([\n",
    "            T.ToTensor(),\n",
    "            normalize,\n",
    "        ])\n",
    "    )\n",
    "\n",
    "\n",
    "device = torch.device('cuda')\n",
    "model = eval('models.'+cfg.MODEL.NAME+'.get_pose_net')(\n",
    "    cfg, is_train=True\n",
    ")\n",
    "\n",
    "if cfg.TEST.MODEL_FILE:\n",
    "    print('=> loading model from {}'.format(cfg.TEST.MODEL_FILE))\n",
    "    model.load_state_dict(torch.load(cfg.TEST.MODEL_FILE), strict=True)\n",
    "else:\n",
    "    raise ValueError(\"please choose one ckpt in cfg.TEST.MODEL_FILE\")\n",
    "\n",
    "model.to(device)\n",
    "print(\"model params:{:.3f}M\".format(sum([p.numel() for p in model.parameters()])/1000**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from lib.core.inference import get_final_preds\n",
    "from lib.utils import transforms, vis\n",
    "import cv2\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    tmp = []\n",
    "    tmp2 = []\n",
    "    img = dataset[0][0]\n",
    "\n",
    "    inputs = torch.cat([img.to(device)]).unsqueeze(0)\n",
    "    outputs = model(inputs)\n",
    "    if isinstance(outputs, list):\n",
    "        output = outputs[-1]\n",
    "    else:\n",
    "        output = outputs\n",
    "\n",
    "    if cfg.TEST.FLIP_TEST:\n",
    "        input_flipped = np.flip(inputs.cpu().numpy(), 3).copy()\n",
    "        input_flipped = torch.from_numpy(input_flipped).cuda()\n",
    "        outputs_flipped = model(input_flipped)\n",
    "\n",
    "        if isinstance(outputs_flipped, list):\n",
    "            output_flipped = outputs_flipped[-1]\n",
    "        else:\n",
    "            output_flipped = outputs_flipped\n",
    "\n",
    "        output_flipped = transforms.flip_back(output_flipped.cpu().numpy(),\n",
    "                                   dataset.flip_pairs)\n",
    "        output_flipped = torch.from_numpy(output_flipped.copy()).cuda()\n",
    "\n",
    "        output = (output + output_flipped) * 0.5\n",
    "\n",
    "    preds, maxvals = get_final_preds(\n",
    "            cfg, output.clone().cpu().numpy(), None, None, transform_back=False)\n",
    "\n",
    "# from heatmap_coord to original_image_coord\n",
    "query_locations = np.array([p*4+0.5 for p in preds[0]])\n",
    "print(query_locations)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from visualize import inspect_atten_map_by_locations\n",
    "\n",
    "inspect_atten_map_by_locations(img, model, query_locations, model_name=\"transposer\", mode='dependency', save_img=True, threshold=0."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}