{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "import os\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import genericpath\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# force working on cpu due to memory limitation\n",
    "#device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def add_margin(pil_img, top, right, bottom, left, color):\n",
    "    width, height = pil_img.size\n",
    "    new_width = width + right + left\n",
    "    new_height = height + top + bottom\n",
    "    result = Image.new(pil_img.mode, (new_width, new_height), color)\n",
    "    result.paste(pil_img, (left, top))\n",
    "    return result\n",
    "\n",
    "\n",
    "class YogaPoseDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataset_path, size=(256, 192), transform=None):\n",
    "        self.data_path = dataset_path\n",
    "        self.size = size\n",
    "        self.transform = transform\n",
    "\n",
    "        # call to init the data\n",
    "        self._init_data()\n",
    "\n",
    "    def _init_data(self):\n",
    "        images = list()\n",
    "\n",
    "        for _, directory_class in enumerate(os.listdir(self.data_path)):\n",
    "            class_path = os.path.join(self.data_path, directory_class)\n",
    "            for file_name in os.listdir(class_path):\n",
    "                f = cv2.imread(os.path.join(class_path, file_name), cv2.IMREAD_COLOR)\n",
    "                f = cv2.cvtColor(f, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                if self.transform is not None:\n",
    "                    f = self.transform(f)\n",
    "\n",
    "                data = torch.reshape(torch.FloatTensor(f), (3, self.size[0], self.size[1]))\n",
    "\n",
    "                # format example  images[x][0] -> (label, input)\n",
    "                # format example  images[x][1] -> [other information]\n",
    "                # images[x] -> ((class_id, image_tensor), [filename])\n",
    "                images.append((int(directory_class), data))\n",
    "\n",
    "        np.random.shuffle(images)\n",
    "        self.images = images\n",
    "\n",
    "    def __len__(self):\n",
    "        # returns the number of samples in our dataset\n",
    "        return len(self.images)\n",
    "\n",
    "    def getData(self):\n",
    "        return self.images\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            idx: the index of the sample\n",
    "\n",
    "        Returns: a tuple (class, input) for the given sample\n",
    "\n",
    "        \"\"\"\n",
    "        return self.images[idx]\n",
    "\n",
    "    def getFileName(self, idx):\n",
    "        return str(self.images[idx][1][0])\n",
    "\n",
    "    def getOriginalImage(self, idx):\n",
    "        class_path = os.path.join(self.data_path, str(self.images[idx][0][0]))\n",
    "        out = cv2.imread(os.path.join(class_path, self.getFileName(idx)), cv2.IMREAD_COLOR)\n",
    "        print(self.getFileName(idx))\n",
    "        return out\n",
    "\n",
    "    def collate_fn(self, data):\n",
    "        #print(data)\n",
    "        Xs = torch.stack([x[1] for x in data])\n",
    "        y = torch.stack([torch.tensor(x[0]) for x in data])\n",
    "        return Xs, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "DATASET_PATH = './data/images/'\n",
    "ANNOTATION_PATH = './data/annotations/'\n",
    "MODEL_NAME = \"tpr_a4_256x192\"\n",
    "norm_transform = T.Compose([T.ToTensor(), T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]), ])\n",
    "dataset = YogaPoseDataset(DATASET_PATH, transform=norm_transform)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/michele/.cache/torch/hub/yangsenius_TransPose_main\n",
      "/home/michele/.cache/torch/hub/yangsenius_TransPose_main/lib/models/transpose_r.py:333: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  dim_t = temperature ** (2 * (dim_t // 2) / one_direction_feats)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>Load pretrained weights from url: https://github.com/yangsenius/TransPose/releases/download/Hub/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      "Successfully loaded model  (on cpu) with pretrained weights!\n"
     ]
    },
    {
     "data": {
      "text/plain": "TransPoseR(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (3): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (reduce): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n  (global_encoder): TransformerEncoder(\n    (layers): ModuleList(\n      (0): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n        )\n        (linear1): Linear(in_features=256, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=1024, out_features=256, bias=True)\n        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n      (1): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n        )\n        (linear1): Linear(in_features=256, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=1024, out_features=256, bias=True)\n        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n      (2): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n        )\n        (linear1): Linear(in_features=256, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=1024, out_features=256, bias=True)\n        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n      (3): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n        )\n        (linear1): Linear(in_features=256, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=1024, out_features=256, bias=True)\n        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n    )\n  )\n  (deconv_layers): Sequential(\n    (0): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n  )\n  (final_layer): Conv2d(256, 17, kernel_size=(1, 1), stride=(1, 1))\n)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get model from torch hub\n",
    "assert MODEL_NAME in [\"tpr_a4_256x192\", \"tph_a4_256x192\"]\n",
    "modelyaml = {\n",
    "    \"tph_a4_256x192\": \"models_yaml/TP_H_w48_256x192_stage3_1_4_d96_h192_relu_enc4_mh1.yaml\",\n",
    "    \"tpr_a4_256x192\": \"models_yaml/TP_R_256x192_d256_h1024_enc4_mh8.yaml\"\n",
    "}\n",
    "model = torch.hub.load('yangsenius/TransPose:main', MODEL_NAME, pretrained=True)\n",
    "model.to(device)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from TransPose.lib.config import cfg\n",
    "from TransPose.lib.utils import transforms\n",
    "from TransPose.lib.core.inference import get_final_preds\n",
    "from TransPose.visualize import inspect_atten_map_by_locations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "\n",
    "train_split_position = (len(dataset) // 10) * 8\n",
    "val_split_position = train_split_position+len(dataset)//10\n",
    "trainset = dataset[:train_split_position]\n",
    "valset = dataset[train_split_position:val_split_position]\n",
    "testset = dataset[val_split_position:]\n",
    "#trainset = dataset[:100]\n",
    "#print(trainset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "OUT_DIR = \"./out/\"\n",
    "idx = 0\n",
    "\n",
    "if not os.path.isdir(OUT_DIR):\n",
    "    os.makedirs(OUT_DIR)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\nwith torch.no_grad():\\n\\tmodel.eval()\\n\\timg = dataset[idx][1]\\n\\n\\tinputs = torch.cat([img.to(device)]).unsqueeze(0)\\n\\toutputs = model(inputs)\\n\\n\\tif isinstance(outputs, list):\\n\\t\\toutput = outputs[-1]\\n\\telse:\\n\\t\\toutput = outputs\\n\\n\\tif cfg.TEST.FLIP_TEST:\\n\\t\\tinput_flipped = np.flip(inputs.cpu().numpy(), 3).copy()\\n\\t\\tinput_flipped = torch.from_numpy(input_flipped).cuda()\\n\\t\\toutputs_flipped = model(input_flipped)\\n\\n\\t\\tif isinstance(outputs_flipped, list):\\n\\t\\t\\toutput_flipped = outputs_flipped[-1]\\n\\t\\telse:\\n\\t\\t\\toutput_flipped = outputs_flipped\\n\\n\\t\\toutput_flipped = transforms.flip_back(output_flipped.cpu().numpy(), dataset.flip_pairs)\\n\\t\\toutput_flipped = torch.from_numpy(output_flipped.copy()).cuda()\\n\\n\\t\\toutput = (output + output_flipped) * 0.5\\n\\n\\tpreds, maxvals = get_final_preds(cfg, output.clone().cpu().numpy(), None, None, transform_back=False)\\n\\n# from heatmap_coord to original_image_coord\\nquery_locations = np.array([p * 4 + 0.5 for p in preds[0]])\\n\\ninspect_atten_map_by_locations(img, model, query_locations, model_name=\"transposer\", mode=\\'dependency\\', save_img=True, threshold=0.0, outinfo=(OUT_DIR, dataset.getFileName(idx)))\\n\\ncv2.imwrite(OUT_DIR + dataset.getFileName(idx) + \"_original_img.jpg\", dataset.getOriginalImage(idx))\\n'"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "with torch.no_grad():\n",
    "\tmodel.eval()\n",
    "\timg = dataset[idx][1]\n",
    "\n",
    "\tinputs = torch.cat([img.to(device)]).unsqueeze(0)\n",
    "\toutputs = model(inputs)\n",
    "\n",
    "\tif isinstance(outputs, list):\n",
    "\t\toutput = outputs[-1]\n",
    "\telse:\n",
    "\t\toutput = outputs\n",
    "\n",
    "\tif cfg.TEST.FLIP_TEST:\n",
    "\t\tinput_flipped = np.flip(inputs.cpu().numpy(), 3).copy()\n",
    "\t\tinput_flipped = torch.from_numpy(input_flipped).cuda()\n",
    "\t\toutputs_flipped = model(input_flipped)\n",
    "\n",
    "\t\tif isinstance(outputs_flipped, list):\n",
    "\t\t\toutput_flipped = outputs_flipped[-1]\n",
    "\t\telse:\n",
    "\t\t\toutput_flipped = outputs_flipped\n",
    "\n",
    "\t\toutput_flipped = transforms.flip_back(output_flipped.cpu().numpy(), dataset.flip_pairs)\n",
    "\t\toutput_flipped = torch.from_numpy(output_flipped.copy()).cuda()\n",
    "\n",
    "\t\toutput = (output + output_flipped) * 0.5\n",
    "\n",
    "\tpreds, maxvals = get_final_preds(cfg, output.clone().cpu().numpy(), None, None, transform_back=False)\n",
    "\n",
    "# from heatmap_coord to original_image_coord\n",
    "query_locations = np.array([p * 4 + 0.5 for p in preds[0]])\n",
    "\n",
    "inspect_atten_map_by_locations(img, model, query_locations, model_name=\"transposer\", mode='dependency', save_img=True, threshold=0.0, outinfo=(OUT_DIR, dataset.getFileName(idx)))\n",
    "\n",
    "cv2.imwrite(OUT_DIR + dataset.getFileName(idx) + \"_original_img.jpg\", dataset.getOriginalImage(idx))\n",
    "'''\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "class PoseClassifier(nn.Module):\n",
    "    def __init__(self, n_class,\n",
    "                 transpose_model,device=device, fine_tune=False, pretrained=True):\n",
    "        super(PoseClassifier, self).__init__()\n",
    "        layers = []\n",
    "        dropout = 0.5\n",
    "        hidden_layers = [128, 512, 512, 512, 512, 256, 128, 128]\n",
    "        self.tph = transpose_model\n",
    "        layers.append(nn.Conv2d(17, 128, 3, padding=1))\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "\n",
    "        #'''\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.conv1 = nn.Conv2d(17, hidden_layers[0], 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(hidden_layers[0])\n",
    "        self.pool1 = nn.MaxPool2d((2, 2), 2)\n",
    "\n",
    "        self.pool2 = nn.MaxPool2d((3, 3), 3)\n",
    "        self.conv2 = nn.Conv2d(hidden_layers[0], hidden_layers[1], 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(hidden_layers[1])\n",
    "        self.conv3 = nn.Conv2d(hidden_layers[1], hidden_layers[2], 3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(hidden_layers[2])\n",
    "        self.conv4 = nn.Conv2d(hidden_layers[2], hidden_layers[3], 3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(hidden_layers[3])\n",
    "        self.conv5 = nn.Conv2d(hidden_layers[3], hidden_layers[4], 3, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(hidden_layers[4])\n",
    "        #self.conv6 = nn.Conv2d(hidden_layers[4], hidden_layers[-1], 3, padding=1)\n",
    "        #self.conv7 = nn.Conv2d(hidden_layers[5], hidden_layers[6], 3, padding=1)\n",
    "        #self.conv8 = nn.Conv2d(hidden_layers[6], hidden_layers[7], 3, padding=1)\n",
    "        #'''\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.lin1 = nn.Linear(hidden_layers[4],hidden_layers[-1])\n",
    "        self.classifier = nn.Linear(hidden_layers[-1],n_class)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.tph(x)\n",
    "        #print(out.size(),\"AFTER TPH\")\n",
    "        out = self.conv1(out)\n",
    "        out = self.bn1(out)\n",
    "        #print(out.size(),\"AFTER CONV1\")\n",
    "        out = self.pool1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        #print(out.size(),\"AFTER POOL\")\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        #print(out.size(),\"AFTER CONV2\")\n",
    "        out = self.pool1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        #print(out.size(),\"AFTER POOL\")\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        #print(out.size(),\"AFTER CONV3\")\n",
    "        out = self.pool2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        #print(out.size(),\"AFTER POOL\")\n",
    "        out = self.conv4(out)\n",
    "        out = self.bn4(out)\n",
    "        #print(out.size(),\"AFTER CONV4\")\n",
    "        out = self.pool2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        #print(out.size(),\"AFTER POOL\")\n",
    "        out = self.conv5(out)\n",
    "        out = self.bn5(out)\n",
    "        #print(out.size(),\"AFTER CONV5\")\n",
    "        #out = self.pool1(out)\n",
    "        out = self.flatten(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        #print(out.size(),\"AFTER POOL\")\n",
    "        out = self.lin1(out)\n",
    "        '''\n",
    "        out = self.pool1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        #print(out.size(),\"AFTER POOL\")\n",
    "        out = self.conv6(out)\n",
    "\n",
    "        #print(out.size(),\"AFTER CONV6\")\n",
    "        '''\n",
    "\n",
    "\n",
    "        out = self.flatten(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        #print(out.size(),\"AFTER FLATTEN\")\n",
    "\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "num_classes = 107\n",
    "model = PoseClassifier(n_class=num_classes, transpose_model=model)\n",
    "num_epochs = 50\n",
    "batch_size = 12\n",
    "learning_rate = 1e-4\n",
    "learning_rate_decay = 0.99\n",
    "params_to_update = model.parameters()\n",
    "\n",
    "\n",
    "def update_lr(optimizer, lr):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "fine_tune = False\n",
    "if fine_tune:\n",
    "    params_to_update = []\n",
    "    for param in model.tph.parameters():\n",
    "        param.requires_grad = False\n",
    "    for p in model.parameters():\n",
    "        if p.requires_grad == True:\n",
    "            params_to_update.append(p)\n",
    "else:\n",
    "    params_to_update = model.parameters()\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params_to_update, lr=learning_rate)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=trainset, batch_size=batch_size, shuffle=False,collate_fn=dataset.collate_fn)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=valset,batch_size=batch_size,shuffle=False,collate_fn=dataset.collate_fn)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=testset,batch_size=batch_size,shuffle=False,collate_fn=dataset.collate_fn)\n",
    "# Train the model\n",
    "lr = learning_rate\n",
    "total_step = len(train_loader)\n",
    "loss_train = []\n",
    "loss_val = []\n",
    "best_accuracy = None\n",
    "accuracy_val = []"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n"
     ]
    },
    {
     "data": {
      "text/plain": "4792"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(train_loader))\n",
    "len(train_loader.dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "params_to_update = model.parameters()\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params_to_update, lr=learning_rate)\n",
    "# Train the model\n",
    "lr = learning_rate\n",
    "total_step = len(train_loader)\n",
    "loss_train = []\n",
    "loss_val = []\n",
    "best_accuracy = None\n",
    "accuracy_val = []\n",
    "#best_model = type(model)(num_classes, fine_tune, pretrained) # get a new instance\n",
    "def train(model,num_epochs=num_epochs,lr=learning_rate):\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        model.train()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        loss_iter = 0\n",
    "\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            # Move tensors to the configured device\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            #print(outputs,labels)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_iter += loss.item()\n",
    "\n",
    "\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                      .format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "\n",
    "        print('Training accuracy is: {} %'.format(accuracy))\n",
    "        loss_train.append(loss_iter / (len(train_loader) * batch_size))\n",
    "\n",
    "        # Code to update the lr\n",
    "        lr *= learning_rate_decay\n",
    "        update_lr(optimizer, lr)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            loss_iter = 0\n",
    "            for images, labels in val_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss_iter += loss.item()\n",
    "            loss_val.append(loss_iter / (len(val_loader) * batch_size))\n",
    "            accuracy = 100 * correct / total\n",
    "            accuracy_val.append(accuracy)\n",
    "            print('Validation accuracy is: {} %'.format(accuracy))\n",
    "            early_stop = False\n",
    "            patience = 3\n",
    "            if epoch > patience - 1:\n",
    "                for j in range(patience - 1):\n",
    "                    if max(accuracy_val) > list(reversed(accuracy_val))[j]:\n",
    "                        if \"not_improving_epochs\" in locals():\n",
    "                            not_improving_epochs += 1\n",
    "                        else:\n",
    "                            not_improving_epochs = 1\n",
    "                        print('Not saving the model')\n",
    "                    else:\n",
    "                        not_improving_epochs = 0\n",
    "                        best_model = model\n",
    "                        print(\"Saving the model\")\n",
    "                        break\n",
    "                    if not_improving_epochs >= patience:\n",
    "                        early_stop = True\n",
    "                        print('Early stopping')\n",
    "                        break\n",
    "                    break\n",
    "\n",
    "    plt.figure(2)\n",
    "    plt.plot(loss_train, 'r', label='Train loss')\n",
    "    plt.plot(loss_val, 'g', label='Val loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(3)\n",
    "    plt.plot(accuracy_val, 'r', label='Val accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def test(model):\n",
    "    with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            loss_iter = 0\n",
    "            for images, labels in test_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss_iter += loss.item()\n",
    "            loss_val.append(loss_iter / (len(val_loader) * batch_size))\n",
    "            accuracy = 100 * correct / total\n",
    "            accuracy_val.append(accuracy)\n",
    "            print('Test accuracy is: {} %'.format(accuracy))\n",
    "            print('Test loss is: {} %'.format(loss_val))\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Step [100/400], Loss: 4.5565\n",
      "Epoch [1/100], Step [200/400], Loss: 4.5901\n",
      "Epoch [1/100], Step [300/400], Loss: 4.6777\n",
      "Epoch [1/100], Step [400/400], Loss: 4.4278\n",
      "Training accuracy is: 1.9198664440734559 %\n",
      "Validation accuracy is: 1.669449081803005 %\n",
      "Epoch [2/100], Step [100/400], Loss: 4.3636\n",
      "Epoch [2/100], Step [200/400], Loss: 4.6377\n",
      "Epoch [2/100], Step [300/400], Loss: 4.1863\n",
      "Epoch [2/100], Step [400/400], Loss: 4.7942\n",
      "Training accuracy is: 2.963272120200334 %\n",
      "Validation accuracy is: 1.335559265442404 %\n",
      "Epoch [3/100], Step [100/400], Loss: 4.0881\n",
      "Epoch [3/100], Step [200/400], Loss: 4.4600\n",
      "Epoch [3/100], Step [300/400], Loss: 4.2646\n",
      "Epoch [3/100], Step [400/400], Loss: 4.0046\n",
      "Training accuracy is: 4.131886477462437 %\n",
      "Validation accuracy is: 4.841402337228715 %\n",
      "Epoch [4/100], Step [100/400], Loss: 3.7219\n",
      "Epoch [4/100], Step [200/400], Loss: 4.2569\n",
      "Epoch [4/100], Step [300/400], Loss: 4.3922\n",
      "Epoch [4/100], Step [400/400], Loss: 4.3557\n",
      "Training accuracy is: 5.029215358931553 %\n",
      "Validation accuracy is: 4.006677796327212 %\n",
      "Not saving the model\n",
      "Epoch [5/100], Step [100/400], Loss: 3.8148\n",
      "Epoch [5/100], Step [200/400], Loss: 4.2922\n",
      "Epoch [5/100], Step [300/400], Loss: 4.0402\n",
      "Epoch [5/100], Step [400/400], Loss: 4.1915\n",
      "Training accuracy is: 6.135225375626043 %\n",
      "Validation accuracy is: 6.510851419031719 %\n",
      "Saving the model\n",
      "Epoch [6/100], Step [100/400], Loss: 3.6050\n",
      "Epoch [6/100], Step [200/400], Loss: 4.1160\n",
      "Epoch [6/100], Step [300/400], Loss: 3.9998\n",
      "Epoch [6/100], Step [400/400], Loss: 3.5806\n",
      "Training accuracy is: 7.53338898163606 %\n",
      "Validation accuracy is: 9.515859766277128 %\n",
      "Saving the model\n",
      "Epoch [7/100], Step [100/400], Loss: 3.5222\n",
      "Epoch [7/100], Step [200/400], Loss: 3.6645\n",
      "Epoch [7/100], Step [300/400], Loss: 3.8955\n",
      "Epoch [7/100], Step [400/400], Loss: 4.1062\n",
      "Training accuracy is: 9.411519198664442 %\n",
      "Validation accuracy is: 13.856427378964941 %\n",
      "Saving the model\n",
      "Epoch [8/100], Step [100/400], Loss: 3.2655\n",
      "Epoch [8/100], Step [200/400], Loss: 3.6307\n",
      "Epoch [8/100], Step [300/400], Loss: 3.6260\n",
      "Epoch [8/100], Step [400/400], Loss: 3.5393\n",
      "Training accuracy is: 12.020033388981636 %\n",
      "Validation accuracy is: 15.525876460767947 %\n",
      "Saving the model\n",
      "Epoch [9/100], Step [100/400], Loss: 3.0840\n",
      "Epoch [9/100], Step [200/400], Loss: 3.5122\n",
      "Epoch [9/100], Step [300/400], Loss: 3.1895\n",
      "Epoch [9/100], Step [400/400], Loss: 3.6350\n",
      "Training accuracy is: 14.273789649415694 %\n",
      "Validation accuracy is: 17.02838063439065 %\n",
      "Saving the model\n",
      "Epoch [10/100], Step [100/400], Loss: 3.0776\n",
      "Epoch [10/100], Step [200/400], Loss: 3.7559\n",
      "Epoch [10/100], Step [300/400], Loss: 3.0470\n",
      "Epoch [10/100], Step [400/400], Loss: 3.8175\n",
      "Training accuracy is: 17.111853088480803 %\n",
      "Validation accuracy is: 22.70450751252087 %\n",
      "Saving the model\n",
      "Epoch [11/100], Step [100/400], Loss: 2.7472\n",
      "Epoch [11/100], Step [200/400], Loss: 3.7020\n",
      "Epoch [11/100], Step [300/400], Loss: 2.8677\n",
      "Epoch [11/100], Step [400/400], Loss: 3.6040\n",
      "Training accuracy is: 20.242070116861434 %\n",
      "Validation accuracy is: 24.707846410684475 %\n",
      "Saving the model\n",
      "Epoch [12/100], Step [100/400], Loss: 2.7734\n",
      "Epoch [12/100], Step [200/400], Loss: 3.5701\n",
      "Epoch [12/100], Step [300/400], Loss: 2.8441\n",
      "Epoch [12/100], Step [400/400], Loss: 3.8852\n",
      "Training accuracy is: 22.224540901502504 %\n",
      "Validation accuracy is: 27.378964941569283 %\n",
      "Saving the model\n",
      "Epoch [13/100], Step [100/400], Loss: 2.4739\n",
      "Epoch [13/100], Step [200/400], Loss: 3.3092\n",
      "Epoch [13/100], Step [300/400], Loss: 2.7270\n",
      "Epoch [13/100], Step [400/400], Loss: 3.4703\n",
      "Training accuracy is: 26.272954924874792 %\n",
      "Validation accuracy is: 29.88313856427379 %\n",
      "Saving the model\n",
      "Epoch [14/100], Step [100/400], Loss: 2.2305\n",
      "Epoch [14/100], Step [200/400], Loss: 2.6808\n",
      "Epoch [14/100], Step [300/400], Loss: 2.0679\n",
      "Epoch [14/100], Step [400/400], Loss: 2.6062\n",
      "Training accuracy is: 28.67278797996661 %\n",
      "Validation accuracy is: 34.5575959933222 %\n",
      "Saving the model\n",
      "Epoch [15/100], Step [100/400], Loss: 2.2047\n",
      "Epoch [15/100], Step [200/400], Loss: 3.2630\n",
      "Epoch [15/100], Step [300/400], Loss: 2.4618\n",
      "Epoch [15/100], Step [400/400], Loss: 2.7593\n",
      "Training accuracy is: 31.26043405676127 %\n",
      "Validation accuracy is: 36.060100166944906 %\n",
      "Saving the model\n",
      "Epoch [16/100], Step [100/400], Loss: 2.1521\n",
      "Epoch [16/100], Step [200/400], Loss: 2.5053\n",
      "Epoch [16/100], Step [300/400], Loss: 2.4518\n",
      "Epoch [16/100], Step [400/400], Loss: 2.4573\n",
      "Training accuracy is: 33.702003338898166 %\n",
      "Validation accuracy is: 39.89983305509182 %\n",
      "Saving the model\n",
      "Epoch [17/100], Step [100/400], Loss: 2.1420\n",
      "Epoch [17/100], Step [200/400], Loss: 2.6742\n",
      "Epoch [17/100], Step [300/400], Loss: 2.2185\n"
     ]
    }
   ],
   "source": [
    "train(model,num_epochs=100,lr=1e-4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#train(model,num_epochs=100,lr=1e-4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}