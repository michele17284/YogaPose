{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numbers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "import os\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import genericpath\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# force working on cpu due to memory limitation\n",
    "#device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "def add_margin(pil_img, top, right, bottom, left, color):\n",
    "    width, height = pil_img.size\n",
    "    new_width = width + right + left\n",
    "    new_height = height + top + bottom\n",
    "    result = Image.new(pil_img.mode, (new_width, new_height), color)\n",
    "    result.paste(pil_img, (left, top))\n",
    "    return result\n",
    "\n",
    "\n",
    "class YogaPoseDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataset_path, size=(256, 192), transform=None, example_images=None):\n",
    "        self.data_path = dataset_path\n",
    "        self.size = size\n",
    "        self.transform = transform\n",
    "        # call to init the data\n",
    "        self._init_data()\n",
    "\n",
    "    def _init_data(self):\n",
    "        images = list()\n",
    "\n",
    "        for _, directory_class in enumerate(os.listdir(self.data_path)):\n",
    "            class_path = os.path.join(self.data_path, directory_class)\n",
    "            for file_name in os.listdir(class_path):\n",
    "                f = cv2.imread(os.path.join(class_path, file_name), cv2.IMREAD_COLOR)\n",
    "                f = cv2.cvtColor(f, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                if self.transform is not None:\n",
    "                    f = self.transform(f)\n",
    "\n",
    "                data = torch.reshape(torch.FloatTensor(f), (3, self.size[0], self.size[1]))\n",
    "\n",
    "                # format example  images[x][0] -> (label, input)\n",
    "                # format example  images[x][1] -> [other information]\n",
    "                # images[x] -> ((class_id, image_tensor), [filename])\n",
    "                images.append((int(directory_class), data))\n",
    "\n",
    "\n",
    "        np.random.shuffle(images)\n",
    "        self.images = images\n",
    "\n",
    "    def __len__(self):\n",
    "        # returns the number of samples in our dataset\n",
    "        return len(self.images)\n",
    "\n",
    "    def getData(self):\n",
    "        return self.images\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            idx: the index of the sample\n",
    "\n",
    "        Returns: a tuple (class, input) for the given sample\n",
    "\n",
    "        \"\"\"\n",
    "        return self.images[idx]\n",
    "\n",
    "    def collate_fn(self, data):\n",
    "        #print(data)\n",
    "        Xs = torch.stack([x[1] for x in data])\n",
    "        y = torch.stack([torch.tensor(x[0]) for x in data])\n",
    "        return Xs, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "DATASET_PATH = './data/images/'\n",
    "ANNOTATION_PATH = './data/annotations/'\n",
    "MODEL_NAME = \"tpr_a4_256x192\"\n",
    "norm_transform = T.Compose([T.ToTensor(), T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]), ])\n",
    "dataset = YogaPoseDataset(DATASET_PATH, transform=norm_transform)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/michele/.cache/torch/hub/yangsenius_TransPose_main\n",
      "/home/michele/.cache/torch/hub/yangsenius_TransPose_main/lib/models/transpose_r.py:333: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  dim_t = temperature ** (2 * (dim_t // 2) / one_direction_feats)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>Load pretrained weights from url: https://github.com/yangsenius/TransPose/releases/download/Hub/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      "Successfully loaded model  (on cpu) with pretrained weights!\n"
     ]
    },
    {
     "data": {
      "text/plain": "TransPoseR(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (3): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (reduce): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n  (global_encoder): TransformerEncoder(\n    (layers): ModuleList(\n      (0): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n        )\n        (linear1): Linear(in_features=256, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=1024, out_features=256, bias=True)\n        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n      (1): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n        )\n        (linear1): Linear(in_features=256, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=1024, out_features=256, bias=True)\n        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n      (2): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n        )\n        (linear1): Linear(in_features=256, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=1024, out_features=256, bias=True)\n        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n      (3): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n        )\n        (linear1): Linear(in_features=256, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=1024, out_features=256, bias=True)\n        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n    )\n  )\n  (deconv_layers): Sequential(\n    (0): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n  )\n  (final_layer): Conv2d(256, 17, kernel_size=(1, 1), stride=(1, 1))\n)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get model from torch hub\n",
    "assert MODEL_NAME in [\"tpr_a4_256x192\", \"tph_a4_256x192\"]\n",
    "\n",
    "transpose_model = torch.hub.load('yangsenius/TransPose:main', MODEL_NAME, pretrained=True)\n",
    "transpose_model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from TransPose.lib.config import cfg\n",
    "from TransPose.lib.utils import transforms\n",
    "from TransPose.lib.core.inference import get_final_preds\n",
    "from TransPose.visualize import inspect_atten_map_by_locations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "\n",
    "train_split_position = (len(dataset) // 10) * 8\n",
    "val_split_position = train_split_position+len(dataset)//10\n",
    "trainset = dataset[:train_split_position]\n",
    "valset = dataset[train_split_position:val_split_position]\n",
    "testset = dataset[val_split_position:]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "OUT_DIR = \"./out/\"\n",
    "idx = 0\n",
    "\n",
    "if not os.path.isdir(OUT_DIR):\n",
    "    os.makedirs(OUT_DIR)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class PoseClassifier(nn.Module):\n",
    "    def __init__(self, n_class,\n",
    "                 transpose_model,device=device, fine_tune=False, pretrained=True):\n",
    "        super(PoseClassifier, self).__init__()\n",
    "        layers = []\n",
    "        dropout = 0.5\n",
    "        hidden_layers = [128, 512, 512, 512, 512, 256, 128, 128]\n",
    "        self.tpr = transpose_model\n",
    "        layers.append(nn.Conv2d(17, 128, 3, padding=1))\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.conv1 = nn.Conv2d(17, hidden_layers[0], 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(hidden_layers[0])\n",
    "        self.pool1 = nn.MaxPool2d((2, 2), 2)\n",
    "\n",
    "        self.pool2 = nn.MaxPool2d((3, 3), 3)\n",
    "        self.conv2 = nn.Conv2d(hidden_layers[0], hidden_layers[1], 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(hidden_layers[1])\n",
    "        self.conv3 = nn.Conv2d(hidden_layers[1], hidden_layers[2], 3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(hidden_layers[2])\n",
    "        self.conv4 = nn.Conv2d(hidden_layers[2], hidden_layers[3], 3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(hidden_layers[3])\n",
    "        self.conv5 = nn.Conv2d(hidden_layers[3], hidden_layers[4], 3, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(hidden_layers[4])\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.lin1 = nn.Linear(hidden_layers[4],hidden_layers[-1])\n",
    "        self.classifier = nn.Linear(hidden_layers[-1],n_class)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.tpr(x)\n",
    "        #print(out.size(),\"AFTER TPH\")\n",
    "        out = self.conv1(out)\n",
    "        out = self.bn1(out)\n",
    "        #print(out.size(),\"AFTER CONV1\")\n",
    "        out = self.pool1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        #print(out.size(),\"AFTER POOL\")\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        #print(out.size(),\"AFTER CONV2\")\n",
    "        out = self.pool1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        #print(out.size(),\"AFTER POOL\")\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        #print(out.size(),\"AFTER CONV3\")\n",
    "        out = self.pool2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        #print(out.size(),\"AFTER POOL\")\n",
    "        out = self.conv4(out)\n",
    "        out = self.bn4(out)\n",
    "        #print(out.size(),\"AFTER CONV4\")\n",
    "        out = self.pool2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        #print(out.size(),\"AFTER POOL\")\n",
    "        out = self.conv5(out)\n",
    "        out = self.bn5(out)\n",
    "        #print(out.size(),\"AFTER CONV5\")\n",
    "        #out = self.pool1(out)\n",
    "        out = self.flatten(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        #print(out.size(),\"AFTER POOL\")\n",
    "        out = self.lin1(out)\n",
    "        '''\n",
    "        out = self.pool1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        #print(out.size(),\"AFTER POOL\")\n",
    "        out = self.conv6(out)\n",
    "\n",
    "        #print(out.size(),\"AFTER CONV6\")\n",
    "        '''\n",
    "\n",
    "\n",
    "        out = self.flatten(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        #print(out.size(),\"AFTER FLATTEN\")\n",
    "\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "num_classes = 107\n",
    "\n",
    "\n",
    "num_epochs = 50\n",
    "batch_size = 12\n",
    "learning_rate = 1e-4\n",
    "learning_rate_decay = 0.99\n",
    "\n",
    "model = PoseClassifier(n_class=num_classes, transpose_model=transpose_model)\n",
    "model.to(device)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=trainset, batch_size=batch_size, shuffle=False,collate_fn=dataset.collate_fn)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=valset,batch_size=batch_size,shuffle=False,collate_fn=dataset.collate_fn)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=testset,batch_size=batch_size,shuffle=False,collate_fn=dataset.collate_fn)\n",
    "\n",
    "\n",
    "fine_tune = False\n",
    "if fine_tune:\n",
    "    params_to_update = []\n",
    "    for param in model.tpr.parameters():\n",
    "        param.requires_grad = False\n",
    "    for p in model.parameters():\n",
    "        if p.requires_grad == True:\n",
    "            params_to_update.append(p)\n",
    "else:\n",
    "    params_to_update = model.parameters()\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params_to_update, lr=learning_rate)\n",
    "\n",
    "def update_lr(optimizer, lr):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n"
     ]
    },
    {
     "data": {
      "text/plain": "4792"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(train_loader))\n",
    "len(train_loader.dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "\n",
    "# Train the model\n",
    "\n",
    "#best_model = type(model)(num_classes, fine_tune, pretrained) # get a new instance\n",
    "def train(model,num_epochs=num_epochs,lr=learning_rate):\n",
    "    total_step = len(train_loader)\n",
    "    loss_train = []\n",
    "    loss_val = []\n",
    "    best_accuracy = None\n",
    "    accuracy_val = []\n",
    "    accuracy_test = []\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        model.train()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        loss_iter = 0\n",
    "\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            # Move tensors to the configured device\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            #print(outputs,labels)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_iter += loss.item()\n",
    "\n",
    "\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                      .format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        accuracy_test.append(accuracy)\n",
    "        print('Training accuracy is: {} %'.format(accuracy))\n",
    "        loss_train.append(loss_iter / (len(train_loader) * batch_size))\n",
    "\n",
    "        # Code to update the lr\n",
    "        lr *= learning_rate_decay\n",
    "        update_lr(optimizer, lr)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            loss_iter = 0\n",
    "            for images, labels in val_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss_iter += loss.item()\n",
    "            loss_val.append(loss_iter / (len(val_loader) * batch_size))\n",
    "            accuracy = 100 * correct / total\n",
    "            accuracy_val.append(accuracy)\n",
    "            print('Validation accuracy is: {} %'.format(accuracy))\n",
    "            early_stop = False\n",
    "            patience = 3\n",
    "            if epoch > patience - 1:\n",
    "                for j in range(patience - 1):\n",
    "                    if max(accuracy_val) > list(reversed(accuracy_val))[j]:\n",
    "                        if \"not_improving_epochs\" in locals():\n",
    "                            not_improving_epochs += 1\n",
    "                        else:\n",
    "                            not_improving_epochs = 1\n",
    "                        print('Not saving the model')\n",
    "                    else:\n",
    "                        not_improving_epochs = 0\n",
    "                        best_model = model\n",
    "                        print(\"Saving the model\")\n",
    "                        break\n",
    "                    if not_improving_epochs >= patience:\n",
    "                        early_stop = True\n",
    "                        print('Early stopping')\n",
    "                        break\n",
    "                    break\n",
    "\n",
    "    plt.figure(2)\n",
    "    plt.plot(loss_train, 'r', label='Train loss')\n",
    "    plt.plot(loss_val, 'g', label='Val loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(3)\n",
    "    plt.plot(accuracy_val, 'r', label='Val accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return {\"Loss/train\": loss_train, \"Loss/val\": loss_val, \"Accuracy/train\":accuracy_val, \"Accuracy/val\": accuracy_test }\n",
    "\n",
    "def test(model):\n",
    "    with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            loss_iter = 0\n",
    "            for images, labels in test_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss_iter += loss.item()\n",
    "            loss_test =(loss_iter / (len(test_loader) * batch_size))\n",
    "            accuracy = 100 * correct / total\n",
    "            print('Test accuracy is: {} %'.format(accuracy))\n",
    "            print('Test Loss: {:.4f}'.format(loss_test))\n",
    "    return {\"Accuracy/test\": accuracy, \"Loss/test\": loss_test}\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Step [100/400], Loss: 4.6517\n",
      "Epoch [1/100], Step [200/400], Loss: 4.5483\n",
      "Epoch [1/100], Step [300/400], Loss: 4.6301\n",
      "Epoch [1/100], Step [400/400], Loss: 4.9565\n",
      "Training accuracy is: 1.523372287145242 %\n",
      "Validation accuracy is: 1.8363939899833055 %\n",
      "Epoch [2/100], Step [100/400], Loss: 4.6122\n",
      "Epoch [2/100], Step [200/400], Loss: 4.2989\n",
      "Epoch [2/100], Step [300/400], Loss: 4.6980\n",
      "Epoch [2/100], Step [400/400], Loss: 4.6074\n",
      "Training accuracy is: 2.5667779632721204 %\n",
      "Validation accuracy is: 2.1702838063439067 %\n",
      "Epoch [3/100], Step [100/400], Loss: 4.5088\n",
      "Epoch [3/100], Step [200/400], Loss: 4.2878\n",
      "Epoch [3/100], Step [300/400], Loss: 4.5418\n",
      "Epoch [3/100], Step [400/400], Loss: 4.8617\n",
      "Training accuracy is: 2.9841402337228713 %\n",
      "Validation accuracy is: 2.671118530884808 %\n",
      "Epoch [4/100], Step [100/400], Loss: 4.2947\n",
      "Epoch [4/100], Step [200/400], Loss: 3.9908\n",
      "Epoch [4/100], Step [300/400], Loss: 4.7316\n",
      "Epoch [4/100], Step [400/400], Loss: 4.3363\n",
      "Training accuracy is: 4.048414023372287 %\n",
      "Validation accuracy is: 3.5058430717863107 %\n",
      "Saving the model\n",
      "Epoch [5/100], Step [100/400], Loss: 4.1081\n",
      "Epoch [5/100], Step [200/400], Loss: 3.8807\n",
      "Epoch [5/100], Step [300/400], Loss: 4.5963\n",
      "Epoch [5/100], Step [400/400], Loss: 4.2250\n",
      "Training accuracy is: 4.841402337228715 %\n",
      "Validation accuracy is: 4.674457429048414 %\n",
      "Saving the model\n",
      "Epoch [6/100], Step [100/400], Loss: 3.9488\n",
      "Epoch [6/100], Step [200/400], Loss: 4.0432\n",
      "Epoch [6/100], Step [300/400], Loss: 4.3682\n",
      "Epoch [6/100], Step [400/400], Loss: 4.5948\n",
      "Training accuracy is: 5.989148580968281 %\n",
      "Validation accuracy is: 4.340567612687813 %\n",
      "Not saving the model\n",
      "Epoch [7/100], Step [100/400], Loss: 4.0524\n",
      "Epoch [7/100], Step [200/400], Loss: 3.3647\n",
      "Epoch [7/100], Step [300/400], Loss: 3.7307\n",
      "Epoch [7/100], Step [400/400], Loss: 4.7888\n",
      "Training accuracy is: 7.721202003338898 %\n",
      "Validation accuracy is: 10.851419031719532 %\n",
      "Saving the model\n",
      "Epoch [8/100], Step [100/400], Loss: 3.8834\n",
      "Epoch [8/100], Step [200/400], Loss: 3.3764\n",
      "Epoch [8/100], Step [300/400], Loss: 3.8639\n",
      "Epoch [8/100], Step [400/400], Loss: 4.2457\n",
      "Training accuracy is: 9.557595993322204 %\n",
      "Validation accuracy is: 12.020033388981636 %\n",
      "Saving the model\n",
      "Epoch [9/100], Step [100/400], Loss: 3.7030\n",
      "Epoch [9/100], Step [200/400], Loss: 3.1276\n",
      "Epoch [9/100], Step [300/400], Loss: 3.6525\n",
      "Epoch [9/100], Step [400/400], Loss: 3.9672\n",
      "Training accuracy is: 11.999165275459099 %\n",
      "Validation accuracy is: 12.020033388981636 %\n",
      "Saving the model\n",
      "Epoch [10/100], Step [100/400], Loss: 3.5063\n",
      "Epoch [10/100], Step [200/400], Loss: 2.8041\n",
      "Epoch [10/100], Step [300/400], Loss: 3.1625\n",
      "Epoch [10/100], Step [400/400], Loss: 4.2133\n",
      "Training accuracy is: 13.647746243739565 %\n",
      "Validation accuracy is: 16.026711185308848 %\n",
      "Saving the model\n",
      "Epoch [11/100], Step [100/400], Loss: 3.4135\n",
      "Epoch [11/100], Step [200/400], Loss: 2.6689\n",
      "Epoch [11/100], Step [300/400], Loss: 3.4954\n",
      "Epoch [11/100], Step [400/400], Loss: 4.1787\n",
      "Training accuracy is: 16.485809682804675 %\n",
      "Validation accuracy is: 20.03338898163606 %\n",
      "Saving the model\n",
      "Epoch [12/100], Step [100/400], Loss: 3.0619\n",
      "Epoch [12/100], Step [200/400], Loss: 2.6938\n",
      "Epoch [12/100], Step [300/400], Loss: 3.4383\n",
      "Epoch [12/100], Step [400/400], Loss: 4.0952\n",
      "Training accuracy is: 19.24040066777963 %\n",
      "Validation accuracy is: 21.368948247078464 %\n",
      "Saving the model\n",
      "Epoch [13/100], Step [100/400], Loss: 2.7120\n",
      "Epoch [13/100], Step [200/400], Loss: 2.3456\n",
      "Epoch [13/100], Step [300/400], Loss: 3.3428\n",
      "Epoch [13/100], Step [400/400], Loss: 3.7492\n",
      "Training accuracy is: 21.84891485809683 %\n",
      "Validation accuracy is: 25.375626043405678 %\n",
      "Saving the model\n",
      "Epoch [14/100], Step [100/400], Loss: 2.7526\n",
      "Epoch [14/100], Step [200/400], Loss: 2.6391\n",
      "Epoch [14/100], Step [300/400], Loss: 2.8813\n",
      "Epoch [14/100], Step [400/400], Loss: 4.6357\n",
      "Training accuracy is: 24.436560934891485 %\n",
      "Validation accuracy is: 30.884808013355592 %\n",
      "Saving the model\n",
      "Epoch [15/100], Step [100/400], Loss: 3.0206\n",
      "Epoch [15/100], Step [200/400], Loss: 2.2588\n",
      "Epoch [15/100], Step [300/400], Loss: 2.7161\n",
      "Epoch [15/100], Step [400/400], Loss: 4.5928\n",
      "Training accuracy is: 27.712854757929883 %\n",
      "Validation accuracy is: 34.0567612687813 %\n",
      "Saving the model\n",
      "Epoch [16/100], Step [100/400], Loss: 2.6938\n",
      "Epoch [16/100], Step [200/400], Loss: 1.8315\n",
      "Epoch [16/100], Step [300/400], Loss: 2.7652\n",
      "Epoch [16/100], Step [400/400], Loss: 4.9738\n",
      "Training accuracy is: 29.862270450751254 %\n",
      "Validation accuracy is: 35.72621035058431 %\n",
      "Saving the model\n",
      "Epoch [17/100], Step [100/400], Loss: 1.9493\n",
      "Epoch [17/100], Step [200/400], Loss: 2.0284\n",
      "Epoch [17/100], Step [300/400], Loss: 2.8556\n",
      "Epoch [17/100], Step [400/400], Loss: 3.9685\n",
      "Training accuracy is: 32.5542570951586 %\n",
      "Validation accuracy is: 34.390651085141904 %\n",
      "Not saving the model\n",
      "Epoch [18/100], Step [100/400], Loss: 1.7576\n",
      "Epoch [18/100], Step [200/400], Loss: 1.9425\n",
      "Epoch [18/100], Step [300/400], Loss: 3.0182\n",
      "Epoch [18/100], Step [400/400], Loss: 4.1169\n",
      "Training accuracy is: 33.806343906510854 %\n",
      "Validation accuracy is: 41.068447412353926 %\n",
      "Saving the model\n",
      "Epoch [19/100], Step [100/400], Loss: 2.4298\n",
      "Epoch [19/100], Step [200/400], Loss: 1.8015\n",
      "Epoch [19/100], Step [300/400], Loss: 2.6369\n",
      "Epoch [19/100], Step [400/400], Loss: 4.3886\n",
      "Training accuracy is: 37.02003338898164 %\n",
      "Validation accuracy is: 39.39899833055092 %\n",
      "Not saving the model\n",
      "Epoch [20/100], Step [100/400], Loss: 1.7251\n",
      "Epoch [20/100], Step [200/400], Loss: 2.1181\n",
      "Epoch [20/100], Step [300/400], Loss: 2.8901\n",
      "Epoch [20/100], Step [400/400], Loss: 5.2108\n",
      "Training accuracy is: 38.939899833055094 %\n",
      "Validation accuracy is: 38.73121869782972 %\n",
      "Not saving the model\n",
      "Epoch [21/100], Step [100/400], Loss: 1.6103\n",
      "Epoch [21/100], Step [200/400], Loss: 2.0415\n",
      "Epoch [21/100], Step [300/400], Loss: 2.5905\n",
      "Epoch [21/100], Step [400/400], Loss: 4.2354\n",
      "Training accuracy is: 41.235392320534224 %\n",
      "Validation accuracy is: 45.075125208681136 %\n",
      "Saving the model\n",
      "Epoch [22/100], Step [100/400], Loss: 1.8653\n",
      "Epoch [22/100], Step [200/400], Loss: 1.4645\n",
      "Epoch [22/100], Step [300/400], Loss: 2.4265\n",
      "Epoch [22/100], Step [400/400], Loss: 3.5810\n",
      "Training accuracy is: 43.447412353923205 %\n",
      "Validation accuracy is: 41.736227045075125 %\n",
      "Not saving the model\n",
      "Epoch [23/100], Step [100/400], Loss: 1.9341\n",
      "Epoch [23/100], Step [200/400], Loss: 1.3819\n",
      "Epoch [23/100], Step [300/400], Loss: 2.4518\n",
      "Epoch [23/100], Step [400/400], Loss: 4.2606\n",
      "Training accuracy is: 44.99165275459099 %\n",
      "Validation accuracy is: 48.91485809682805 %\n",
      "Saving the model\n",
      "Epoch [24/100], Step [100/400], Loss: 1.7759\n",
      "Epoch [24/100], Step [200/400], Loss: 2.3857\n",
      "Epoch [24/100], Step [300/400], Loss: 2.1074\n",
      "Epoch [24/100], Step [400/400], Loss: 2.9541\n",
      "Training accuracy is: 48.22621035058431 %\n",
      "Validation accuracy is: 47.74624373956594 %\n",
      "Not saving the model\n",
      "Epoch [25/100], Step [100/400], Loss: 1.4174\n",
      "Epoch [25/100], Step [200/400], Loss: 0.9676\n",
      "Epoch [25/100], Step [300/400], Loss: 1.8113\n",
      "Epoch [25/100], Step [400/400], Loss: 3.2060\n",
      "Training accuracy is: 49.08180300500835 %\n",
      "Validation accuracy is: 47.74624373956594 %\n",
      "Not saving the model\n",
      "Epoch [26/100], Step [100/400], Loss: 1.3741\n",
      "Epoch [26/100], Step [200/400], Loss: 1.6474\n",
      "Epoch [26/100], Step [300/400], Loss: 1.9158\n",
      "Epoch [26/100], Step [400/400], Loss: 3.3141\n",
      "Training accuracy is: 51.02253756260434 %\n",
      "Validation accuracy is: 51.08514190317195 %\n",
      "Saving the model\n",
      "Epoch [27/100], Step [100/400], Loss: 1.7162\n",
      "Epoch [27/100], Step [200/400], Loss: 0.8557\n",
      "Epoch [27/100], Step [300/400], Loss: 2.0729\n",
      "Epoch [27/100], Step [400/400], Loss: 3.0785\n",
      "Training accuracy is: 53.38063439065108 %\n",
      "Validation accuracy is: 52.42070116861436 %\n",
      "Saving the model\n",
      "Epoch [28/100], Step [100/400], Loss: 1.6523\n",
      "Epoch [28/100], Step [200/400], Loss: 1.3808\n",
      "Epoch [28/100], Step [300/400], Loss: 1.9880\n",
      "Epoch [28/100], Step [400/400], Loss: 4.6113\n",
      "Training accuracy is: 54.98747913188648 %\n",
      "Validation accuracy is: 50.08347245409015 %\n",
      "Not saving the model\n",
      "Epoch [29/100], Step [100/400], Loss: 1.0981\n",
      "Epoch [29/100], Step [200/400], Loss: 1.1394\n",
      "Epoch [29/100], Step [300/400], Loss: 1.9512\n"
     ]
    }
   ],
   "source": [
    "train_values = train(model,num_epochs=100,lr=1e-4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_values = test(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def TransPoseVisualizer(exampledataset, out_path, out_index):\n",
    "    import shutil\n",
    "    activation = {}\n",
    "    def get_activation(name):\n",
    "        def hook(model, input, output):\n",
    "            activation[name] = output.detach()\n",
    "        return hook\n",
    "    model.tph.register_forward_hook(get_activation('tph'))\n",
    "\n",
    "    examples_dataloader = torch.utils.data.DataLoader(dataset=exampledataset, batch_size=batch_size, shuffle=False,collate_fn=dataset.collate_fn)\n",
    "\n",
    "    for index, (images,_) in enumerate(examples_dataloader):\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            images = images.to(device)\n",
    "            model(images)\n",
    "            outputs = activation['tph']\n",
    "            if isinstance(outputs, list):\n",
    "                output = outputs[-1]\n",
    "            else:\n",
    "                output = outputs\n",
    "\n",
    "            preds, maxvals = get_final_preds(cfg, output.clone().cpu().numpy(), None, None, transform_back=False)\n",
    "\n",
    "        # from heatmap_coord to original_image_coord\n",
    "        query_locations = np.array([p * 4 + 0.5 for p in preds[0]])\n",
    "\n",
    "        inspect_atten_map_by_locations(exampledataset[index][1], model.tph , query_locations, model_name=\"transposer\", mode='dependency', save_img=True, threshold=0.005, outinfo=(out_path , str(out_index)), device=device)\n",
    "\n",
    "def get_model_code():\n",
    "    import inspect, sys\n",
    "    from IPython.core.magics.code import extract_symbols\n",
    "\n",
    "    def new_getfile(object, _old_getfile=inspect.getfile):\n",
    "        if not inspect.isclass(object):\n",
    "            return _old_getfile(object)\n",
    "\n",
    "        # Lookup by parent module (as in current inspect)\n",
    "        if hasattr(object, '__module__'):\n",
    "            object_ = sys.modules.get(object.__module__)\n",
    "            if hasattr(object_, '__file__'):\n",
    "                return object_.__file__\n",
    "\n",
    "        # If parent module is __main__, lookup by methods (NEW)\n",
    "        for name, member in inspect.getmembers(object):\n",
    "            if inspect.isfunction(member) and object.__qualname__ + '.' + member.__name__ == member.__qualname__:\n",
    "                return inspect.getfile(member)\n",
    "        else:\n",
    "            raise TypeError('Source for {!r} not found'.format(object))\n",
    "\n",
    "    inspect.getfile = new_getfile\n",
    "    obj = PoseClassifier\n",
    "    cell_code = \"\".join(inspect.linecache.getlines(new_getfile(obj)))\n",
    "    class_code = extract_symbols(cell_code, obj.__name__)[0][0]\n",
    "    return class_code\n",
    "\n",
    "def save_run(train_dict = None, test_dict= None, save_examples = False, ):\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "    import numbers, json\n",
    "    def get_next_logdir():\n",
    "        from datetime import datetime\n",
    "        now = datetime.now()\n",
    "        date_time = now.strftime(\"%d_%b[%H-%M-%S]\")\n",
    "        multi_val_index = 1\n",
    "        logs_dir = OUT_DIR + \"logs/\"\n",
    "        dir_name = logs_dir + date_time\n",
    "        while os.path.exists(dir_name):\n",
    "            multi_val_index += 1\n",
    "            dir_name = \"\".join([logs_dir, date_time, \"(\", str(multi_val_index), \")\"])\n",
    "        return dir_name\n",
    "\n",
    "    def save_log(value_dict):\n",
    "        for key,value in value_dict.items():\n",
    "            if isinstance(value, numbers.Number):\n",
    "                writer.add_scalar(key, value, 0)\n",
    "            else:\n",
    "                for e, e_value in enumerate(value):\n",
    "                    writer.add_scalar(key, e_value, e)\n",
    "\n",
    "    dir_name = get_next_logdir()\n",
    "    model_path = os.path.join(dir_name, \"model.ckpt\")\n",
    "    model_json_path = os.path.join(dir_name, \"model_params.json\")\n",
    "    model_classcode_path = os.path.join(dir_name, \"model_class_code.txt\")\n",
    "    example_images_path = os.path.join(dir_name, \"example\")\n",
    "    writer = SummaryWriter(log_dir=dir_name)\n",
    "    model_dict = { \"params\": {\"num_classes\": num_classes, \"num_epochs\": num_epochs, \"batch_size\": batch_size, \"learning_rate\": learning_rate, \"learning_rate_decay\": learning_rate_decay, \"criterion\": str(criterion.__class__), \"optimizer\": str(optimizer.__class__)}}\n",
    "\n",
    "\n",
    "    print(\"Saving {} in {}\".format(\"model\", model_path), end=\" \")\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(\"DONE\")\n",
    "\n",
    "\n",
    "    print(\"Saving {} in {}\".format(\"Logs (if any)\", dir_name), end=\" \")\n",
    "    if train_dict is not None: save_log(train_dict)\n",
    "    if test_dict is not None:save_log(test_dict)\n",
    "    writer.close()\n",
    "    print(\"DONE\")\n",
    "\n",
    "    print(\"Saving {} in {}\".format(\"parameters\", model_json_path), end=\" \")\n",
    "    with open(model_json_path, \"w\") as jf:\n",
    "        json.dump(model_dict, jf)\n",
    "    print(\"DONE\")\n",
    "\n",
    "    print(\"Saving {} in {}\".format(\"class code\", model_classcode_path), end=\" \")\n",
    "    with open(model_classcode_path, \"w\") as f:\n",
    "        f.write(get_model_code())\n",
    "    print(\"DONE\")\n",
    "\n",
    "    if save_examples:\n",
    "        EXAMPLE_DIR = \"./data/examples/\"\n",
    "        if not os.path.exists(EXAMPLE_DIR):\n",
    "            print(\"{} path does not exist. Be sure insert in the path the images that you want to try. Follow the istruction in the README.md file\".format(EXAMPLE_DIR))\n",
    "        else:\n",
    "            print(\"Saving {} in {}\".format(\"Example Images\", example_images_path), end=\" \")\n",
    "            if not os.path.exists(example_images_path): os.makedirs(example_images_path)\n",
    "\n",
    "            example_dataset = YogaPoseDataset(EXAMPLE_DIR, transform=norm_transform)\n",
    "            example_dataset1 = example_dataset[0:1]\n",
    "            example_dataset2 = example_dataset[1:2]\n",
    "            example_dataset3 = example_dataset[2:3]\n",
    "\n",
    "            TransPoseVisualizer(example_dataset1, example_images_path, 1)\n",
    "            TransPoseVisualizer(example_dataset2, example_images_path, 2)\n",
    "            TransPoseVisualizer(example_dataset3, example_images_path, 3)\n",
    "\n",
    "            print(\"DONE\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "save_run(train_values, test_values, save_examples=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}