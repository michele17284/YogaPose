{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>Load pretrained weights from url: https://github.com/yangsenius/TransPose/releases/download/Hub/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      "Successfully loaded model  (on cpu) with pretrained weights!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/michele/.cache/torch/hub/yangsenius_TransPose_main\n",
      "/home/michele/.cache/torch/hub/yangsenius_TransPose_main/lib/models/transpose_r.py:333: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  dim_t = temperature ** (2 * (dim_t // 2) / one_direction_feats)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "import os\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")            #comment this line to run with GPU\n",
    "tpr = torch.hub.load('yangsenius/TransPose:main', 'tpr_a4_256x192', pretrained=True)\n",
    "#tph = torch.hub.load('yangsenius/TransPose:main', 'tph_a4_256x192', pretrained=True, device=device)\n",
    "\n",
    "\n",
    "#print(tph)\n",
    "DATASET_PATH = './dataset/'\n",
    "positions = os.listdir(DATASET_PATH)\n",
    "#print(os.listdir(DATASET_PATH+entries[0]))\n",
    "images = list()\n",
    "\n",
    "def add_margin(pil_img, top, right, bottom, left, color):\n",
    "    width, height = pil_img.size\n",
    "    new_width = width + right + left\n",
    "    new_height = height + top + bottom\n",
    "    result = Image.new(pil_img.mode, (new_width, new_height), color)\n",
    "    result.paste(pil_img, (left, top))\n",
    "    return result\n",
    "\n",
    "class YogaPoseDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataset_path):\n",
    "        self.data_path = dataset_path\n",
    "        # call to init the data\n",
    "        self.size = 256,192\n",
    "        self._init_data()\n",
    "\n",
    "    def _init_data(self):\n",
    "        positions = os.listdir(self.data_path)\n",
    "        images = list()\n",
    "        labels = list()\n",
    "        for idx, position in enumerate(positions):\n",
    "            for file in os.listdir(DATASET_PATH + position):\n",
    "                if not file.endswith('.gif'):\n",
    "                    f = cv2.imread(DATASET_PATH + position + '/' + file,cv2.IMREAD_COLOR)\n",
    "\n",
    "                    #print(position + '/' + file)\n",
    "                    #print(f.shape)\n",
    "                    height,width, channels = f.shape\n",
    "                    if height > width:\n",
    "                        scale_percent = 256/height\n",
    "                        f = cv2.resize(f,(int(width*scale_percent),256))\n",
    "                        if f.shape[1] > 192:\n",
    "                            scale_percent = 192/width\n",
    "                            f = cv2.resize(f,(192,int(height*scale_percent)))\n",
    "                    else:\n",
    "                        scale_percent = 192/width\n",
    "                        f = cv2.resize(f,(192,int(height*scale_percent)))\n",
    "                        if f.shape[0] > 256:\n",
    "                            scale_percent = 256/height\n",
    "                            f = cv2.resize(f,(int(width*scale_percent),256))\n",
    "\n",
    "                    height,width, channels = f.shape\n",
    "                    #print(f.shape,\"after resize\")\n",
    "                    f = cv2.copyMakeBorder(f, 256-height, 0, 192-width, 0, cv2.BORDER_CONSTANT,value=0)\n",
    "                    #print(f.shape,\"after padding\")\n",
    "                    #if torch.FloatTensor(f.getdata()).size()[1] != 3: f.show()\n",
    "                    data = torch.reshape(torch.FloatTensor(f).to(device),(3,256,192))\n",
    "                    #print(data.shape)\n",
    "                    #print(data.size())\n",
    "                    ##images.append(data)\n",
    "                    ##labels.append(torch.tensor(idx))\n",
    "                    images.append((idx,data))\n",
    "\n",
    "\n",
    "        #print(images)\n",
    "        ##images = torch.stack(images)\n",
    "        ##labels = torch.stack(labels)\n",
    "        #print(labels)\n",
    "        #print(images.size())\n",
    "        ##mask = np.arange(labels.size()[0])\n",
    "        np.random.shuffle(images)\n",
    "        self.images = images\n",
    "        #print(type(images),type(images[0]))\n",
    "        ##self.images = images[mask]\n",
    "        ##self.labels = labels[mask]\n",
    "        #print(self.labels.size())\n",
    "        #print(self.labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        # returns the number of samples in our dataset\n",
    "        return len(self.images)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def getData(self):\n",
    "        return self.images\n",
    "\n",
    "    #def getY(self):\n",
    "    #    return self.labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # returns the idx-th sample\n",
    "        return self.images[idx]\n",
    "\n",
    "    def collate_fn(self,data):\n",
    "        #print(len(data),data)\n",
    "        #print(data[0].size())\n",
    "        #print(data[1].size())\n",
    "        #print('lwwwwwwwwwwwwwww',[x[0].size() for x in data])\n",
    "        #print(torch.cat([x[0] for x in data]).size(),data[0][0].size())\n",
    "        #print(torch.stack([x[1] for x in data]).size(),data[0][1],[x[1] for x in data])\n",
    "        Xs = torch.stack([x[1] for x in data])\n",
    "        print(Xs.size())\n",
    "        y = torch.stack([torch.tensor(x[0]) for x in data])\n",
    "        print(y.size())\n",
    "        return Xs,y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: Ignoring incorrect cHRM white(.34575,.35855) r(.6485,.33088)g(.32121,.59787)b(.15589,.06604) when sRGB is also present\n",
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = YogaPoseDataset(DATASET_PATH)\n",
    "split_position = int((len(dataset)//10)*7)\n",
    "trainset = dataset[:split_position]\n",
    "testset = dataset[split_position:]\n",
    "print(trainset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "(84,\n tensor([[[  0.,   0.,   0.,  ..., 194., 194., 194.],\n          [200., 200., 200.,  ...,  63.,  63.,  63.],\n          [ 50.,  50.,  50.,  ..., 180., 180., 180.],\n          ...,\n          [107., 107., 107.,  ..., 202., 202., 202.],\n          [207., 207., 207.,  ..., 180., 180., 180.],\n          [  0.,   0.,   0.,  ...,  93.,  93.,  93.]],\n \n         [[170., 170., 170.,  ..., 189., 189., 189.],\n          [193., 193., 193.,  ..., 198., 198., 198.],\n          [  0.,   0.,   0.,  ..., 157., 157., 157.],\n          ...,\n          [ 84.,  84.,  84.,  ..., 187., 187., 187.],\n          [  0.,   0.,   0.,  ..., 160., 160., 160.],\n          [154., 154., 154.,  ..., 108., 108., 108.]],\n \n         [[ 55.,  55.,  55.,  ..., 188., 188., 188.],\n          [  0.,   0.,   0.,  ..., 152., 152., 152.],\n          [137., 137., 137.,  ...,  79.,  79.,  79.],\n          ...,\n          [  0.,   0.,   0.,  ...,  99.,  99.,  99.],\n          [ 81.,  81.,  81.,  ..., 148., 148., 148.],\n          [120., 120., 120.,  ..., 165., 165., 165.]]], device='cuda:0'))"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class PoseClassifier(nn.Module):\n",
    "    def __init__(self, n_class, fine_tune=False, pretrained=True):\n",
    "        super(PoseClassifier, self).__init__()\n",
    "\n",
    "        self.tph = torch.hub.load('yangsenius/TransPose:main', 'tph_a4_256x192', pretrained=True, device=device)\n",
    "        self.fc1 = nn.Linear(52224,1000).to(device)\n",
    "        self.fc2 = nn.Linear(1000,n_class).to(device)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = x.to(device)\n",
    "        out = self.tph(x)\n",
    "        out = torch.reshape(out,(out.size(0),-1))\n",
    "        print(out.size())\n",
    "        out = self.fc1(out)\n",
    "        #m = nn.BatchNorm1d(1000,device=device)\n",
    "        #out = m(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>Load pretrained weights from url: https://github.com/yangsenius/TransPose/releases/download/Hub/tp_h_48_256x192_enc4_d96_h192_mh1.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/michele/.cache/torch/hub/yangsenius_TransPose_main\n",
      "/home/michele/.cache/torch/hub/yangsenius_TransPose_main/lib/models/transpose_h.py:514: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  dim_t = temperature ** (2 * (dim_t // 2) / one_direction_feats)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded model  (on cpu) with pretrained weights!\n"
     ]
    }
   ],
   "source": [
    "num_classes = 107\n",
    "model = PoseClassifier(n_class=num_classes)\n",
    "num_epochs = 30\n",
    "batch_size = 32\n",
    "learning_rate = 1e-3\n",
    "learning_rate_decay = 0.99\n",
    "params_to_update = model.parameters()\n",
    "def update_lr(optimizer, lr):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "params_to_update = model.parameters()\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params_to_update, lr=learning_rate)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=trainset,batch_size=batch_size,shuffle=False,collate_fn=dataset.collate_fn)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=testset,batch_size=batch_size,shuffle=False,collate_fn=dataset.collate_fn)\n",
    "# Train the model\n",
    "lr = learning_rate\n",
    "total_step = len(train_loader)\n",
    "loss_train = []\n",
    "loss_val = []\n",
    "best_accuracy = None\n",
    "accuracy_val = []"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 256, 192])\n",
      "torch.Size([5])\n",
      "tensor([[[[  0.,   0.,   0.,  ..., 194., 194., 194.],\n",
      "          [200., 200., 200.,  ...,  63.,  63.,  63.],\n",
      "          [ 50.,  50.,  50.,  ..., 180., 180., 180.],\n",
      "          ...,\n",
      "          [107., 107., 107.,  ..., 202., 202., 202.],\n",
      "          [207., 207., 207.,  ..., 180., 180., 180.],\n",
      "          [  0.,   0.,   0.,  ...,  93.,  93.,  93.]],\n",
      "\n",
      "         [[170., 170., 170.,  ..., 189., 189., 189.],\n",
      "          [193., 193., 193.,  ..., 198., 198., 198.],\n",
      "          [  0.,   0.,   0.,  ..., 157., 157., 157.],\n",
      "          ...,\n",
      "          [ 84.,  84.,  84.,  ..., 187., 187., 187.],\n",
      "          [  0.,   0.,   0.,  ..., 160., 160., 160.],\n",
      "          [154., 154., 154.,  ..., 108., 108., 108.]],\n",
      "\n",
      "         [[ 55.,  55.,  55.,  ..., 188., 188., 188.],\n",
      "          [  0.,   0.,   0.,  ..., 152., 152., 152.],\n",
      "          [137., 137., 137.,  ...,  79.,  79.,  79.],\n",
      "          ...,\n",
      "          [  0.,   0.,   0.,  ...,  99.,  99.,  99.],\n",
      "          [ 81.,  81.,  81.,  ..., 148., 148., 148.],\n",
      "          [120., 120., 120.,  ..., 165., 165., 165.]]],\n",
      "\n",
      "\n",
      "        [[[  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          ...,\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.]],\n",
      "\n",
      "         [[  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          ...,\n",
      "          [  0.,   0.,   0.,  ..., 184., 187., 192.],\n",
      "          [153., 159., 164.,  ..., 107., 117., 121.],\n",
      "          [108., 117., 121.,  ...,   0.,   0.,   0.]],\n",
      "\n",
      "         [[  0.,   0.,   0.,  ..., 184., 187., 192.],\n",
      "          [154., 160., 165.,  ..., 107., 118., 122.],\n",
      "          [109., 118., 121.,  ...,   0.,   0.,   0.],\n",
      "          ...,\n",
      "          [ 37.,  49.,  51.,  ...,  34.,  45.,  49.],\n",
      "          [ 32.,  44.,  46.,  ...,  34.,  45.,  49.],\n",
      "          [ 35.,  46.,  50.,  ...,  73.,  84.,  88.]]],\n",
      "\n",
      "\n",
      "        [[[  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          ...,\n",
      "          [247., 249., 249.,  ...,  36.,  37.,  44.],\n",
      "          [ 40.,  42.,  49.,  ..., 189., 207., 221.],\n",
      "          [239., 242., 246.,  ..., 248., 251., 251.]],\n",
      "\n",
      "         [[246., 248., 248.,  ...,  33.,  32.,  34.],\n",
      "          [ 33.,  33.,  33.,  ..., 189., 208., 220.],\n",
      "          [228., 232., 236.,  ..., 248., 250., 250.],\n",
      "          ...,\n",
      "          [ 82., 114., 168.,  ..., 231., 236., 237.],\n",
      "          [253., 253., 253.,  ..., 122.,  83.,  19.],\n",
      "          [121.,  81.,  18.,  ...,  78., 108., 164.]],\n",
      "\n",
      "         [[ 82., 114., 167.,  ..., 232., 237., 238.],\n",
      "          [253., 253., 253.,  ..., 125.,  84.,  21.],\n",
      "          [121.,  82.,  19.,  ...,  83., 114., 169.],\n",
      "          ...,\n",
      "          [249., 250., 247.,  ..., 244., 241., 236.],\n",
      "          [242., 238., 233.,  ..., 217., 220., 240.],\n",
      "          [151., 151., 118.,  ..., 231., 232., 230.]]],\n",
      "\n",
      "\n",
      "        [[[  0.,   0.,   0.,  ..., 254., 254., 254.],\n",
      "          [254., 254., 254.,  ..., 255., 255., 255.],\n",
      "          [255., 255., 255.,  ..., 255., 255., 255.],\n",
      "          ...,\n",
      "          [255., 255., 255.,  ..., 255., 255., 255.],\n",
      "          [255., 255., 255.,  ..., 255., 255., 255.],\n",
      "          [  0.,   0.,   0.,  ..., 255., 255., 255.]],\n",
      "\n",
      "         [[255., 255., 255.,  ..., 255., 255., 255.],\n",
      "          [255., 255., 255.,  ..., 255., 255., 255.],\n",
      "          [  0.,   0.,   0.,  ..., 255., 255., 255.],\n",
      "          ...,\n",
      "          [189., 186., 201.,  ..., 255., 255., 255.],\n",
      "          [  0.,   0.,   0.,  ..., 255., 255., 255.],\n",
      "          [255., 255., 255.,  ..., 185., 184., 197.]],\n",
      "\n",
      "         [[187., 186., 198.,  ..., 255., 255., 255.],\n",
      "          [  0.,   0.,   0.,  ..., 255., 255., 255.],\n",
      "          [255., 255., 255.,  ..., 197., 195., 206.],\n",
      "          ...,\n",
      "          [  0.,   0.,   0.,  ..., 255., 255., 255.],\n",
      "          [255., 255., 255.,  ..., 255., 255., 255.],\n",
      "          [255., 255., 255.,  ..., 255., 255., 255.]]],\n",
      "\n",
      "\n",
      "        [[[  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          ...,\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.]],\n",
      "\n",
      "         [[  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          ...,\n",
      "          [117., 142., 212.,  ..., 255., 255., 255.],\n",
      "          [255., 255., 255.,  ..., 246., 108., 239.],\n",
      "          [236.,  82., 226.,  ..., 116., 141., 212.]],\n",
      "\n",
      "         [[118., 144., 211.,  ..., 255., 255., 255.],\n",
      "          [255., 255., 255.,  ..., 241., 255., 250.],\n",
      "          [255., 237., 255.,  ..., 115., 142., 212.],\n",
      "          ...,\n",
      "          [133., 122., 124.,  ...,  95.,  85.,  83.],\n",
      "          [ 98.,  89.,  87.,  ...,  80.,  74.,  64.],\n",
      "          [132.,  60., 116.,  ..., 178., 169., 172.]]]], device='cuda:0') tensor([ 84, 103,  18,  89,  40])\n"
     ]
    }
   ],
   "source": [
    "for image,label in train_loader:\n",
    "    print(image,label)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 180.00 MiB (GPU 0; 7.79 GiB total capacity; 4.63 GiB already allocated; 110.25 MiB free; 5.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_18882/37637588.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     25\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     26\u001B[0m         \u001B[0;31m# Forward pass\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 27\u001B[0;31m         \u001B[0moutputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mimages\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     28\u001B[0m         \u001B[0mloss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcriterion\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlabels\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     29\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/AML/Project/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1102\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1103\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_18882/2534620423.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     12\u001B[0m         \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 13\u001B[0;31m         \u001B[0mout\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtph\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     14\u001B[0m         \u001B[0mout\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreshape\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mout\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mout\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     15\u001B[0m         \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mout\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/AML/Project/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1102\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1103\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.cache/torch/hub/yangsenius_TransPose_main/lib/models/transpose_h.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    644\u001B[0m         \u001B[0mbs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mh\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mw\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    645\u001B[0m         \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mflatten\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpermute\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 646\u001B[0;31m         \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mglobal_encoder\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpos\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpos_embedding\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    647\u001B[0m         \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpermute\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m2\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcontiguous\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mview\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mh\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    648\u001B[0m         \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfinal_layer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/AML/Project/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1102\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1103\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.cache/torch/hub/yangsenius_TransPose_main/lib/models/transpose_h.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, src, mask, src_key_padding_mask, pos)\u001B[0m\n\u001B[1;32m    133\u001B[0m                 \u001B[0matten_maps_list\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0matt_map\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    134\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 135\u001B[0;31m                 output = layer(output, src_mask=mask,  pos=pos,\n\u001B[0m\u001B[1;32m    136\u001B[0m                                src_key_padding_mask=src_key_padding_mask)\n\u001B[1;32m    137\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/AML/Project/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1102\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1103\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.cache/torch/hub/yangsenius_TransPose_main/lib/models/transpose_h.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, src, src_mask, src_key_padding_mask, pos)\u001B[0m\n\u001B[1;32m    238\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnormalize_before\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    239\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mforward_pre\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msrc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msrc_mask\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msrc_key_padding_mask\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpos\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 240\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mforward_post\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msrc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msrc_mask\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msrc_key_padding_mask\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpos\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    241\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    242\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.cache/torch/hub/yangsenius_TransPose_main/lib/models/transpose_h.py\u001B[0m in \u001B[0;36mforward_post\u001B[0;34m(self, src, src_mask, src_key_padding_mask, pos)\u001B[0m\n\u001B[1;32m    198\u001B[0m                                            key_padding_mask=src_key_padding_mask)\n\u001B[1;32m    199\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 200\u001B[0;31m             src2 = self.self_attn(q, k, value=src, attn_mask=src_mask,\n\u001B[0m\u001B[1;32m    201\u001B[0m                                   key_padding_mask=src_key_padding_mask)[0]\n\u001B[1;32m    202\u001B[0m         \u001B[0msrc\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msrc\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdropout1\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msrc2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/AML/Project/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1102\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1103\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/AML/Project/venv/lib/python3.8/site-packages/torch/nn/modules/activation.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask)\u001B[0m\n\u001B[1;32m   1001\u001B[0m                 v_proj_weight=self.v_proj_weight)\n\u001B[1;32m   1002\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1003\u001B[0;31m             attn_output, attn_output_weights = F.multi_head_attention_forward(\n\u001B[0m\u001B[1;32m   1004\u001B[0m                 \u001B[0mquery\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkey\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0membed_dim\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnum_heads\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1005\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0min_proj_weight\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0min_proj_bias\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/AML/Project/venv/lib/python3.8/site-packages/torch/nn/functional.py\u001B[0m in \u001B[0;36mmulti_head_attention_forward\u001B[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v)\u001B[0m\n\u001B[1;32m   5099\u001B[0m     \u001B[0;31m# (deep breath) calculate attention and out projection\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   5100\u001B[0m     \u001B[0;31m#\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 5101\u001B[0;31m     \u001B[0mattn_output\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mattn_output_weights\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_scaled_dot_product_attention\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mq\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mk\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mv\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mattn_mask\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdropout_p\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   5102\u001B[0m     \u001B[0mattn_output\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mattn_output\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtranspose\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcontiguous\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mview\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtgt_len\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbsz\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0membed_dim\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   5103\u001B[0m     \u001B[0mattn_output\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlinear\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mattn_output\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mout_proj_weight\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mout_proj_bias\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/AML/Project/venv/lib/python3.8/site-packages/torch/nn/functional.py\u001B[0m in \u001B[0;36m_scaled_dot_product_attention\u001B[0;34m(q, k, v, attn_mask, dropout_p)\u001B[0m\n\u001B[1;32m   4847\u001B[0m     \u001B[0mattn\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msoftmax\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mattn\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdim\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   4848\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mdropout_p\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m0.0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 4849\u001B[0;31m         \u001B[0mattn\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdropout\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mattn\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mp\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdropout_p\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   4850\u001B[0m     \u001B[0;31m# (B, Nt, Ns) x (B, Ns, E) -> (B, Nt, E)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   4851\u001B[0m     \u001B[0moutput\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbmm\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mattn\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mv\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/AML/Project/venv/lib/python3.8/site-packages/torch/nn/functional.py\u001B[0m in \u001B[0;36mdropout\u001B[0;34m(input, p, training, inplace)\u001B[0m\n\u001B[1;32m   1167\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mp\u001B[0m \u001B[0;34m<\u001B[0m \u001B[0;36m0.0\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0mp\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m1.0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1168\u001B[0m         \u001B[0;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"dropout probability has to be between 0 and 1, \"\u001B[0m \u001B[0;34m\"but got {}\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mp\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1169\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0m_VF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdropout_\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mp\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtraining\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0minplace\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0m_VF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdropout\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mp\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtraining\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1170\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1171\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: CUDA out of memory. Tried to allocate 180.00 MiB (GPU 0; 7.79 GiB total capacity; 4.63 GiB already allocated; 110.25 MiB free; 5.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "params_to_update = model.parameters()\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params_to_update, lr=learning_rate)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=trainset,batch_size=batch_size,shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=testset,batch_size=batch_size,shuffle=False)\n",
    "# Train the model\n",
    "lr = learning_rate\n",
    "total_step = len(train_loader)\n",
    "loss_train = []\n",
    "loss_val = []\n",
    "best_accuracy = None\n",
    "accuracy_val = []\n",
    "#best_model = type(model)(num_classes, fine_tune, pretrained) # get a new instance\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    loss_iter = 0\n",
    "    for i, (labels, images) in enumerate(train_loader):\n",
    "        # Move tensors to the configured device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_iter += loss.item()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "    loss_train.append(loss_iter/(len(train_loader)*batch_size))\n",
    "\n",
    "\n",
    "    # Code to update the lr\n",
    "    lr *= learning_rate_decay\n",
    "    update_lr(optimizer, lr)\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        loss_iter = 0\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss_iter += loss.item()\n",
    "\n",
    "        loss_val.append(loss_iter/(len(val_loader)*batch_size))\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        accuracy_val.append(accuracy)\n",
    "\n",
    "        print('Validataion accuracy is: {} %'.format(accuracy))\n",
    "        #################################################################################\n",
    "        # TODO: Q2.b Use the early stopping mechanism from previous questions to save   #\n",
    "        # the model with the best validation accuracy so-far (use best_model).          #\n",
    "        #################################################################################\n",
    "\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        early_stop = False\n",
    "        patience = 3\n",
    "        if epoch > patience - 1:\n",
    "            for j in range(patience - 1):\n",
    "                if max(accuracy_val) > list(reversed(accuracy_val))[j]:\n",
    "                    if \"not_improving_epochs\" in locals(): not_improving_epochs += 1\n",
    "                    else: not_improving_epochs = 1\n",
    "                    print('Not saving the model')\n",
    "                else:\n",
    "                    not_improving_epochs = 0\n",
    "                    best_model = model\n",
    "                    print(\"Saving the model\")\n",
    "                    break\n",
    "                if not_improving_epochs >= patience:\n",
    "                    early_stop = True\n",
    "                    print('Early stopping')\n",
    "                    break\n",
    "                break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}