{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>Load pretrained weights from url: https://github.com/yangsenius/TransPose/releases/download/Hub/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      "Successfully loaded model  (on cpu) with pretrained weights!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/michele/.cache/torch/hub/yangsenius_TransPose_main\n",
      "/home/michele/.cache/torch/hub/yangsenius_TransPose_main/lib/models/transpose_r.py:333: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  dim_t = temperature ** (2 * (dim_t // 2) / one_direction_feats)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "import os\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")            #comment this line to run with GPU\n",
    "tpr = torch.hub.load('yangsenius/TransPose:main', 'tpr_a4_256x192', pretrained=True)\n",
    "#tph = torch.hub.load('yangsenius/TransPose:main', 'tph_a4_256x192', pretrained=True, device=device)\n",
    "\n",
    "\n",
    "#print(tph)\n",
    "DATASET_PATH = './dataset/'\n",
    "positions = os.listdir(DATASET_PATH)\n",
    "#print(os.listdir(DATASET_PATH+entries[0]))\n",
    "images = list()\n",
    "\n",
    "def add_margin(pil_img, top, right, bottom, left, color):\n",
    "    width, height = pil_img.size\n",
    "    new_width = width + right + left\n",
    "    new_height = height + top + bottom\n",
    "    result = Image.new(pil_img.mode, (new_width, new_height), color)\n",
    "    result.paste(pil_img, (left, top))\n",
    "    return result\n",
    "\n",
    "class YogaPoseDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataset_path):\n",
    "        self.data_path = dataset_path\n",
    "        # call to init the data\n",
    "        self.size = 256,192\n",
    "        self._init_data()\n",
    "\n",
    "    def _init_data(self):\n",
    "        positions = os.listdir(self.data_path)\n",
    "        images = list()\n",
    "        labels = list()\n",
    "        for idx, position in enumerate(positions):\n",
    "            for file in os.listdir(DATASET_PATH + position):\n",
    "                if not file.endswith('.gif'):\n",
    "                    f = cv2.imread(DATASET_PATH + position + '/' + file,cv2.IMREAD_COLOR)\n",
    "\n",
    "                    #print(position + '/' + file)\n",
    "                    #print(f.shape)\n",
    "                    height,width, channels = f.shape\n",
    "                    if height > width:\n",
    "                        scale_percent = 256/height\n",
    "                        f = cv2.resize(f,(int(width*scale_percent),256))\n",
    "                        if f.shape[1] > 192:\n",
    "                            scale_percent = 192/width\n",
    "                            f = cv2.resize(f,(192,int(height*scale_percent)))\n",
    "                    else:\n",
    "                        scale_percent = 192/width\n",
    "                        f = cv2.resize(f,(192,int(height*scale_percent)))\n",
    "                        if f.shape[0] > 256:\n",
    "                            scale_percent = 256/height\n",
    "                            f = cv2.resize(f,(int(width*scale_percent),256))\n",
    "\n",
    "                    height,width, channels = f.shape\n",
    "                    #print(f.shape,\"after resize\")\n",
    "                    f = cv2.copyMakeBorder(f, 256-height, 0, 192-width, 0, cv2.BORDER_CONSTANT,value=0)\n",
    "                    #print(f.shape,\"after padding\")\n",
    "                    #if torch.FloatTensor(f.getdata()).size()[1] != 3: f.show()\n",
    "                    data = torch.reshape(torch.FloatTensor(f).to(device),(3,256,192))\n",
    "                    #print(data.shape)\n",
    "                    #print(data.size())\n",
    "                    ##images.append(data)\n",
    "                    ##labels.append(torch.tensor(idx))\n",
    "                    images.append((idx,data))\n",
    "\n",
    "\n",
    "        #print(images)\n",
    "        ##images = torch.stack(images)\n",
    "        ##labels = torch.stack(labels)\n",
    "        #print(labels)\n",
    "        #print(images.size())\n",
    "        ##mask = np.arange(labels.size()[0])\n",
    "        np.random.shuffle(images)\n",
    "        self.images = images\n",
    "        #print(type(images),type(images[0]))\n",
    "        ##self.images = images[mask]\n",
    "        ##self.labels = labels[mask]\n",
    "        #print(self.labels.size())\n",
    "        #print(self.labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        # returns the number of samples in our dataset\n",
    "        return len(self.images)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def getData(self):\n",
    "        return self.images\n",
    "\n",
    "    #def getY(self):\n",
    "    #    return self.labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # returns the idx-th sample\n",
    "        return self.images[idx]\n",
    "\n",
    "    def collate_fn(self,data):\n",
    "        #print(len(data),data)\n",
    "        #print(data[0].size())\n",
    "        #print(data[1].size())\n",
    "        #print('lwwwwwwwwwwwwwww',[x[0].size() for x in data])\n",
    "        #print(torch.cat([x[0] for x in data]).size(),data[0][0].size())\n",
    "        #print(torch.stack([x[1] for x in data]).size(),data[0][1],[x[1] for x in data])\n",
    "        Xs = torch.stack([x[1] for x in data])\n",
    "        print(Xs.size())\n",
    "        y = torch.stack([torch.tensor(x[0]) for x in data])\n",
    "        print(y.size())\n",
    "        return Xs,y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: Ignoring incorrect cHRM white(.34575,.35855) r(.6485,.33088)g(.32121,.59787)b(.15589,.06604) when sRGB is also present\n",
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = YogaPoseDataset(DATASET_PATH)\n",
    "split_position = int((len(dataset)//10)*7)\n",
    "trainset = dataset[:split_position]\n",
    "testset = dataset[split_position:]\n",
    "print(trainset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "(63,\n tensor([[[  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n          ...,\n          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n          [  0.,   0.,   0.,  ...,   0.,   0.,   0.]],\n \n         [[  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n          ...,\n          [253., 253., 253.,  ..., 255., 255., 255.],\n          [255., 255., 255.,  ...,  76.,  41.,  38.],\n          [ 77.,  43.,  40.,  ..., 147., 189., 242.]],\n \n         [[199., 218., 244.,  ..., 255., 255., 255.],\n          [255., 255., 255.,  ...,  76.,  41.,  38.],\n          [ 77.,  41.,  39.,  ..., 147., 188., 242.],\n          ...,\n          [255., 255., 255.,  ..., 167., 145., 144.],\n          [181., 164., 164.,  ..., 255., 255., 255.],\n          [255., 255., 255.,  ..., 255., 255., 255.]]], device='cuda:0'))"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class PoseClassifier(nn.Module):\n",
    "    def __init__(self, n_class, fine_tune=False, pretrained=True):\n",
    "        super(PoseClassifier, self).__init__()\n",
    "\n",
    "        self.tph = torch.hub.load('yangsenius/TransPose:main', 'tph_a4_256x192', pretrained=True, device=device)\n",
    "        self.fc1 = nn.Linear(1000,1000).to(device)\n",
    "        self.fc2 = nn.Linear(1000,n_class).to(device)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = x.to(device)\n",
    "        out = self.tph(x)\n",
    "        out = self.fc1(out)\n",
    "        #m = nn.BatchNorm1d(1000,device=device)\n",
    "        #out = m(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "output.size()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "batch_size"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>Load pretrained weights from url: https://github.com/yangsenius/TransPose/releases/download/Hub/tp_h_48_256x192_enc4_d96_h192_mh1.pth\n",
      "Successfully loaded model  (on cpu) with pretrained weights!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/michele/.cache/torch/hub/yangsenius_TransPose_main\n",
      "/home/michele/.cache/torch/hub/yangsenius_TransPose_main/lib/models/transpose_h.py:514: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  dim_t = temperature ** (2 * (dim_t // 2) / one_direction_feats)\n"
     ]
    }
   ],
   "source": [
    "num_classes = 107\n",
    "model = PoseClassifier(n_class=num_classes)\n",
    "num_epochs = 30\n",
    "batch_size = 16\n",
    "learning_rate = 1e-3\n",
    "learning_rate_decay = 0.99\n",
    "params_to_update = model.parameters()\n",
    "def update_lr(optimizer, lr):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "params_to_update = model.parameters()\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params_to_update, lr=learning_rate)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=trainset,batch_size=batch_size,shuffle=False,collate_fn=dataset.collate_fn)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=testset,batch_size=batch_size,shuffle=False,collate_fn=dataset.collate_fn)\n",
    "# Train the model\n",
    "lr = learning_rate\n",
    "total_step = len(train_loader)\n",
    "loss_train = []\n",
    "loss_val = []\n",
    "best_accuracy = None\n",
    "accuracy_val = []"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3, 256, 192])\n",
      "torch.Size([16])\n",
      "tensor([[[[  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          ...,\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.]],\n",
      "\n",
      "         [[  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          ...,\n",
      "          [253., 253., 253.,  ..., 255., 255., 255.],\n",
      "          [255., 255., 255.,  ...,  76.,  41.,  38.],\n",
      "          [ 77.,  43.,  40.,  ..., 147., 189., 242.]],\n",
      "\n",
      "         [[199., 218., 244.,  ..., 255., 255., 255.],\n",
      "          [255., 255., 255.,  ...,  76.,  41.,  38.],\n",
      "          [ 77.,  41.,  39.,  ..., 147., 188., 242.],\n",
      "          ...,\n",
      "          [255., 255., 255.,  ..., 167., 145., 144.],\n",
      "          [181., 164., 164.,  ..., 255., 255., 255.],\n",
      "          [255., 255., 255.,  ..., 255., 255., 255.]]],\n",
      "\n",
      "\n",
      "        [[[  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          ...,\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.]],\n",
      "\n",
      "         [[  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          ...,\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.]],\n",
      "\n",
      "         [[  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          ...,\n",
      "          [252., 252., 252.,  ...,  29.,  28.,  16.],\n",
      "          [ 29.,  28.,  16.,  ...,  24.,  24.,  11.],\n",
      "          [ 23.,  23.,  10.,  ...,  54.,  57.,  48.]]],\n",
      "\n",
      "\n",
      "        [[[  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          ...,\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.]],\n",
      "\n",
      "         [[  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          ...,\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.]],\n",
      "\n",
      "         [[  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          ...,\n",
      "          [237., 203.,  38.,  ..., 252., 203., 101.],\n",
      "          [252., 203., 101.,  ..., 252., 204., 103.],\n",
      "          [252., 203., 102.,  ..., 252., 203., 101.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          ...,\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.]],\n",
      "\n",
      "         [[  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          ...,\n",
      "          [255., 255., 255.,  ..., 255., 255., 255.],\n",
      "          [255., 255., 255.,  ..., 254., 254., 254.],\n",
      "          [254., 254., 254.,  ..., 255., 255., 255.]],\n",
      "\n",
      "         [[255., 255., 255.,  ..., 255., 255., 255.],\n",
      "          [255., 255., 255.,  ..., 254., 254., 254.],\n",
      "          [254., 254., 254.,  ..., 255., 255., 255.],\n",
      "          ...,\n",
      "          [255., 255., 255.,  ..., 223., 221., 223.],\n",
      "          [223., 221., 223.,  ...,  39.,  35.,  32.],\n",
      "          [ 40.,  36.,  33.,  ..., 255., 254., 254.]]],\n",
      "\n",
      "\n",
      "        [[[  0.,   0.,   0.,  ...,  48.,  95., 178.],\n",
      "          [ 76., 124., 193.,  ...,  14.,  43., 134.],\n",
      "          [ 16.,  48., 135.,  ...,  51.,  97., 161.],\n",
      "          ...,\n",
      "          [ 74., 116., 181.,  ...,  39.,  75., 159.],\n",
      "          [ 67., 106., 165.,  ...,  54.,  74.,  94.],\n",
      "          [  0.,   0.,   0.,  ...,  38.,  83., 163.]],\n",
      "\n",
      "         [[ 73., 117., 181.,  ...,  43.,  81., 165.],\n",
      "          [ 73., 116., 173.,  ...,  55.,  74.,  95.],\n",
      "          [  0.,   0.,   0.,  ...,  38.,  83., 163.],\n",
      "          ...,\n",
      "          [ 53.,  91., 156.,  ...,   9.,  14.,  13.],\n",
      "          [  0.,   0.,   0.,  ...,  22.,  25.,  29.],\n",
      "          [ 22.,  25.,  29.,  ...,  28.,  64., 154.]],\n",
      "\n",
      "         [[ 53.,  90., 155.,  ...,  10.,  15.,  14.],\n",
      "          [  0.,   0.,   0.,  ...,  21.,  24.,  27.],\n",
      "          [ 21.,  24.,  27.,  ...,  26.,  62., 153.],\n",
      "          ...,\n",
      "          [  0.,   0.,   0.,  ..., 112., 137., 192.],\n",
      "          [106., 131., 187.,  ...,  33.,  42.,  56.],\n",
      "          [ 30.,  39.,  54.,  ..., 131., 149., 209.]]],\n",
      "\n",
      "\n",
      "        [[[  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          ...,\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.]],\n",
      "\n",
      "         [[  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          ...,\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.]],\n",
      "\n",
      "         [[  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          ...,\n",
      "          [235., 237., 244.,  ..., 190., 208., 219.],\n",
      "          [190., 208., 219.,  ..., 124., 129., 132.],\n",
      "          [114., 123., 126.,  ...,  78., 108., 112.]]]], device='cuda:0') tensor([63, 97, 68, 86, 57, 35, 10,  5, 56, 13,  4, 51, 45, 45, 53,  3])\n"
     ]
    }
   ],
   "source": [
    "for image,label in train_loader:\n",
    "    print(image,label)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 7.79 GiB total capacity; 4.66 GiB already allocated; 23.75 MiB free; 5.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_16235/37637588.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     25\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     26\u001B[0m         \u001B[0;31m# Forward pass\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 27\u001B[0;31m         \u001B[0moutputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mimages\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     28\u001B[0m         \u001B[0mloss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcriterion\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlabels\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     29\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/AML/Project/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1102\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1103\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_16235/1999393345.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     12\u001B[0m         \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 13\u001B[0;31m         \u001B[0mout\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtph\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     14\u001B[0m         \u001B[0mout\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfc1\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mout\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     15\u001B[0m         \u001B[0;31m#m = nn.BatchNorm1d(1000,device=device)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/AML/Project/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1102\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1103\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.cache/torch/hub/yangsenius_TransPose_main/lib/models/transpose_h.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    639\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    640\u001B[0m                 \u001B[0mx_list\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0my_list\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 641\u001B[0;31m         \u001B[0my_list\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstage3\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx_list\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    642\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    643\u001B[0m         \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreduce\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0my_list\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/AML/Project/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1102\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1103\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/AML/Project/venv/lib/python3.8/site-packages/torch/nn/modules/container.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    139\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    140\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mmodule\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 141\u001B[0;31m             \u001B[0minput\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodule\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    142\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    143\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/AML/Project/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1102\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1103\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.cache/torch/hub/yangsenius_TransPose_main/lib/models/transpose_h.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    392\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    393\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mi\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnum_branches\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 394\u001B[0;31m             \u001B[0mx\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbranches\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    395\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    396\u001B[0m         \u001B[0mx_fuse\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/AML/Project/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1102\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1103\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/AML/Project/venv/lib/python3.8/site-packages/torch/nn/modules/container.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    139\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    140\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mmodule\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 141\u001B[0;31m             \u001B[0minput\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodule\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    142\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    143\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/AML/Project/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1102\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1103\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.cache/torch/hub/yangsenius_TransPose_main/lib/models/transpose_h.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     49\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     50\u001B[0m         \u001B[0mout\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconv1\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 51\u001B[0;31m         \u001B[0mout\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbn1\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mout\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     52\u001B[0m         \u001B[0mout\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrelu\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mout\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     53\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/AML/Project/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1102\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1103\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/AML/Project/venv/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    166\u001B[0m         \u001B[0mused\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mnormalization\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m \u001B[0;32min\u001B[0m \u001B[0meval\u001B[0m \u001B[0mmode\u001B[0m \u001B[0mwhen\u001B[0m \u001B[0mbuffers\u001B[0m \u001B[0mare\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    167\u001B[0m         \"\"\"\n\u001B[0;32m--> 168\u001B[0;31m         return F.batch_norm(\n\u001B[0m\u001B[1;32m    169\u001B[0m             \u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    170\u001B[0m             \u001B[0;31m# If buffers are not to be tracked, ensure that they won't be updated\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/AML/Project/venv/lib/python3.8/site-packages/torch/nn/functional.py\u001B[0m in \u001B[0;36mbatch_norm\u001B[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001B[0m\n\u001B[1;32m   2280\u001B[0m         \u001B[0m_verify_batch_size\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2281\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2282\u001B[0;31m     return torch.batch_norm(\n\u001B[0m\u001B[1;32m   2283\u001B[0m         \u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mweight\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbias\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrunning_mean\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrunning_var\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtraining\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmomentum\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0meps\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackends\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcudnn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0menabled\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2284\u001B[0m     )\n",
      "\u001B[0;31mRuntimeError\u001B[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 7.79 GiB total capacity; 4.66 GiB already allocated; 23.75 MiB free; 5.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "params_to_update = model.parameters()\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params_to_update, lr=learning_rate)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=trainset,batch_size=batch_size,shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=testset,batch_size=batch_size,shuffle=False)\n",
    "# Train the model\n",
    "lr = learning_rate\n",
    "total_step = len(train_loader)\n",
    "loss_train = []\n",
    "loss_val = []\n",
    "best_accuracy = None\n",
    "accuracy_val = []\n",
    "#best_model = type(model)(num_classes, fine_tune, pretrained) # get a new instance\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    loss_iter = 0\n",
    "    for i, (labels, images) in enumerate(train_loader):\n",
    "        # Move tensors to the configured device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_iter += loss.item()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "    loss_train.append(loss_iter/(len(train_loader)*batch_size))\n",
    "\n",
    "\n",
    "    # Code to update the lr\n",
    "    lr *= learning_rate_decay\n",
    "    update_lr(optimizer, lr)\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        loss_iter = 0\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss_iter += loss.item()\n",
    "\n",
    "        loss_val.append(loss_iter/(len(val_loader)*batch_size))\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        accuracy_val.append(accuracy)\n",
    "\n",
    "        print('Validataion accuracy is: {} %'.format(accuracy))\n",
    "        #################################################################################\n",
    "        # TODO: Q2.b Use the early stopping mechanism from previous questions to save   #\n",
    "        # the model with the best validation accuracy so-far (use best_model).          #\n",
    "        #################################################################################\n",
    "\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        early_stop = False\n",
    "        patience = 3\n",
    "        if epoch > patience - 1:\n",
    "            for j in range(patience - 1):\n",
    "                if max(accuracy_val) > list(reversed(accuracy_val))[j]:\n",
    "                    if \"not_improving_epochs\" in locals(): not_improving_epochs += 1\n",
    "                    else: not_improving_epochs = 1\n",
    "                    print('Not saving the model')\n",
    "                else:\n",
    "                    not_improving_epochs = 0\n",
    "                    best_model = model\n",
    "                    print(\"Saving the model\")\n",
    "                    break\n",
    "                if not_improving_epochs >= patience:\n",
    "                    early_stop = True\n",
    "                    print('Early stopping')\n",
    "                    break\n",
    "                break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}