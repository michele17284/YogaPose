{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numbers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "import os\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import genericpath\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# force working on cpu due to memory limitation\n",
    "#device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "def add_margin(pil_img, top, right, bottom, left, color):\n",
    "    width, height = pil_img.size\n",
    "    new_width = width + right + left\n",
    "    new_height = height + top + bottom\n",
    "    result = Image.new(pil_img.mode, (new_width, new_height), color)\n",
    "    result.paste(pil_img, (left, top))\n",
    "    return result\n",
    "\n",
    "\n",
    "class YogaPoseDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataset_path, size=(256, 192), transform=None, example_images=None):\n",
    "        self.data_path = dataset_path\n",
    "        self.size = size\n",
    "        self.transform = transform\n",
    "        # call to init the data\n",
    "        self._init_data()\n",
    "\n",
    "    def _init_data(self):\n",
    "        images = list()\n",
    "\n",
    "        for _, directory_class in enumerate(os.listdir(self.data_path)):\n",
    "            class_path = os.path.join(self.data_path, directory_class)\n",
    "            for file_name in os.listdir(class_path):\n",
    "                f = cv2.imread(os.path.join(class_path, file_name), cv2.IMREAD_COLOR)\n",
    "                f = cv2.cvtColor(f, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                if self.transform is not None:\n",
    "                    f = self.transform(f)\n",
    "\n",
    "                data = torch.reshape(torch.FloatTensor(f), (3, self.size[0], self.size[1]))\n",
    "\n",
    "                # format example  images[x][0] -> (label, input)\n",
    "                # format example  images[x][1] -> [other information]\n",
    "                # images[x] -> ((class_id, image_tensor), [filename])\n",
    "                images.append((int(directory_class), data))\n",
    "\n",
    "\n",
    "        np.random.shuffle(images)\n",
    "        self.images = images\n",
    "\n",
    "    def __len__(self):\n",
    "        # returns the number of samples in our dataset\n",
    "        return len(self.images)\n",
    "\n",
    "    def getData(self):\n",
    "        return self.images\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            idx: the index of the sample\n",
    "\n",
    "        Returns: a tuple (class, input) for the given sample\n",
    "\n",
    "        \"\"\"\n",
    "        return self.images[idx]\n",
    "\n",
    "    def collate_fn(self, data):\n",
    "        #print(data)\n",
    "        Xs = torch.stack([x[1] for x in data])\n",
    "        y = torch.stack([torch.tensor(x[0]) for x in data])\n",
    "        return Xs, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "DATASET_PATH = './data/images/'\n",
    "ANNOTATION_PATH = './data/annotations/'\n",
    "MODEL_NAME = \"tpr_a4_256x192\"\n",
    "norm_transform = T.Compose([T.ToTensor(), T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]), ])\n",
    "dataset = YogaPoseDataset(DATASET_PATH, transform=norm_transform)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/orlando/.cache/torch/hub/yangsenius_TransPose_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download https://github.com/yangsenius/TransPose/releases/download/Yaml/TP_R_256x192_d256_h1024_enc4_mh8.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n",
      "/home/orlando/.cache/torch/hub/yangsenius_TransPose_main/lib/models/transpose_r.py:333: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  dim_t = temperature ** (2 * (dim_t // 2) / one_direction_feats)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>Load pretrained weights from url: https://github.com/yangsenius/TransPose/releases/download/Hub/tp_r_256x192_enc4_d256_h1024_mh8.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded model  (on cpu) with pretrained weights!\n"
     ]
    },
    {
     "data": {
      "text/plain": "TransPoseR(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (3): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (reduce): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n  (global_encoder): TransformerEncoder(\n    (layers): ModuleList(\n      (0): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n        )\n        (linear1): Linear(in_features=256, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=1024, out_features=256, bias=True)\n        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n      (1): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n        )\n        (linear1): Linear(in_features=256, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=1024, out_features=256, bias=True)\n        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n      (2): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n        )\n        (linear1): Linear(in_features=256, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=1024, out_features=256, bias=True)\n        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n      (3): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n        )\n        (linear1): Linear(in_features=256, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=1024, out_features=256, bias=True)\n        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n    )\n  )\n  (deconv_layers): Sequential(\n    (0): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n  )\n  (final_layer): Conv2d(256, 17, kernel_size=(1, 1), stride=(1, 1))\n)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get model from torch hub\n",
    "assert MODEL_NAME in [\"tpr_a4_256x192\", \"tph_a4_256x192\"]\n",
    "\n",
    "transpose_model = torch.hub.load('yangsenius/TransPose:main', MODEL_NAME, pretrained=True)\n",
    "transpose_model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from TransPose.lib.config import cfg\n",
    "from TransPose.lib.utils import transforms\n",
    "from TransPose.lib.core.inference import get_final_preds\n",
    "from TransPose.visualize import inspect_atten_map_by_locations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "\n",
    "train_split_position = (len(dataset) // 10) * 8\n",
    "val_split_position = train_split_position+len(dataset)//10\n",
    "trainset = dataset[:train_split_position]\n",
    "valset = dataset[train_split_position:val_split_position]\n",
    "testset = dataset[val_split_position:]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "OUT_DIR = \"./out/\"\n",
    "idx = 0\n",
    "\n",
    "if not os.path.isdir(OUT_DIR):\n",
    "    os.makedirs(OUT_DIR)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class PoseClassifier(nn.Module):\n",
    "    def __init__(self, n_class,\n",
    "                 transpose_model,device=device, fine_tune=False, pretrained=True):\n",
    "        super(PoseClassifier, self).__init__()\n",
    "        layers = []\n",
    "        dropout = 0.5\n",
    "        hidden_layers = [128, 512, 512, 512, 512, 256, 128, 128]\n",
    "        self.tpr = transpose_model\n",
    "        layers.append(nn.Conv2d(17, 128, 3, padding=1))\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.conv1 = nn.Conv2d(17, hidden_layers[0], 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(hidden_layers[0])\n",
    "        self.pool1 = nn.MaxPool2d((2, 2), 2)\n",
    "\n",
    "        self.pool2 = nn.MaxPool2d((3, 3), 3)\n",
    "        self.conv2 = nn.Conv2d(hidden_layers[0], hidden_layers[1], 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(hidden_layers[1])\n",
    "        self.conv3 = nn.Conv2d(hidden_layers[1], hidden_layers[2], 3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(hidden_layers[2])\n",
    "        self.conv4 = nn.Conv2d(hidden_layers[2], hidden_layers[3], 3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(hidden_layers[3])\n",
    "        self.conv5 = nn.Conv2d(hidden_layers[3], hidden_layers[4], 3, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(hidden_layers[4])\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.lin1 = nn.Linear(hidden_layers[4],hidden_layers[-1])\n",
    "        self.classifier = nn.Linear(hidden_layers[-1],n_class)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.tpr(x)\n",
    "        #print(out.size(),\"AFTER TPH\")\n",
    "        out = self.conv1(out)\n",
    "        out = self.bn1(out)\n",
    "        #print(out.size(),\"AFTER CONV1\")\n",
    "        out = self.pool1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        #print(out.size(),\"AFTER POOL\")\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        #print(out.size(),\"AFTER CONV2\")\n",
    "        out = self.pool1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        #print(out.size(),\"AFTER POOL\")\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        #print(out.size(),\"AFTER CONV3\")\n",
    "        out = self.pool2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        #print(out.size(),\"AFTER POOL\")\n",
    "        out = self.conv4(out)\n",
    "        out = self.bn4(out)\n",
    "        #print(out.size(),\"AFTER CONV4\")\n",
    "        out = self.pool2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        #print(out.size(),\"AFTER POOL\")\n",
    "        out = self.conv5(out)\n",
    "        out = self.bn5(out)\n",
    "        #print(out.size(),\"AFTER CONV5\")\n",
    "        #out = self.pool1(out)\n",
    "        out = self.flatten(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        #print(out.size(),\"AFTER POOL\")\n",
    "        out = self.lin1(out)\n",
    "        '''\n",
    "        out = self.pool1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        #print(out.size(),\"AFTER POOL\")\n",
    "        out = self.conv6(out)\n",
    "\n",
    "        #print(out.size(),\"AFTER CONV6\")\n",
    "        '''\n",
    "\n",
    "\n",
    "        out = self.flatten(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        #print(out.size(),\"AFTER FLATTEN\")\n",
    "\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "num_classes = 107\n",
    "num_epochs = 3\n",
    "batch_size = 12\n",
    "learning_rate = 1e-4\n",
    "learning_rate_decay = 0.99\n",
    "\n",
    "model = PoseClassifier(n_class=num_classes, transpose_model=transpose_model)\n",
    "model.to(device)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=trainset, batch_size=batch_size, shuffle=False,collate_fn=dataset.collate_fn)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=valset,batch_size=batch_size,shuffle=False,collate_fn=dataset.collate_fn)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=testset,batch_size=batch_size,shuffle=False,collate_fn=dataset.collate_fn)\n",
    "\n",
    "\n",
    "fine_tune = False\n",
    "if fine_tune:\n",
    "    params_to_update = []\n",
    "    for param in model.tpr.parameters():\n",
    "        param.requires_grad = False\n",
    "    for p in model.parameters():\n",
    "        if p.requires_grad == True:\n",
    "            params_to_update.append(p)\n",
    "else:\n",
    "    params_to_update = model.parameters()\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params_to_update, lr=learning_rate)\n",
    "\n",
    "def update_lr(optimizer, lr):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n"
     ]
    },
    {
     "data": {
      "text/plain": "4792"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(train_loader))\n",
    "len(train_loader.dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "\n",
    "# Train the model\n",
    "\n",
    "#best_model = type(model)(num_classes, fine_tune, pretrained) # get a new instance\n",
    "def train(model,num_epochs=num_epochs,lr=learning_rate):\n",
    "    total_step = len(train_loader)\n",
    "    loss_train = []\n",
    "    loss_val = []\n",
    "    best_accuracy = None\n",
    "    accuracy_val = []\n",
    "    accuracy_test = []\n",
    "\n",
    "    train_f1 = []\n",
    "    val_f1 = []\n",
    "    for epoch in range(num_epochs):\n",
    "        train_predicts = torch.tensor([]).to(device)\n",
    "        train_labels = torch.tensor([]).to(device)\n",
    "        val_predicts = torch.tensor([]).to(device)\n",
    "        val_labels = torch.tensor([]).to(device)\n",
    "        model.train()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        loss_iter = 0\n",
    "\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            # Move tensors to the configured device\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            #print(outputs,labels)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_iter += loss.item()\n",
    "\n",
    "\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                      .format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))\n",
    "\n",
    "            train_predicts = torch.cat((train_predicts, predicted), dim=0)\n",
    "            train_labels = torch.cat((train_labels, labels), dim=0)\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        accuracy_test.append(accuracy)\n",
    "        loss_train.append(loss_iter / (len(train_loader) * batch_size))\n",
    "        print('Training accuracy is: {} %'.format(accuracy))\n",
    "        train_labels = train_labels.cpu().numpy()\n",
    "        train_predicts = train_predicts.cpu().numpy()\n",
    "        f1 =f1_score(train_labels, train_predicts, average='weighted')\n",
    "        f1 = f1 * 100\n",
    "        train_f1.append(f1)\n",
    "        print('Training F1 is: {} %'.format(f1))\n",
    "\n",
    "        # Code to update the lr\n",
    "        lr *= learning_rate_decay\n",
    "        update_lr(optimizer, lr)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            loss_iter = 0\n",
    "            for images, labels in val_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss_iter += loss.item()\n",
    "                val_predicts = torch.cat((val_predicts, predicted), dim=0)\n",
    "                val_labels = torch.cat((val_labels, labels), dim=0)\n",
    "            loss_val.append(loss_iter / (len(val_loader) * batch_size))\n",
    "            accuracy = 100 * correct / total\n",
    "            accuracy_val.append(accuracy)\n",
    "            print('Validation accuracy is: {} %'.format(accuracy))\n",
    "            val_labels = val_labels.cpu().numpy()\n",
    "            val_predicts = val_predicts.cpu().numpy()\n",
    "            f1 =f1_score(val_labels, val_predicts, average='weighted')\n",
    "            f1 = f1 * 100\n",
    "            val_f1.append(f1)\n",
    "            print('Validation F1 is: {} %'.format(f1))\n",
    "\n",
    "            early_stop = False\n",
    "            patience = 3\n",
    "            if epoch > patience - 1:\n",
    "                for j in range(patience - 1):\n",
    "                    if max(accuracy_val) > list(reversed(accuracy_val))[j]:\n",
    "                        if \"not_improving_epochs\" in locals():\n",
    "                            not_improving_epochs += 1\n",
    "                        else:\n",
    "                            not_improving_epochs = 1\n",
    "                        print('Not saving the model')\n",
    "                    else:\n",
    "                        not_improving_epochs = 0\n",
    "                        best_model = model\n",
    "                        print(\"Saving the model\")\n",
    "                        break\n",
    "                    if not_improving_epochs >= patience:\n",
    "                        early_stop = True\n",
    "                        print('Early stopping')\n",
    "                        break\n",
    "                    break\n",
    "\n",
    "    plt.figure(2)\n",
    "    plt.plot(loss_train, 'r', label='Train loss')\n",
    "    plt.plot(loss_val, 'g', label='Val loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(3)\n",
    "    plt.plot(accuracy_val, 'r', label='Val accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return {\"Loss/train\": loss_train, \"Loss/val\": loss_val, \"Accuracy/train\":accuracy_val, \"Accuracy/val\": accuracy_test, \"F1/train\": train_f1, \"F1/val\": val_f1}\n",
    "\n",
    "def test(model):\n",
    "    with torch.no_grad():\n",
    "            all_preds = torch.tensor([]).to(device)\n",
    "            all_labels = torch.tensor([]).to(device)\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            loss_iter = 0\n",
    "            for images, labels in test_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss_iter += loss.item()\n",
    "                all_preds = torch.cat((all_preds, predicted), dim=0)\n",
    "                all_labels = torch.cat((all_labels, labels), dim=0)\n",
    "\n",
    "            loss_test =(loss_iter / (len(test_loader) * batch_size))\n",
    "            accuracy = 100 * correct / total\n",
    "\n",
    "            all_preds = all_preds.cpu().numpy()\n",
    "            all_labels = all_labels.cpu().numpy()\n",
    "            f1 =  f1_score(test_labels, test_predicted, average='weighted')\n",
    "            f1 = f1 * 100\n",
    "            print('Test accuracy is: {} %'.format(accuracy))\n",
    "            print('Test f1 is: {} %'.format(f1))\n",
    "            print('Test Loss: {:.4f} %'.format(loss_test))\n",
    "    return all_preds, all_labels, {\"Accuracy/test\": accuracy, \"Loss/test\": loss_test, \"F1/test\" : f1}\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [100/400], Loss: 3.9929\n",
      "Epoch [1/3], Step [200/400], Loss: 4.4232\n",
      "Epoch [1/3], Step [300/400], Loss: 3.9350\n",
      "Epoch [1/3], Step [400/400], Loss: 3.6308\n",
      "Training accuracy is: 5.82220367278798 %\n",
      "Training F1 is: 0.04093189949647491 %\n",
      "Validation accuracy is: 7.345575959933222 %\n",
      "Validation F1 is: 0.05045167896475804 %\n",
      "Epoch [2/3], Step [100/400], Loss: 3.9266\n",
      "Epoch [2/3], Step [200/400], Loss: 3.7387\n",
      "Epoch [2/3], Step [300/400], Loss: 3.6815\n",
      "Epoch [2/3], Step [400/400], Loss: 3.5770\n",
      "Training accuracy is: 7.742070116861436 %\n",
      "Training F1 is: 0.058284056938643775 %\n",
      "Validation accuracy is: 7.512520868113523 %\n",
      "Validation F1 is: 0.04916517396775352 %\n",
      "Epoch [3/3], Step [100/400], Loss: 3.4315\n",
      "Epoch [3/3], Step [200/400], Loss: 3.7774\n",
      "Epoch [3/3], Step [300/400], Loss: 3.7181\n",
      "Epoch [3/3], Step [400/400], Loss: 3.5262\n",
      "Training accuracy is: 9.515859766277128 %\n",
      "Training F1 is: 0.07502610849216221 %\n",
      "Validation accuracy is: 9.181969949916528 %\n",
      "Validation F1 is: 0.07360003826856681 %\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAAxmUlEQVR4nO3dd3hUZfr/8fedBBIILZRQA4QqARJAhAVEiQFBURBBOgmWVVFEXd0Vu8uubWXVrz901XV1E0ACyqIoIitFAVGkmBCKtISS0EKvIe35/fFMhoGlJGQmk0zu13VxOXPOmZk7h/HDyTnPuR8xxqCUUsp3+Xm7AKWUUp6lQa+UUj5Og14ppXycBr1SSvk4DXqllPJxAd4u4EK1a9c2TZs29XYZSilVpqxZs+agMabOxdaVuqBv2rQpq1ev9nYZSilVpojIzkut01M3Sinl4zTolVLKx2nQK6WUjyt15+iVUr4rJyeH9PR0srKyvF1KmRUUFESjRo2oUKFCoV+jQa+UKjHp6elUrVqVpk2bIiLeLqfMMcZw6NAh0tPTCQ8PL/Tr9NSNUqrEZGVlUatWLQ35qyQi1KpVq8i/EWnQK6VKlIZ88VzN/vOZoD+WdYw//vePfLvtW05ln/J2OUopVWr4TNCnHEjhnV/e4ZbptxDyegg3/vtGJv0wiRW7V5CTl+Pt8pRSpcChQ4fo0KEDHTp0oF69ejRs2ND5PDs7+7KvXb16NRMmTCjS5zVt2pSDBw8Wp2S3kNI28Ujnzp3N1d4ZezrnND/u+pGFqQtZlLaItXvXYjBUqViFXk17ERMeQ0x4DO1C2+mvj0p5waZNm2jTpo23ywDgpZdeokqVKjz55JPOZbm5uQQEuG+MSsGd/rVr13bbe8LF96OIrDHGdL7Y9j416qZyhcr0ad6HPs37AHD4zGGWpC1xBv/XW74GoG5wXW4Kv4nezXoTEx5DkxpNvFm2UsqLxo4dS1BQEL/++is9evRg+PDhPProo2RlZVGpUiU++eQTWrduzffff8/kyZP5+uuveemll9i1axepqans2rWLxx577IpH+2+++SYff/wxAPfddx+PPfYYp06dYujQoaSnp5OXl8fzzz/PsGHDmDhxInPnziUgIICbb76ZyZMnF+tn9Kmgv1DNSjUZHDGYwRGDAdh1bBeLUhexKG0RC1MXMmP9DABa1GzhPNq/KfwmalWu5c2ylSofHnsMkpLc+54dOsDbbxf5Zenp6axYsQJ/f3+OHz/OsmXLCAgIYOHChTzzzDPMnj37f17z22+/sWTJEk6cOEHr1q0ZN27cJce2r1mzhk8++YSVK1dijKFr167ceOONpKam0qBBA+bNmwfAsWPHOHToEHPmzOG3335DRDh69GiRf54L+XTQX6hx9cbc3fFu7u54N8YYNmZudB7tf5ryKR+s+QBB6FCvg/Nov2eTnlSuUNnbpSulPOiuu+7C398fsGEbFxfH1q1bERFyci5+ja9///4EBgYSGBhIaGgo+/fvp1GjRhfddvny5QwaNIjg4GAA7rzzTpYtW0a/fv144okneOqpp7jtttvo2bMnubm5BAUFce+993Lbbbdx2223FfvnK1dB70pEaBvalrahbXn0d4+Sm5/LqoxVzqP9t39+mzdWvEFF/4p0a9TNGfzXNbyOAL9yu9uUcp+rOPL2lIIABnj++eeJjo5mzpw57Nixg169el30NYGBgc7H/v7+5ObmFvlzW7Vqxdq1a/nmm2947rnniImJ4YUXXuCXX35h0aJFfP7550yZMoXFixcX+b1daWI5BPgF0C2sG93CuvHcDc9xKvsUy3ctdwb/C0te4Pklz1MtsBo3NrmRmPAYejfrTUSdCL2wq5QPOXbsGA0bNgTg3//+t1ves2fPnowdO5aJEydijGHOnDlMnTqVPXv2ULNmTUaPHk2NGjX46KOPOHnyJKdPn+bWW2+lR48eNGvWrNifr0F/CcEVg+nboi99W/QF4ODpgyxJW8KiNHuO/6stXwFQr0o95/n9mGYxNK7e2JtlK6WK6U9/+hNxcXH89a9/pX///m55z06dOjF27Fi6dOkC2IuxHTt2ZMGCBfzxj3/Ez8+PChUq8I9//IMTJ04wcOBAsrKyMMbw5ptvFvvzfWp4ZUnaeXSn82h/UdoiDpw6AEDLmi2dR/vR4dHUrFTTy5UqVXqUpuGVZVm5Hl5ZkprUaMI9He/hno73YIxh/YH1zqP9aSnTeH/N+whCp/qdnMHfo3EPvbCrlCpxekTvATl5Oazas8p5tP/T7p/Iyc+hon9FeoT1cJ7m6dygs17YVeWKHtG7R1GP6DXoS8Cp7FMs27XMGfxJ+5IAqBZYjV5Ne9E7vDcxzWJoU7uNXthVPk2D3j301E0pFFwxmH4t+tGvRT8AMk9lsmTHEhalLmJh2kLmbp4LQP0q9YlpFuO8uBtWPcybZSulfIQGvRfUCa7D0LZDGdp2KABpR9Kc5/cXbFvAtHXTAGhVq5XzaD+6aTQhlUK8WbZSqozSoC8FwkPCuS/kPu7rdB/5Jt9e2HUc7ccnx/Pe6vcQhGsbXOsM/h5hPahUoZK3S1dKlQE+06bYV/iJH5F1I3m82+PMGzmPw08dZtndy3jxxhcJCghi8k+T6TO1DyGvhxCTEMMry17hl4xfyMvP83bpSpV60dHRLFiw4Lxlb7/9NuPGjbvka3r16sXFrhteanlppEFfylX0r8j1ja/nxV4vsuzuZRx56gjzRs7j4ese5tDpQzy7+Fm6ftSVWn+rxaCZg5jyyxQ2ZW6itF1kV6o0GDFiBImJiectS0xMZMSIEV6qqGRo0JcxVSpW4daWt/L3vn8n6cEk9j+5n8TBidwVcRfJ+5J5ZP4jRLwXQaO3GhE7J5aE5AQyjmd4u2ylSoUhQ4Ywb9485yQjO3bsYM+ePfTs2ZNx48bRuXNn2rZty4svvlik950xYwbt27enXbt2PPXUUwDk5eUxduxY2rVrR/v27XnrrbcAeOedd4iIiCAyMpLhw4e79we8BD1HX8aFBocyrN0whrUbBkDqkVRnK+b52+Yzdd1UAK6pfY3zxq1eTXtRI6iGF6tWCh779jHnUGN36VCvA2/3e/uS62vWrEmXLl2YP38+AwcOJDExkaFDhyIivPzyy9SsWZO8vDxiYmJYt24dkZGRV/zMPXv28NRTT7FmzRpCQkK4+eab+eKLLwgLCyMjI4P169cDONsNv/baa6SlpREYGOiWFsSFUagjehHpJyKbRWSbiEy8yPoHRSRFRJJEZLmIRDiWNxWRM47lSSLyvrt/AHW+ZiHN+P21vydxSCL7n9xP0gNJTO4zmfAa4XyS9AmDZg6i1t9q0fWjrjyz6BkWpS4iK7doM8orVZa5nr5xPW0za9YsOnXqRMeOHdmwYQMbN24s1PutWrWKXr16UadOHQICAhg1ahRLly6lWbNmpKam8sgjj/Dtt99SrVo1ACIjIxk1ahTTpk1z62xWl3PFTxERf+BdoA+QDqwSkbnGGNe98Kkx5n3H9gOAN4F+jnXbjTEd3Fq1KhQ/8SOqXhRR9aJ4ovsTZOdlszJ9pfPGrTdWvMGry18lKCCIHmE9nK2YO9XvhL+fv7fLVz7uckfenjRw4EAef/xx1q5dy+nTp7n22mtJS0tj8uTJrFq1ipCQEMaOHUtWVvEOgEJCQkhOTmbBggW8//77zJo1i48//ph58+axdOlSvvrqK15++WVSUlI8HviFefcuwDZjTCqAiCQCAwFn0BtjjrtsHwzolcBSqKJ/RXo26UnPJj35c/SfOXH2BEt3LnU2Z3t60dMA1AiqQXTTaOepnla1Wukdu8pnVKlShejoaO655x7n0fzx48cJDg6mevXq7N+/n/nz51+yD/2FunTpwoQJEzh48CAhISHMmDGDRx55hIMHD1KxYkUGDx5M69atGT16NPn5+ezevZvo6Giuv/56EhMTOXnyJDVq1PDcD0zhgr4hsNvleTrQ9cKNRORh4A9AReAml1XhIvIrcBx4zhiz7CKvvR+4H6BxY23zW1KqBlalf6v+9G9lW7HuP7mfxWmLncE/57c5ADSs2tB5tB/TLIYGVRt4s2ylim3EiBEMGjTIeQonKiqKjh07cs011xAWFkaPHj0K/V7169fntddeIzo6GmMM/fv3Z+DAgSQnJ3P33XeTn58PwKuvvkpeXh6jR4/m2LFjGGOYMGGCx0MeCtHrRkSGAP2MMfc5no8Buhpjxl9i+5FAX2NMnIgEAlWMMYdE5FrgC6DtBb8BnMcXe92URcYYe2HXEfqL0xZz6MwhANrUbnPehd3qQdW9XK0qK7TXjXt4otdNBuDadKWRY9mlJAL/ADDGnAXOOh6vEZHtQCtAk7yUExGa12xO85rNuf/a+8k3+STvS3YG/8dJHzNl1RT8xI/rGlznDP5uYd0ICgjydvlKKReFCfpVQEsRCccG/HBgpOsGItLSGLPV8bQ/sNWxvA5w2BiTJyLNgJZAqruKVyXHT/zoWL8jHet35MnuT3I29ywrM85d2H39x9d5ZfkrBAUE0bNxT+dpno71OuqFXaW87IpBb4zJFZHxwALAH/jYGLNBRCYBq40xc4HxItIbyAGOAHGOl98ATBKRHCAfeNAYc9gTP4gqWYEBgdzQ5AZuaHIDk6IncfzscZbuXOoM/omLJsIiCAkKITo82tmjp2XNlnpht5wzxuh3oBiu5q537UevPGLfyX32wq6jOduuY7sACKsWdl4r5vpV63u5UlWS0tLSqFq1KrVq1dKwvwrGGA4dOsSJEycIDw8/b51OPKK8yhjD9iPbnUf7i9MWc/iM/cUuok6E82j/xiY36oVdH5eTk0N6enqxx6iXZ0FBQTRq1IgKFSqct1yDXpUq+SafpH1JzuBftnMZZ3LP4C/+XNfwOmfwd2vUjcCAQG+Xq1SZoEGvSrWzuWf5Kf0n52meVRmryDN5VAqoRM8mPZ0jejrU64CfaB8+pS5Gg16VKceyjvHDzh+cwb8x096EXbNSTW4Kv8l5fr9FzRZ6nlcpBw16VabtPbGXxWmLWZi2kEWpi9h93N6o3bh6Y+fR/k3hN1GvSj0vV6qU92jQK59hjGHr4a3Oo/0laUs4knUEgHah7ZzBf0OTG6gWWM3L1SpVcjTolc/Ky887/8LurmVk5WbhL/50adjF2aPnd41+pxd2lU/ToFflRlZuFj/t/skZ/Kv2rCLf5FO5QmV6Nu7pDP6oelF6YVf5FA16VW4dzTrKDzt+cPbo2XRwEwC1KtVyXtjt3aw3zUKa6YVdVaZp0CvlsOfEHudUiwtTF5Jxwvbna1K9ifNo/6bwm6hbpa6XK1WqaDTolboIYwxbDm1xhv6SHUs4mnUUgPah7c+7sFs1sKp3i1XqCjTolSqEvPw81u5d6wz+5buWczbvLAF+AXRt2JWY8BgGRwwmsu6VJ4xWqqRp0Ct1Fc7knGHF7hXO4F+zdw35Jp8O9ToQGxnLyPYj9RSPKjU06JVyg4OnDzJz/Uzik+NZtWcV/uJPvxb9iI2KZUDrATrhivIqDXql3GxT5iamrpvK1HVTST+eTvXA6gxrO4zYqFi6h3XXETyqxGnQK+Uhefl5fL/je+KT45m9aTanc07TPKQ5sVGxjIkcQ3hI+JXfRCk30KBXqgSczD7J7I2zSViXwJK0JRgMNzS5gbioOIZEDNGWDMqjNOiVKmG7ju1i2rppxCfHs+XQFoICghh0zSDiouLo3ay3zqOr3E6DXikvMcbwS8YvJCQnMGP9DI5kHaF+lfqMjhxNbFQs7ULbebtE5SM06JUqBc7mnmXe1nkkJCcwb+s8cvNz6VivI3FRcYxoP4LQ4FBvl6jKMA16pUqZzFOZJK5PJD45njV71+Av/tzS8hbiouK4rdVtOlRTFZkGvVKl2IYDG5xDNfec2EONoBoMbzuc2KhYftfodzpUUxWKBr1SZUBefh6L0xaTsC6B2Rtncyb3DC1rtnQO1WxSo4m3S1SlmAa9UmXMibMnmL1pNvHJ8Xy/43sAejXtRWxkLEMihmiTNfU/NOiVKsN2Ht3J1HVTSUhOYOvhrVQKqMSdbe4kNiqWmPAYHaqpAA16pXyCMYaf038mITmBxA2JHM06SoOqDRjdfjRxHeKIqBPh7RKVF2nQK+VjsnKz+HrL1yQkJ/DN1m/IM3lcW/9aYqNiGdFuBHWC63i7RFXCNOiV8mEHTh1gRsoMEtYlsHbvWgL8Ari15a3ERcXRv2V/nRS9nNCgV6qcWH9gPQnJCUxbN429J/dSs1JN51DNLg276FBNH6ZBr1Q5k5efx8LUhSSsS2DOpjmcyT1D61qtiY2KZXTkaBpXb+ztEpWbXS7o/Qr5Bv1EZLOIbBORiRdZ/6CIpIhIkogsF5GIC9Y3FpGTIvLk1f0ISqmi8Pfzp2+Lvky/czr7ntzHvwb8i7pV6vLs4mdp+nZTYhJiiE+K52T2SW+XqkrAFY/oRcQf2AL0AdKBVcAIY8xGl22qGWOOOx4PAB4yxvRzWf85YICVxpjJl/s8PaJXynPSjqQ5h2puP7KdyhUqM7jNYGKjYoluGq1DNcuw4h7RdwG2GWNSjTHZQCIw0HWDgpB3CMaGesGH3wGkARuKWLdSys3CQ8J54cYX2PrIVn6850dGtx/N3M1z6TO1D03/rylPL3yaTZmbvF2mcrPCBH1DYLfL83THsvOIyMMish34GzDBsawK8BTw5+KXqpRyFxGhe1h3Prj9A/Y9uY+ZQ2YSVTeKN1a8QcR7EXT5Zxem/DKFQ6cPebtU5QaFOkdfGMaYd40xzbHB/pxj8UvAW8aYy54IFJH7RWS1iKzOzMx0V0lKqUIICghiaNuhfD3yazL+kMGbN79JTn4Oj8x/hPp/r8+gmYP44rcvyM7L9nap6ioV5hx9N+AlY0xfx/OnAYwxr15iez/giDGmuogsA8Icq2oA+cALxpgpl/o8PUevVOmwbv86EpITmJ4ynX0n91GrUi2GtxtOXFQcnRt01qGapUyxhleKSAD2YmwMkIG9GDvSGLPBZZuWxpitjse3Ay9e+IEi8hJwUi/GKlW25Obn8t3270hYl8AXv31BVm4WbWq3ITYqllHtRxFWPezKb6I8rlgXY40xucB4YAGwCZhljNkgIpMcI2wAxovIBhFJAv4AxLmndKWUtwX4BXBLy1uYMXgG+57Yxz9v/ye1K9fm6UVP0+TtJvRO6M3U5Kk6VLMU0xumlFJXZfvh7UxbN42EdQmkHkkluEIwQyKGEBsVS6+mvfATt10CVIWgd8YqpTzGGMOPu38kPimeWRtncfzsccKqhTEmcgyxUbG0rt3a2yWWCxr0SqkScSbnDF9u/pKE5AQWbF9Avsmna8OuxEbFMrzdcGpWquntEn2WBr1SqsTtPbGXT1M+JT45npQDKVTwq8DtrW8nNjKWW1reQkX/it4u0ado0CulvCp5XzLxyfFMT5nOgVMHqF25NiPajSAuKo5O9TvpUE030KBXSpUKufm5LNi2gIR1CXz525eczTtLRJ0IYiNtV82G1f7npntVSBr0SqlS52jWUWZtmEVCcgI/7v4RQejdrDdxUXHccc0dBFcM9naJZYoGvVKqVNt2eBtTk6eSsC6BHUd3UKViFYZEDCEuKo4bmtygQzULQYNeKVUm5Jt8lu9aTkJyArM2zOJE9gmaVG/CmMgxjIkaQ6tarbxdYqmlQa+UKnNO55zmy9++JGFdAv/d/l/yTT7dGnUjNiqWYW2HEVIpxNsllioa9EqpMm3PiT3OoZrrD6ynon9FBrQeQGxkLP1a9KOCfwVvl+h1GvRKKZ9gjCFpXxLxyfF8mvIpmaczqVO5DiPbjyQ2KpaO9TqW26GaGvRKKZ+Tk5fDgu0LiE+OZ+7muWTnZdMutB2xkbGMihxFg6oNvF1iidKgV0r5tCNnjjBzw0wSkhP4Kf0n/MSPPs36EBsVyx3X3EHlCpW9XaLHadArpcqNLYe2OIdq7jq2i6oVq3JXxF3EdYjj+sbX++xQTQ16pVS5k2/yWbpzKQnJCXy28TNOZp+kaY2mxEbGMiZqDC1qtvB2iW6lQa+UKtdOZZ/ii9++IGFdAt9t/w6DoXtYd+Ki4hjadig1gmp4u8Ri06BXSimHjOMZTE+ZTnxyPBszNxLoH8iA1gOIi4rj5uY3l9mhmhr0Sil1AWMMa/euJSE5gU/Xf8rB0wcJDQ5lVPtRxEbF0qFeB2+XWCQa9EopdRnZedl8u+1b4pPj+WrzV+Tk5xBZN5LYyFhGth9J/ar1vV3iFWnQK6VUIR06fcg5VHNlxkr8xI++zfsSGxXLwNYDqVShkrdLvCgNeqWUugqbD24mITmBqeumsvv4bqoFVmNoxFDiOsTRI6xHqboLV4NeKaWKId/k8/2O70lITuDzjZ9zKucUzUKaOSdAbxbSzNslatArpZS7nMw+yZxNc0hYl8Ci1EUYDNc3vp64qDjuiriL6kHVvVKXBr1SSnnA7mO7nUM1fzv4G0EBQQxsPZC4qDj6NO9DgF9AidWiQa+UUh5kjGH1ntUkJCcwY/0MDp05RN3guoxqP4q4DnFE1o30eA0a9EopVUKy87L5Zus3xCfHM2/LPHLyc4iqG0VcVBwj24+kbpW6HvlcDXqllPKCg6cPMnP9TOKT41m1ZxX+4k+/Fv2IjYplQOsBBAUEue2zykfQZ2XBO+/A4MHQvLn7C1NKqWLYlLnJOVQz40QG1QOrM6ztMGKjYuke1r3YQzXLR9B//z1ER9vHHTvCXXfBkCHQsqVb61NKqeLIy8/j+x3fE58cz+xNszmdc5rmIc2JjYplTOQYwkPCr+p9y0fQA+zcCZ9/Dp99BitX2mVRUTb077oLWukM8kqp0uNk9klmb5xNwroElqQt4fbWt/Pl8C+v6r3KT9C72rULZs+2of/TT3ZZZOS50G/duvifoZRSbrLr2C5OZZ+iTZ02V/X6ywV9oaZaEZF+IrJZRLaJyMSLrH9QRFJEJElElotIhGN5F8eyJBFJFpFBV/UTXI3GjeHxx2HFChv6b70FVarA88/DNddA+/YwaRJs2lRiJSml1KU0rt74qkP+Sq54RC8i/sAWoA+QDqwCRhhjNrpsU80Yc9zxeADwkDGmn4hUBrKNMbkiUh9IBhoYY3Iv9XkeH3WTkXHuSP/HH8EYaNv23JF+RITnPlsppTykuEf0XYBtxphUY0w2kAgMdN2gIOQdggHjWH7aJdSDCpZ7VcOGMGECLFsG6el2pE7NmvDnP9vAj4iAF1+E9evtPwJKKVXGFSboGwK7XZ6nO5adR0QeFpHtwN+ACS7Lu4rIBiAFePBiR/Micr+IrBaR1ZmZmUX9Ga5egwbwyCOwdKk90p8yBUJD4S9/sad2IiLghRcgJUVDXylVZrltOnRjzLvGmObAU8BzLstXGmPaAtcBT4vI/9whYIz50BjT2RjTuU6dOu4qqWjq14eHH7bDNPfsgXffhXr14OWX7UXcNm3guecgOVlDXylVphQm6DOAMJfnjRzLLiURuOPChcaYTcBJoF0R6vOOevXgoYdgyRIb+v/4hz3l8+qr0KGDHbHz7LOQlKShr5Qq9QoT9KuAliISLiIVgeHAXNcNRMT1rqT+wFbH8nARCXA8bgJcA+xwQ90lp25dePBBWLQI9u6F99+3I3pee83emNWqFTz9NKxdq6GvlCqVrhj0jnPq44EFwCZgljFmg4hMcoywARgvIhtEJAn4AxDnWH49kOxYPgc7Guegm3+GkhMaCg88AAsXwr598OGHEB4Ob7wB115r78KdOBHWrNHQV0qVGr57w1RJOngQvvjCDtlctAjy8uw/AEOG2CGbnTtDKZpyTCnle8rnnbHecugQfPmlDf2FCyE3F5o2PRf6112noa+UcjsNem85fPj80M/JgSZNbOgPGQJdu2roK6XcQoO+NDhyBObOtaH/3//a0A8LO3ek37Ur+LlttKtSqpwpdq8b5QYhIRAXB19/DQcOQHy87az57rvQvbs90n/8cduWIT/f29UqpXyIBr031KgBsbHw1Vc29KdOhU6d4L334Prr7fDNRx+F5cs19JVSxaZB723Vq8Po0fZcfmYmTJtmR+l88AH07GlP7xT05tHQV0pdBQ360qRaNRg1yg7VPHAApk+35+7/+U+44QZo1AjGj4cffrBDOJVSqhA06EuratVg5Ej4z39s6M+YAd26wb/+Bb162dAv6M2joa+UugwN+rKgalUYPtz20c/MhMREey7/k0/sPLkNGsC4cbB4sR23r5RSLjToy5oqVWDYMDtMMzMTZs2CG2+EhASIibGhX9CbR0NfKYUGfdkWHGzH4M+aZU/vfPYZ3HSTvaDbu7dtvfzAA/Dddxr6SpVjGvS+IjjY3nyVmGhDf/ZsG/bTp8PNN9vWy7///bmbtZRS5YYGvS+qXBnuvNNewM3MtBd0+/a1/wj07WtD/7774NtvNfSVKgc06H1dpUowaJA9ss/MtEM3b7nFnu655Rbbb/+ee2D+fMjO9na1SikP0KAvT4KCYOBAew7/wAF7k9Ztt9nTPLfeakP/7rth3jwNfaV8iAZ9eRUUBAMG2NE6Bw7YdgwDBsCcOTb8Q0PP9eY5e9bb1SqlikGDXkFgoA33+HjYv9+G+x132G6bt99uj/QLevNkZXm7WqVUEWnQq/MFBkL//vDvf9vQ/+Ybe2H366/tEX9o6LnePBr6SpUJGvTq0ipWtBdsP/7YzpE7f74dtz9/vj3iDw0915vnzBlvV6uUugQNelU4FStCv362186+fbBggb1Dd8ECO6onNBRGjLBDOTX0lSpVNOhV0VWoYG/C+uc/Ye9eexPWiBF2usTBg6FOnXO9eU6f9na1SpV7GvSqeCpUgD594MMPbegvXGjP4S9ebO/UrVPnXG+eU6e8Xa1S5ZIGvXKfgADbWO3992HPHttYLTbWtlIeOtSe3inozaOhr1SJ0aBXnhEQYBus/eMfNvSXLIGxY+1MWcOG2SP9gt48J096u1qlfJoGvfI8f387Wcq770JGhj3Cv+ceOxH6iBE29At685w44e1qlfI5GvSqZPn72/75U6ZAejosXWq7av78s51Rq04dO4rn00/h+HFvV6uUT9CgV97j728nQH/nHRv6y5bZ/vmrVtnx+aGh53rzHDvm7WqVKrM06FXp4Odnp0f8v/+DXbvsaZ1x42DtWhgzxob+gAEwdSocPertapUqUzToVenj5wfdu8Nbb8HOnbBihZ0IPSnJjuIJDT3Xm0dDX6kr0qBXpZufH3TrBm++aUP/559hwgRISbGjeEJDz/XmOXLE29UqVSoVKuhFpJ+IbBaRbSIy8SLrHxSRFBFJEpHlIhLhWN5HRNY41q0RkZvc/QOockQEunaFyZNhxw5YuRIefRQ2bLB99ENDz/XmOXzY29UqVWqIMebyG4j4A1uAPkA6sAoYYYzZ6LJNNWPMccfjAcBDxph+ItIR2G+M2SMi7YAFxpiGl/u8zp07m9WrVxfrh1LljDGwejV8/rm9Azct7dzNW3fdZRuw1arl7SqV8igRWWOM6XyxdYU5ou8CbDPGpBpjsoFEYKDrBgUh7xAMGMfyX40xexzLNwCVRCSwqD+AUpclAtddB6+/Dtu329B/4gnYutXOjVu37rnePAcPertapUpcYYK+IbDb5Xm6Y9l5RORhEdkO/A2YcJH3GQysNcb8z3RFInK/iKwWkdWZmZmFq1ypixGBa6+F116DbdtgzRr405/sUf7999uJ0Qt68+h3TZUTbrsYa4x51xjTHHgKeM51nYi0BV4HHrjEaz80xnQ2xnSuU6eOu0pS5Z0IdOoEr7wCW7bAr7/CU0/Zi7oPPGBDv6A3z4ED3q5WKY8pTNBnAGEuzxs5ll1KInBHwRMRaQTMAWKNMduvokalik8EOnSAl1+GzZvtUM1nnrEtGcaNg/r1z/Xm2b/f29Uq5VaFCfpVQEsRCReRisBwYK7rBiLS0uVpf2CrY3kNYB4w0Rjzo1sqVqq4RCAqCv7yF9i0Cdatg2eftW2WH3oIGjSA6Gjbm2ffPm9Xq1SxXTHojTG5wHhgAbAJmGWM2SAikxwjbADGi8gGEUkC/gDEFSwHWgAvOIZeJolIqNt/CqWulgi0bw+TJsHGjXZ8/vPP21M548fb0C/ozbN3r7erVeqqXHF4ZUnT4ZWq1Ni40Q7X/OwzO1ZfxPbmGT783KQqSpUSxR1eqVT5FBEBL74I69fb0H/pJTtS56GH7Dn9vn3tzVl6R64q5TTolSqMNm3ghRfskf26dTBxoh2zf++9dpz+gAEwfbr201elkga9UkVRcE7/r3+1N2StWmXbMCQl2blyC6ZL/PxzOHPG29UqBWjQK3X1RKBzZ3jjDdt7Z/lyeyfusmU27ENDbfh//TVkZ3u7WlWOadAr5Q5+ftCjB/y//2fH5i9aZKdJnD8fbr/dnt6591747jvIzfV2taqc0aBXyt38/e3NVx9+aMfhf/ONPYf/+ee2506DBvaC7tKlkJ/v7WpVOaBBr5QnVahgWyfHx9s7bufMsf8IxMfb8fmNG8Pjj9uWy6VsqLPyHRr0SpWUoCDbMjkx0d6QlZhou26+9x787nfQrJkdzZOUpKGv3EqDXilvCA6GYcPsEf6BA3aGrDZt4O9/h44d7eOXXrItGpQqJg16pbytenWIi7Pn8vfuhQ8+gIYNbVuGiAjbl+fVVyE11duVqjJKg16p0qR2bds3f9EiO3rnnXegalXbabN5c+jSxc6fm57u7UpVGaJBr1RpVb8+PPKIHZ+/c6cdr5+fb2fPCguzfXfefVfbKqsr0qBXqixo3BiefNJOk7h1q70z9+jRcx02e/eGjz7SSdHVRWnQK1XWtGhh++enpNiGa88+C7t2we9/b2/M6t8fpk6F48ev/F6qXNCgV6osa9vWXrTdvNnOj/uHP9jwj421LRgGD4ZZs+D0aW9XqrxIg14pX1AwP+7rr9u+OytWwIMPwk8/2WGcoaEwciTMnQtnz3q7WlXCNOiV8jUi0K0bvP027N4NS5bAmDHw3//CwIH29M7dd8O330JOjrerVSVAg14pX+bvD7162UnP9+614T5okL1R65Zb7MieBx+E77+HvDxvV6s8RINeqfKiQgU7K9Ynn9ghmV9+aZusTZtmJ0Nv1Mj21v/pJ23B4GM06JUqjwIDbUfNTz+1LRhmzYLu3e1dud27Q3g4/OlPsHathr4P0KBXqryrXNlOlDJ7tg39qVOhXTt46y249lpo3Rqef95Oo6jKJA16pdQ51aqdmxVr/357E1aTJvDKKzb8XadRVGWGBr1S6uJq1jw3K9aePTBlCoSE2KP7Vq3sNIqTJ9ubtVSppkGvlLqyunXh4YftrFi7dtl2yv7+8Mc/2iP+gmkU9+3zdqXqIjTolVJFExZm78BduRK2b7endU6ehAkTbN+dgmkUDx70dqXKQYNeKXX1mjWDp5+G5GTYuBFeeMGe5nngATtGv2AaxWPHvF1puaZBr5RyD9dZsZKSbLfN336DsWNtC4aCaRRPnfJuneWQBr1Syr1Ezp8Va+VKe35/1SoYMcKGfsE0illZ3q62XNCgV0p5jsi5WbF274YffrBH+EuWwJ132tAvmEZR++54jAa9Uqpk+PnBDTfYWbH27LFN1u66y3bU7N8f6tWz0yguXqx9d9ysUEEvIv1EZLOIbBORiRdZ/6CIpIhIkogsF5EIx/JaIrJERE6KyBR3F6+UKqMCAqBPH/jXv+yNWV99ZS/czpgBMTF2cvSCaRTz871dbZkn5gp9LETEH9gC9AHSgVXACGPMRpdtqhljjjseDwAeMsb0E5FgoCPQDmhnjBl/pYI6d+5sVq9efbU/j1KqLDtzxp7GSUy0d+dmZdlma8OGwfDhtiWDiLerLJVEZI0xpvPF1hXmiL4LsM0Yk2qMyQYSgYGuGxSEvEMwYBzLTxljlgN6xUUpdWWVKtlZsT77zPbdmT4dOnaEd96B6647fxpFbbZWaIUJ+obAbpfn6Y5l5xGRh0VkO/A3YEJRihCR+0VktYiszszMLMpLlVK+qmrVc7Ni7d8PH39sg/711yEy0vbemTQJtmzxdqWlntsuxhpj3jXGNAeeAp4r4ms/NMZ0NsZ0rlOnjrtKUkr5ipAQOyvWggX2Qu5770GdOnbcfuvW50+jqP5HYYI+Awhzed7IsexSEoE7ilGTUkpdWmgojBtnZ8Xavdu2Uw4MhIkTbR/9gmkUMy4XU+VLYYJ+FdBSRMJFpCIwHJjruoGItHR52h/QHqZKKc9r2BAee8zOipWaCq+9Zic/f/xx25OnVy94/30o56eErzjqBkBEbgXeBvyBj40xL4vIJGC1MWauiPwf0BvIAY4A440xGxyv3QFUAyoCR4GbXUfsXEhH3Silim3zZpg5047e2bTJdtqMibEjdwYNgho1vF2h211u1E2hgr4kadArpdzGGFi/3gZ+YqI96q9QAfr1s6E/YABUqeLtKt2iuMMrlVKqbBKxs2K9/DJs22b77UyYAL/+CqNG2fP9BdMonjnj7Wo9RoNeKVU+iJybFWvnTli2zM6gtXQpDBliQ79gGsXsbG9X61Ya9Eqp8sfPD66/3s6KlZEBCxfazprz58Ptt9sZtQqmUczN9Xa1xaZBr5Qq3wIC7IXaDz+EvXth3jx77v6zz+Dmm+2sWQXTKJbRvjsa9EopVaBiRbj1Vjsr1oED8J//QHQ0fPIJ3HgjNG5sp1H85Zcy1YJBg14ppS4mKMgOxZw504b+jBn2HP+770LXrtC8uZ1GMSmp1Ie+Br1SSl1JlSp2OOYXX9i+O//+t2298MYbtuma6zSKpZAGvVJKFUWNGnZWrPnzYd8++OADex5/0iSIiDh/GsVSQoNeKaWuVu3a52bFysiw7ZSrVIFnnrGndrp2tdMopqd7tUwNeqWUcof69e2sWD/+aMfpv/GGnRLxiSds352CaRT37y/x0jTolVLK3Ro3hiefhNWrbb/8v/wFjhyB8ePtaZ4+feCjj+Dw4RIpR4NeKaU8qWVLeO45OytWSoo9rbNjB/z+9/bGrNtug2nT4PjxK77V1dKgV0qpktKunT2637IF1qyx7ZRTUmDMGNuC4YknPPKxGvRKKVXSROysWH/7G6SlwYoV8MAD0KSJRz4uwCPvqpRSqnD8/OysWN26ee4jPPbOSimlSgUNeqWU8nEa9Eop5eM06JVSysdp0CullI/ToFdKKR+nQa+UUj5Og14ppXycmFI2M4qIZAI7i/EWtYGDbirHnbSuotG6ikbrKhpfrKuJMabOxVaUuqAvLhFZbYzp7O06LqR1FY3WVTRaV9GUt7r01I1SSvk4DXqllPJxvhj0H3q7gEvQuopG6yoaratoylVdPneOXiml1Pl88YheKaWUCw16pZTycWUm6EWkn4hsFpFtIjLxIusDRWSmY/1KEWnqsu5px/LNItK3hOv6g4hsFJF1IrJIRJq4rMsTkSTHn7klXNdYEcl0+fz7XNbFichWx5+4Eq7rLZeatojIUZd1ntxfH4vIARFZf4n1IiLvOOpeJyKdXNZ5cn9dqa5RjnpSRGSFiES5rNvhWJ4kIqtLuK5eInLM5e/rBZd1l/0OeLiuP7rUtN7xnarpWOfJ/RUmIkscWbBBRB69yDae+44ZY0r9H8Af2A40AyoCyUDEBds8BLzveDwcmOl4HOHYPhAId7yPfwnWFQ1UdjweV1CX4/lJL+6vscCUi7y2JpDq+G+I43FISdV1wfaPAB97en853vsGoBOw/hLrbwXmAwL8Dljp6f1VyLq6F3wecEtBXY7nO4DaXtpfvYCvi/sdcHddF2x7O7C4hPZXfaCT43FVYMtF/p/02HesrBzRdwG2GWNSjTHZQCIw8IJtBgLxjsefAzEiIo7licaYs8aYNGCb4/1KpC5jzBJjzGnH05+BRm767GLVdRl9ge+MMYeNMUeA74B+XqprBDDDTZ99WcaYpcDhy2wyEEgw1s9ADRGpj2f31xXrMsascHwulNz3qzD761KK8910d10l+f3aa4xZ63h8AtgENLxgM499x8pK0DcEdrs8T+d/d5JzG2NMLnAMqFXI13qyLlf3Yv/FLhAkIqtF5GcRucNNNRWlrsGOXxE/F5GwIr7Wk3XhOMUVDix2Weyp/VUYl6rdk/urqC78fhngvyKyRkTu90I93UQkWUTmi0hbx7JSsb9EpDI2LGe7LC6R/SX2tHJHYOUFqzz2HdPJwUuIiIwGOgM3uixuYozJEJFmwGIRSTHGbC+hkr4CZhhjzorIA9jfhm4qoc8ujOHA58aYPJdl3txfpZqIRGOD/nqXxdc79lco8J2I/OY44i0Ja7F/XydF5FbgC6BlCX12YdwO/GiMcT369/j+EpEq2H9cHjPGHHfne19OWTmizwDCXJ43ciy76DYiEgBUBw4V8rWerAsR6Q08CwwwxpwtWG6MyXD8NxX4HvuvfInUZYw55FLLR8C1hX2tJ+tyMZwLfq324P4qjEvV7sn9VSgiEon9OxxojDlUsNxlfx0A5uC+U5ZXZIw5bow56Xj8DVBBRGpTCvaXw+W+Xx7ZXyJSARvy040x/7nIJp77jnniwoO7/2B/80jF/ipfcAGn7QXbPMz5F2NnOR635fyLsam472JsYerqiL341PKC5SFAoONxbWArbrooVci66rs8HgT8bM5d+Elz1BfieFyzpOpybHcN9sKYlMT+cvmMplz64mJ/zr9Q9oun91ch62qMve7U/YLlwUBVl8crgH4lWFe9gr8/bGDucuy7Qn0HPFWXY3117Hn84JLaX46fPQF4+zLbeOw75rad6+k/2CvSW7Ch+axj2STsUTJAEPCZ40v/C9DM5bXPOl63GbilhOtaCOwHkhx/5jqWdwdSHF/0FODeEq7rVWCD4/OXANe4vPYex37cBtxdknU5nr8EvHbB6zy9v2YAe4Ec7DnQe4EHgQcd6wV411F3CtC5hPbXler6CDji8v1a7VjezLGvkh1/z8+WcF3jXb5fP+PyD9HFvgMlVZdjm7HYARqur/P0/roeew1gncvf1a0l9R3TFghKKeXjyso5eqWUUldJg14ppXycBr1SSvk4DXqllPJxGvRKKeXjNOiVUsrHadArpZSP+/+t2zrP/CmMggAAAABJRU5ErkJggg==\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_values = train(model,num_epochs=num_epochs,lr=learning_rate)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is: 11.148086522462563 %\n",
      "Test f1 is: 9.332177218117007 %\n",
      "Test Loss: 0.3287 %\n"
     ]
    }
   ],
   "source": [
    "test_predicted, test_labels , test_values = test(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "def TransPoseVisualizer(exampledataset, out_path, out_index):\n",
    "    import shutil\n",
    "    activation = {}\n",
    "    def get_activation(name):\n",
    "        def hook(model, input, output):\n",
    "            activation[name] = output.detach()\n",
    "        return hook\n",
    "    model.tph.register_forward_hook(get_activation('tph'))\n",
    "    examples_dataloader = torch.utils.data.DataLoader(dataset=exampledataset, batch_size=batch_size, shuffle=False,collate_fn=dataset.collate_fn)\n",
    "\n",
    "    for index, (images,_) in enumerate(examples_dataloader):\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            images = images.to(device)\n",
    "            model(images)\n",
    "            outputs = activation['tph']\n",
    "            if isinstance(outputs, list):\n",
    "                output = outputs[-1]\n",
    "            else:\n",
    "                output = outputs\n",
    "\n",
    "            preds, maxvals = get_final_preds(cfg, output.clone().cpu().numpy(), None, None, transform_back=False)\n",
    "\n",
    "        # from heatmap_coord to original_image_coord\n",
    "        query_locations = np.array([p * 4 + 0.5 for p in preds[0]])\n",
    "\n",
    "        inspect_atten_map_by_locations(exampledataset[index][1], model.tph , query_locations, model_name=\"transposer\", mode='dependency', save_img=True, threshold=0.005, outinfo=(out_path , str(out_index)), device=device)\n",
    "\n",
    "def get_model_code():\n",
    "    import inspect, sys\n",
    "    from IPython.core.magics.code import extract_symbols\n",
    "\n",
    "    def new_getfile(object, _old_getfile=inspect.getfile):\n",
    "        if not inspect.isclass(object):\n",
    "            return _old_getfile(object)\n",
    "\n",
    "        # Lookup by parent module (as in current inspect)\n",
    "        if hasattr(object, '__module__'):\n",
    "            object_ = sys.modules.get(object.__module__)\n",
    "            if hasattr(object_, '__file__'):\n",
    "                return object_.__file__\n",
    "\n",
    "        # If parent module is __main__, lookup by methods (NEW)\n",
    "        for name, member in inspect.getmembers(object):\n",
    "            if inspect.isfunction(member) and object.__qualname__ + '.' + member.__name__ == member.__qualname__:\n",
    "                return inspect.getfile(member)\n",
    "        else:\n",
    "            raise TypeError('Source for {!r} not found'.format(object))\n",
    "\n",
    "    inspect.getfile = new_getfile\n",
    "    obj = PoseClassifier\n",
    "    cell_code = \"\".join(inspect.linecache.getlines(new_getfile(obj)))\n",
    "    class_code = extract_symbols(cell_code, obj.__name__)[0][0]\n",
    "    return class_code\n",
    "\n",
    "def save_run(train_dict = None, test_dict= None, save_examples = False, ):\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "    import numbers, json\n",
    "    def get_next_logdir():\n",
    "        from datetime import datetime\n",
    "        now = datetime.now()\n",
    "        date_time = now.strftime(\"%d_%b[%H-%M-%S]\")\n",
    "        multi_val_index = 1\n",
    "        logs_dir = OUT_DIR + \"logs/\"\n",
    "        dir_name = logs_dir + date_time\n",
    "        while os.path.exists(dir_name):\n",
    "            multi_val_index += 1\n",
    "            dir_name = \"\".join([logs_dir, date_time, \"(\", str(multi_val_index), \")\"])\n",
    "        return dir_name\n",
    "\n",
    "    def save_log(value_dict):\n",
    "        for key,value in value_dict.items():\n",
    "            if isinstance(value, numbers.Number):\n",
    "                writer.add_scalar(key, value, 0)\n",
    "            else:\n",
    "                for e, e_value in enumerate(value):\n",
    "                    writer.add_scalar(key, e_value, e)\n",
    "\n",
    "    dir_name = get_next_logdir()\n",
    "    model_path = os.path.join(dir_name, \"model.ckpt\")\n",
    "    model_json_path = os.path.join(dir_name, \"model_params.json\")\n",
    "    model_classcode_path = os.path.join(dir_name, \"model_class_code.txt\")\n",
    "    example_images_path = os.path.join(dir_name, \"example\")\n",
    "    writer = SummaryWriter(log_dir=dir_name)\n",
    "    model_dict = { \"params\": {\"num_classes\": num_classes, \"num_epochs\": num_epochs, \"batch_size\": batch_size, \"learning_rate\": learning_rate, \"learning_rate_decay\": learning_rate_decay, \"criterion\": str(criterion.__class__), \"optimizer\": str(optimizer.__class__)}}\n",
    "\n",
    "\n",
    "    print(\"Saving {} in {}\".format(\"model\", model_path), end=\" \")\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(\"DONE\")\n",
    "\n",
    "\n",
    "    print(\"Saving {} in {}\".format(\"Logs (if any)\", dir_name), end=\" \")\n",
    "    if train_dict is not None: save_log(train_dict)\n",
    "    if test_dict is not None:save_log(test_dict)\n",
    "    writer.close()\n",
    "    print(\"DONE\")\n",
    "\n",
    "    print(\"Saving {} in {}\".format(\"parameters\", model_json_path), end=\" \")\n",
    "    with open(model_json_path, \"w\") as jf:\n",
    "        json.dump(model_dict, jf)\n",
    "    print(\"DONE\")\n",
    "\n",
    "    print(\"Saving {} in {}\".format(\"class code\", model_classcode_path), end=\" \")\n",
    "    with open(model_classcode_path, \"w\") as f:\n",
    "        f.write(get_model_code())\n",
    "    print(\"DONE\")\n",
    "\n",
    "    if save_examples:\n",
    "        EXAMPLE_DIR = \"./data/examples/\"\n",
    "        if not os.path.exists(EXAMPLE_DIR):\n",
    "            print(\"{} path does not exist. Be sure insert in the path the images that you want to try. Follow the istruction in the README.md file\".format(EXAMPLE_DIR))\n",
    "        else:\n",
    "            print(\"Saving {} in {}\".format(\"Example Images\", example_images_path), end=\" \")\n",
    "            if not os.path.exists(example_images_path): os.makedirs(example_images_path)\n",
    "\n",
    "            example_dataset = YogaPoseDataset(EXAMPLE_DIR, transform=norm_transform)\n",
    "            example_dataset1 = example_dataset[0:1]\n",
    "            example_dataset2 = example_dataset[1:2]\n",
    "            example_dataset3 = example_dataset[2:3]\n",
    "\n",
    "            TransPoseVisualizer(example_dataset1, example_images_path, 1)\n",
    "            TransPoseVisualizer(example_dataset2, example_images_path, 2)\n",
    "            TransPoseVisualizer(example_dataset3, example_images_path, 3)\n",
    "            print(\"DONE\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model in ./out/logs/13_Dec[17-19-06]/model.ckpt DONE\n",
      "Saving Logs (if any) in ./out/logs/13_Dec[17-19-06] DONE\n",
      "Saving parameters in ./out/logs/13_Dec[17-19-06]/model_params.json DONE\n",
      "Saving class code in ./out/logs/13_Dec[17-19-06]/model_class_code.txt DONE\n"
     ]
    }
   ],
   "source": [
    "save_run(train_values, test_values, save_examples=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}