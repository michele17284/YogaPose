{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numbers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torch import nn\n",
    "import os\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import genericpath\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# force working on cpu due to memory limitation\n",
    "#device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "def add_margin(pil_img, top, right, bottom, left, color):\n",
    "    width, height = pil_img.size\n",
    "    new_width = width + right + left\n",
    "    new_height = height + top + bottom\n",
    "    result = Image.new(pil_img.mode, (new_width, new_height), color)\n",
    "    result.paste(pil_img, (left, top))\n",
    "    return result\n",
    "\n",
    "\n",
    "class YogaPoseDataset(Dataset):\n",
    "\n",
    "    def __init__(self, root, train, size=(256, 192), transform=None, example_images=None):\n",
    "        root = root\n",
    "        subdir = \"train\" if train else \"test\"\n",
    "        self.data_path = os.path.join(root,subdir)\n",
    "        self.size = size\n",
    "        self.transform = transform\n",
    "        # call to init the data\n",
    "        self._init_data()\n",
    "\n",
    "    def _init_data(self):\n",
    "        images = list()\n",
    "\n",
    "        for _, directory_class in enumerate(os.listdir(self.data_path)):\n",
    "            class_path = os.path.join(self.data_path, directory_class)\n",
    "            for file_name in os.listdir(class_path):\n",
    "                f = cv2.imread(os.path.join(class_path, file_name), cv2.IMREAD_COLOR)\n",
    "\n",
    "                f = cv2.cvtColor(f, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                if self.transform is not None:\n",
    "                    f = self.transform(f)\n",
    "\n",
    "                data = torch.reshape(torch.FloatTensor(f), (3, self.size[0], self.size[1]))\n",
    "\n",
    "                # format example  images[x][0] -> (label, input)\n",
    "                # format example  images[x][1] -> [other information]\n",
    "                # images[x] -> ((class_id, image_tensor), [filename])\n",
    "                images.append((int(directory_class), data))\n",
    "\n",
    "\n",
    "        np.random.shuffle(images)\n",
    "        self.images = images\n",
    "\n",
    "    def __len__(self):\n",
    "        # returns the number of samples in our dataset\n",
    "        return len(self.images)\n",
    "\n",
    "    def getData(self):\n",
    "        return self.images\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            idx: the index of the sample\n",
    "\n",
    "        Returns: a tuple (class, input) for the given sample\n",
    "\n",
    "        \"\"\"\n",
    "        return self.images[idx]\n",
    "\n",
    "    def collate_fn(self, data):\n",
    "        #print(data)\n",
    "        Xs = torch.stack([x[1] for x in data])\n",
    "        y = torch.stack([torch.tensor(x[0]) for x in data])\n",
    "        return Xs, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "img should be PIL Image. Got <class 'numpy.ndarray'>",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_20889/596911436.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     20\u001B[0m \u001B[0mMODEL_NAME\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"tpr_a4_256x192\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     21\u001B[0m \u001B[0mtestset\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mYogaPoseDataset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mDATASET_PATH\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mtrain\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtransform\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtest_transform\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 22\u001B[0;31m \u001B[0mtrainset\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mYogaPoseDataset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mDATASET_PATH\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mtrain\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtransform\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mnorm_transform\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     23\u001B[0m \u001B[0mvalset\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtrainset\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrainset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m//\u001B[0m\u001B[0;36m10\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0;36m9\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[0mtrainset\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtrainset\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrainset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m//\u001B[0m\u001B[0;36m10\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0;36m9\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_20889/3379376976.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, root, train, size, transform, example_images)\u001B[0m\n\u001B[1;32m     43\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtransform\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtransform\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     44\u001B[0m         \u001B[0;31m# call to init the data\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 45\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_init_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     46\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     47\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_init_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_20889/3379376976.py\u001B[0m in \u001B[0;36m_init_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     56\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     57\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtransform\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 58\u001B[0;31m                     \u001B[0mf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtransform\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     59\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     60\u001B[0m                 \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreshape\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mFloatTensor\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0;36m3\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msize\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msize\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/AML/YogaPose/venvnew/lib/python3.8/site-packages/torchvision/transforms/transforms.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, img)\u001B[0m\n\u001B[1;32m     59\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__call__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mimg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     60\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mt\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtransforms\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 61\u001B[0;31m             \u001B[0mimg\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mt\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mimg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     62\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mimg\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     63\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/AML/YogaPose/venvnew/lib/python3.8/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1102\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1103\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/AML/YogaPose/venvnew/lib/python3.8/site-packages/torchvision/transforms/transforms.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, img)\u001B[0m\n\u001B[1;32m    628\u001B[0m         \"\"\"\n\u001B[1;32m    629\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpadding\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 630\u001B[0;31m             \u001B[0mimg\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpad\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mimg\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpadding\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfill\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpadding_mode\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    631\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    632\u001B[0m         \u001B[0mwidth\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mheight\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_image_size\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mimg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/AML/YogaPose/venvnew/lib/python3.8/site-packages/torchvision/transforms/functional.py\u001B[0m in \u001B[0;36mpad\u001B[0;34m(img, padding, fill, padding_mode)\u001B[0m\n\u001B[1;32m    470\u001B[0m     \"\"\"\n\u001B[1;32m    471\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mimg\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mTensor\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 472\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mF_pil\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpad\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mimg\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpadding\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mpadding\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfill\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mfill\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpadding_mode\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mpadding_mode\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    473\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    474\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mF_t\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpad\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mimg\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpadding\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mpadding\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfill\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mfill\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpadding_mode\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mpadding_mode\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/AML/YogaPose/venvnew/lib/python3.8/site-packages/torchvision/transforms/functional_pil.py\u001B[0m in \u001B[0;36mpad\u001B[0;34m(img, padding, fill, padding_mode)\u001B[0m\n\u001B[1;32m    135\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    136\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0m_is_pil_image\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mimg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 137\u001B[0;31m         \u001B[0;32mraise\u001B[0m \u001B[0mTypeError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"img should be PIL Image. Got {}\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mimg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    138\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    139\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpadding\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mnumbers\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mNumber\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtuple\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: img should be PIL Image. Got <class 'numpy.ndarray'>"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "data_aug_parameters = {\n",
    "    \"RC_size\": 32, \"RC_padding\": 2,  # default = none\n",
    "    \"CJ_brightness\": 0,  # default = 0\n",
    "    \"CJ_contrast\": 0,  # default = 0\n",
    "    \"CJ_saturation\": 0,  # default = 0\n",
    "    \"CJ_hue\": 0,  # default = 0\n",
    "    \"P_padding\": 3, \"P_type\": \"constant\",  # default = constant\n",
    "    \"HF_p\": 0.5, \"VF_p\": 0.5, \"RR_degrees\": 10, \"RG_p\": 0.2}\n",
    "\n",
    "data_aug_transform = [transforms.RandomCrop(data_aug_parameters[\"RC_size\"], padding=data_aug_parameters[\"RC_padding\"]),\n",
    "                        transforms.RandomHorizontalFlip(data_aug_parameters[\"HF_p\"]),\n",
    "                        # transforms.RandomVerticalFlip(data_aug_parameters[\"VF_p\"]),\n",
    "                        transforms.RandomRotation(degrees=data_aug_parameters[\"RR_degrees\"]),\n",
    "                        # transforms.ColorJitter(brightness=data_aug_parameters[\"CJ_brightness\"], contrast=data_aug_parameters[\"CJ_contrast\"], saturation=data_aug_parameters[\"CJ_saturation\"], hue=data_aug_parameters[\"CJ_hue\"]),\n",
    "                        transforms.RandomGrayscale(data_aug_parameters[\"RG_p\"]), ]\n",
    "norm_transform = transforms.Compose(data_aug_transform + [transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "test_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "DATASET_PATH = './data/split/'\n",
    "ANNOTATION_PATH = './data/annotations/'\n",
    "MODEL_NAME = \"tpr_a4_256x192\"\n",
    "testset = YogaPoseDataset(DATASET_PATH,train=False, transform=test_transform)\n",
    "trainset = YogaPoseDataset(DATASET_PATH,train=True, transform=norm_transform)\n",
    "valset = trainset[(len(trainset)//10)*9:]\n",
    "trainset = trainset[:(len(trainset)//10)*9]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get model from torch hub\n",
    "assert MODEL_NAME in [\"tpr_a4_256x192\", \"tph_a4_256x192\"]\n",
    "\n",
    "transpose_model = torch.hub.load('yangsenius/TransPose:main', MODEL_NAME, pretrained=True)\n",
    "transpose_model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from TransPose.lib.config import cfg\n",
    "from TransPose.lib.utils import transforms\n",
    "from TransPose.lib.core.inference import get_final_preds\n",
    "from TransPose.visualize import inspect_atten_map_by_locations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "OUT_DIR = \"./out/\"\n",
    "idx = 0\n",
    "\n",
    "if not os.path.isdir(OUT_DIR):\n",
    "    os.makedirs(OUT_DIR)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class PoseClassifier(nn.Module):\n",
    "    def __init__(self, n_class,\n",
    "                 transpose_model,device=device, fine_tune=False, pretrained=True):\n",
    "        super(PoseClassifier, self).__init__()\n",
    "        layers = []\n",
    "        dropout = 0.5\n",
    "        hidden_layers = [128, 256, 512, 256, 128, 256, 128, 128]\n",
    "        self.tpr = transpose_model\n",
    "        layers.append(nn.Conv2d(17, 128, 3, padding=1))\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.conv1 = nn.Conv2d(17, hidden_layers[0], 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(hidden_layers[0])\n",
    "        self.pool1 = nn.MaxPool2d((2, 2), 2)\n",
    "\n",
    "        self.pool2 = nn.MaxPool2d((3, 3), 3)\n",
    "        self.conv2 = nn.Conv2d(hidden_layers[0], hidden_layers[1], 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(hidden_layers[1])\n",
    "        self.conv3 = nn.Conv2d(hidden_layers[1], hidden_layers[2], 3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(hidden_layers[2])\n",
    "        self.conv4 = nn.Conv2d(hidden_layers[2], hidden_layers[3], 3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(hidden_layers[3])\n",
    "        self.conv5 = nn.Conv2d(hidden_layers[3], hidden_layers[4], 3, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(hidden_layers[4])\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.lin1 = nn.Linear(hidden_layers[4],hidden_layers[-1])\n",
    "        self.classifier = nn.Linear(hidden_layers[-1],n_class)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.tpr(x)\n",
    "        #print(out.size(),\"AFTER TPH\")\n",
    "        out = self.conv1(out)\n",
    "        out = self.bn1(out)\n",
    "        #print(out.size(),\"AFTER CONV1\")\n",
    "        out = self.pool1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        #print(out.size(),\"AFTER POOL\")\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        #print(out.size(),\"AFTER CONV2\")\n",
    "        out = self.pool1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        #print(out.size(),\"AFTER POOL\")\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        #print(out.size(),\"AFTER CONV3\")\n",
    "        out = self.pool2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        #print(out.size(),\"AFTER POOL\")\n",
    "        out = self.conv4(out)\n",
    "        out = self.bn4(out)\n",
    "        #print(out.size(),\"AFTER CONV4\")\n",
    "        out = self.pool2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        #print(out.size(),\"AFTER POOL\")\n",
    "        out = self.conv5(out)\n",
    "        out = self.bn5(out)\n",
    "        #print(out.size(),\"AFTER CONV5\")\n",
    "        #out = self.pool1(out)\n",
    "        out = self.flatten(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        #print(out.size(),\"AFTER POOL\")\n",
    "        out = self.lin1(out)\n",
    "        '''\n",
    "        out = self.pool1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        #print(out.size(),\"AFTER POOL\")\n",
    "        out = self.conv6(out)\n",
    "\n",
    "        #print(out.size(),\"AFTER CONV6\")\n",
    "        '''\n",
    "\n",
    "\n",
    "        out = self.flatten(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        #print(out.size(),\"AFTER FLATTEN\")\n",
    "\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_classes = 107\n",
    "num_epochs = 100\n",
    "batch_size = 12\n",
    "learning_rate = 1e-4\n",
    "learning_rate_decay = 0.99\n",
    "\n",
    "model = PoseClassifier(n_class=num_classes, transpose_model=transpose_model)\n",
    "model.to(device)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=trainset, batch_size=batch_size, shuffle=False,collate_fn=dataset.collate_fn)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=valset,batch_size=batch_size,shuffle=False,collate_fn=dataset.collate_fn)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=testset,batch_size=batch_size,shuffle=False,collate_fn=dataset.collate_fn)\n",
    "\n",
    "\n",
    "fine_tune = False\n",
    "if fine_tune:\n",
    "    params_to_update = []\n",
    "    for param in model.tpr.parameters():\n",
    "        param.requires_grad = False\n",
    "    for p in model.parameters():\n",
    "        if p.requires_grad == True:\n",
    "            params_to_update.append(p)\n",
    "else:\n",
    "    params_to_update = model.parameters()\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params_to_update, lr=learning_rate)\n",
    "\n",
    "def update_lr(optimizer, lr):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(len(train_loader))\n",
    "len(train_loader.dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Train the model\n",
    "\n",
    "#best_model = type(model)(num_classes, fine_tune, pretrained) # get a new instance\n",
    "def train(model,num_epochs=num_epochs,lr=learning_rate):\n",
    "    total_step = len(train_loader)\n",
    "    loss_train = []\n",
    "    loss_val = []\n",
    "    best_accuracy = None\n",
    "    accuracy_val = []\n",
    "    accuracy_test = []\n",
    "\n",
    "    train_f1 = []\n",
    "    val_f1 = []\n",
    "    for epoch in range(num_epochs):\n",
    "        train_predicts = torch.tensor([]).to(device)\n",
    "        train_labels = torch.tensor([]).to(device)\n",
    "        val_predicts = torch.tensor([]).to(device)\n",
    "        val_labels = torch.tensor([]).to(device)\n",
    "        model.train()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        loss_iter = 0\n",
    "\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            # Move tensors to the configured device\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            #print(outputs,labels)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_iter += loss.item()\n",
    "\n",
    "\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                      .format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))\n",
    "\n",
    "            train_predicts = torch.cat((train_predicts, predicted), dim=0)\n",
    "            train_labels = torch.cat((train_labels, labels), dim=0)\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        accuracy_test.append(accuracy)\n",
    "        loss_train.append(loss_iter / (len(train_loader) * batch_size))\n",
    "        print('Training accuracy is: {} %'.format(accuracy))\n",
    "        train_labels = train_labels.cpu().numpy()\n",
    "        train_predicts = train_predicts.cpu().numpy()\n",
    "        f1 =f1_score(train_labels, train_predicts, average='weighted')\n",
    "        f1 = f1 * 100\n",
    "        train_f1.append(f1)\n",
    "        print('Training F1 is: {} %'.format(f1))\n",
    "\n",
    "        # Code to update the lr\n",
    "        lr *= learning_rate_decay\n",
    "        update_lr(optimizer, lr)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            loss_iter = 0\n",
    "            for images, labels in val_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss_iter += loss.item()\n",
    "                val_predicts = torch.cat((val_predicts, predicted), dim=0)\n",
    "                val_labels = torch.cat((val_labels, labels), dim=0)\n",
    "            loss_val.append(loss_iter / (len(val_loader) * batch_size))\n",
    "            accuracy = 100 * correct / total\n",
    "            accuracy_val.append(accuracy)\n",
    "            print('Validation accuracy is: {} %'.format(accuracy))\n",
    "            val_labels = val_labels.cpu().numpy()\n",
    "            val_predicts = val_predicts.cpu().numpy()\n",
    "            f1 =f1_score(val_labels, val_predicts, average='weighted')\n",
    "            f1 = f1 * 100\n",
    "            val_f1.append(f1)\n",
    "            print('Validation F1 is: {} %'.format(f1))\n",
    "            print(\"--------------------- NEXT EPOCH ---------------------\")\n",
    "            early_stop = True\n",
    "            patience = 3\n",
    "            if epoch > patience - 1:\n",
    "                for j in range(patience - 1):\n",
    "                    if max(accuracy_val) > list(reversed(accuracy_val))[j]:\n",
    "                        if \"not_improving_epochs\" in locals():\n",
    "                            not_improving_epochs += 1\n",
    "                        else:\n",
    "                            not_improving_epochs = 1\n",
    "                        print('Not saving the model')\n",
    "                    else:\n",
    "                        not_improving_epochs = 0\n",
    "                        best_model = model\n",
    "                        print(\"Saving the model\")\n",
    "                        break\n",
    "                    if not_improving_epochs >= patience:\n",
    "                        early_stop = True\n",
    "                        print('Early stopping')\n",
    "                        break\n",
    "                    break\n",
    "\n",
    "    plt.figure(2)\n",
    "    plt.plot(loss_train, 'r', label='Train loss')\n",
    "    plt.plot(loss_val, 'g', label='Val loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(3)\n",
    "    plt.plot(accuracy_val, 'r', label='Val accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return {\"Loss/train\": loss_train, \"Loss/val\": loss_val, \"Accuracy/train\":accuracy_val, \"Accuracy/val\": accuracy_test, \"F1/train\": train_f1, \"F1/val\": val_f1}\n",
    "\n",
    "def test(model):\n",
    "    with torch.no_grad():\n",
    "            all_preds = torch.tensor([]).to(device)\n",
    "            all_labels = torch.tensor([]).to(device)\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            loss_iter = 0\n",
    "            for images, labels in test_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss_iter += loss.item()\n",
    "                all_preds = torch.cat((all_preds, predicted), dim=0)\n",
    "                all_labels = torch.cat((all_labels, labels), dim=0)\n",
    "\n",
    "            loss_test =(loss_iter / (len(test_loader) * batch_size))\n",
    "            accuracy = 100 * correct / total\n",
    "\n",
    "            all_preds = all_preds.cpu().numpy()\n",
    "            all_labels = all_labels.cpu().numpy()\n",
    "            f1 =  f1_score(all_labels, all_preds, average='weighted')\n",
    "            f1 = f1 * 100\n",
    "            print('Test accuracy is: {} %'.format(accuracy))\n",
    "            print('Test f1 is: {} %'.format(f1))\n",
    "            print('Test Loss: {:.4f}'.format(loss_test))\n",
    "    return all_preds, all_labels, {\"Accuracy/test\": accuracy, \"Loss/test\": loss_test, \"F1/test\" : f1}\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_values = train(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_predicted, test_labels , test_values = test(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def TransPoseVisualizer(exampledataset, out_path, out_index):\n",
    "    import shutil\n",
    "    activation = {}\n",
    "    def get_activation(name):\n",
    "        def hook(model, input, output):\n",
    "            activation[name] = output.detach()\n",
    "        return hook\n",
    "    model.tph.register_forward_hook(get_activation('tph'))\n",
    "    examples_dataloader = torch.utils.data.DataLoader(dataset=exampledataset, batch_size=batch_size, shuffle=False,collate_fn=dataset.collate_fn)\n",
    "\n",
    "    for index, (images,_) in enumerate(examples_dataloader):\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            images = images.to(device)\n",
    "            model(images)\n",
    "            outputs = activation['tph']\n",
    "            if isinstance(outputs, list):\n",
    "                output = outputs[-1]\n",
    "            else:\n",
    "                output = outputs\n",
    "\n",
    "            preds, maxvals = get_final_preds(cfg, output.clone().cpu().numpy(), None, None, transform_back=False)\n",
    "\n",
    "        # from heatmap_coord to original_image_coord\n",
    "        query_locations = np.array([p * 4 + 0.5 for p in preds[0]])\n",
    "\n",
    "        inspect_atten_map_by_locations(exampledataset[index][1], model.tph , query_locations, model_name=\"transposer\", mode='dependency', save_img=True, threshold=0.005, outinfo=(out_path , str(out_index)), device=device)\n",
    "\n",
    "def get_model_code():\n",
    "    import inspect, sys\n",
    "    from IPython.core.magics.code import extract_symbols\n",
    "\n",
    "    def new_getfile(object, _old_getfile=inspect.getfile):\n",
    "        if not inspect.isclass(object):\n",
    "            return _old_getfile(object)\n",
    "\n",
    "        # Lookup by parent module (as in current inspect)\n",
    "        if hasattr(object, '__module__'):\n",
    "            object_ = sys.modules.get(object.__module__)\n",
    "            if hasattr(object_, '__file__'):\n",
    "                return object_.__file__\n",
    "\n",
    "        # If parent module is __main__, lookup by methods (NEW)\n",
    "        for name, member in inspect.getmembers(object):\n",
    "            if inspect.isfunction(member) and object.__qualname__ + '.' + member.__name__ == member.__qualname__:\n",
    "                return inspect.getfile(member)\n",
    "        else:\n",
    "            raise TypeError('Source for {!r} not found'.format(object))\n",
    "\n",
    "    inspect.getfile = new_getfile\n",
    "    obj = PoseClassifier\n",
    "    cell_code = \"\".join(inspect.linecache.getlines(new_getfile(obj)))\n",
    "    class_code = extract_symbols(cell_code, obj.__name__)[0][0]\n",
    "    return class_code\n",
    "\n",
    "def save_run(train_dict = None, test_dict= None, save_examples = False):\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "    import numbers, json\n",
    "    def get_next_logdir():\n",
    "        from datetime import datetime\n",
    "        now = datetime.now()\n",
    "        date_time = now.strftime(\"%d_%b[%H-%M-%S]\")\n",
    "        multi_val_index = 1\n",
    "        logs_dir = OUT_DIR + \"logs/\"\n",
    "        dir_name = logs_dir + date_time\n",
    "        while os.path.exists(dir_name):\n",
    "            multi_val_index += 1\n",
    "            dir_name = \"\".join([logs_dir, date_time, \"(\", str(multi_val_index), \")\"])\n",
    "        return dir_name\n",
    "\n",
    "    def save_log(value_dict):\n",
    "        for key,value in value_dict.items():\n",
    "            if isinstance(value, numbers.Number):\n",
    "                writer.add_scalar(key, value, 0)\n",
    "            else:\n",
    "                for e, e_value in enumerate(value):\n",
    "                    writer.add_scalar(key, e_value, e)\n",
    "\n",
    "    dir_name = get_next_logdir()\n",
    "    model_path = os.path.join(dir_name, \"model.ckpt\")\n",
    "    model_json_path = os.path.join(dir_name, \"model_params.json\")\n",
    "    result_path = os.path.join(dir_name, \"results.json\")\n",
    "\n",
    "    model_classcode_path = os.path.join(dir_name, \"model_class_code.txt\")\n",
    "    example_images_path = os.path.join(dir_name, \"example\")\n",
    "    writer = SummaryWriter(log_dir=dir_name)\n",
    "    model_dict = { \"params\": {\"num_classes\": num_classes, \"num_epochs\": num_epochs, \"batch_size\": batch_size, \"learning_rate\": learning_rate, \"learning_rate_decay\": learning_rate_decay, \"criterion\": str(criterion.__class__), \"optimizer\": str(optimizer.__class__)}}\n",
    "\n",
    "\n",
    "    print(\"Saving {} in {}\".format(\"model\", model_path), end=\" \")\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(\"DONE\")\n",
    "\n",
    "\n",
    "    print(\"Saving {} in {}\".format(\"Logs (if any)\", dir_name), end=\" \")\n",
    "    if train_dict is not None: save_log(train_dict)\n",
    "    if test_dict is not None:save_log(test_dict)\n",
    "    writer.close()\n",
    "    print(\"DONE\")\n",
    "\n",
    "    print(\"Saving {} in {}\".format(\"parameters\", model_json_path), end=\" \")\n",
    "    with open(model_json_path, \"w\") as jf:\n",
    "        json.dump(model_dict, jf)\n",
    "    print(\"DONE\")\n",
    "\n",
    "    print(\"Saving {} in {}\".format(\"results\", result_path), end=\" \")\n",
    "    result_json = dict()\n",
    "    for k,v in train_dict.items():\n",
    "        if any(sub in k for sub in [\"F1\", \"Accuracy\", \"Loss\"]): result_json[k] = v\n",
    "    for k,v in test_dict.items():\n",
    "        if any(sub in k for sub in [\"F1\", \"Accuracy\", \"Loss\"]): result_json[k] = v\n",
    "    with open(result_path, \"w\") as jf:\n",
    "        json.dump(result_json, jf)\n",
    "    print(\"DONE\")\n",
    "\n",
    "    print(\"Saving {} in {}\".format(\"class code\", model_classcode_path), end=\" \")\n",
    "    with open(model_classcode_path, \"w\") as f:\n",
    "        f.write(get_model_code())\n",
    "    print(\"DONE\")\n",
    "\n",
    "    if save_examples:\n",
    "        EXAMPLE_DIR = \"./data/examples/\"\n",
    "        if not os.path.exists(EXAMPLE_DIR):\n",
    "            print(\"{} path does not exist. Be sure insert in the path the images that you want to try. Follow the istruction in the README.md file\".format(EXAMPLE_DIR))\n",
    "        else:\n",
    "            print(\"Saving {} in {}\".format(\"Example Images\", example_images_path), end=\" \")\n",
    "            if not os.path.exists(example_images_path): os.makedirs(example_images_path)\n",
    "\n",
    "            example_dataset = YogaPoseDataset(EXAMPLE_DIR, transform=norm_transform)\n",
    "            example_dataset1 = example_dataset[0:1]\n",
    "            example_dataset2 = example_dataset[1:2]\n",
    "            example_dataset3 = example_dataset[2:3]\n",
    "\n",
    "            TransPoseVisualizer(example_dataset1, example_images_path, 1)\n",
    "            TransPoseVisualizer(example_dataset2, example_images_path, 2)\n",
    "            TransPoseVisualizer(example_dataset3, example_images_path, 3)\n",
    "            print(\"DONE\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "save_run(train_values, test_values, save_examples=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}